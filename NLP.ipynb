{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "9HA7KAotBu5N"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/shakespeare.txt','r',encoding='utf8') as f:\n",
        "    text = f.read()"
      ],
      "metadata": {
        "id": "psYWwPKGCcbr"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(text[:1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "PYrrNHuzCmvD",
        "outputId": "11a6b673-a0b2-4c77-8bea-8c02b4be3cb6"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n                     1\\n  From fairest creatures we desire increase,\\n  That thereby beauty's rose might never die,\\n  But as the riper should by time decease,\\n  His tender heir might bear his memory:\\n  But thou contracted to thine own bright eyes,\\n  Feed'st thy light's flame with self-substantial fuel,\\n  Making a famine where abundance lies,\\n  Thy self thy foe, to thy sweet self too cruel:\\n  Thou that art now the world's fresh ornament,\\n  And only herald to the gaudy spring,\\n  Within thine own bud buriest thy content,\\n  And tender churl mak'st waste in niggarding:\\n    Pity the world, or else this glutton be,\\n    To eat the world's due, by the grave and thee.\\n\\n\\n                     2\\n  When forty winters shall besiege thy brow,\\n  And dig deep trenches in thy beauty's field,\\n  Thy youth's proud livery so gazed on now,\\n  Will be a tattered weed of small worth held:  \\n  Then being asked, where all thy beauty lies,\\n  Where all the treasure of thy lusty days;\\n  To say within thine own deep su\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(text[:1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Inn92Zf4C4c5",
        "outputId": "5799b41c-7ab9-4a13-a100-6417a6094058"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "                     1\n",
            "  From fairest creatures we desire increase,\n",
            "  That thereby beauty's rose might never die,\n",
            "  But as the riper should by time decease,\n",
            "  His tender heir might bear his memory:\n",
            "  But thou contracted to thine own bright eyes,\n",
            "  Feed'st thy light's flame with self-substantial fuel,\n",
            "  Making a famine where abundance lies,\n",
            "  Thy self thy foe, to thy sweet self too cruel:\n",
            "  Thou that art now the world's fresh ornament,\n",
            "  And only herald to the gaudy spring,\n",
            "  Within thine own bud buriest thy content,\n",
            "  And tender churl mak'st waste in niggarding:\n",
            "    Pity the world, or else this glutton be,\n",
            "    To eat the world's due, by the grave and thee.\n",
            "\n",
            "\n",
            "                     2\n",
            "  When forty winters shall besiege thy brow,\n",
            "  And dig deep trenches in thy beauty's field,\n",
            "  Thy youth's proud livery so gazed on now,\n",
            "  Will be a tattered weed of small worth held:  \n",
            "  Then being asked, where all thy beauty lies,\n",
            "  Where all the treasure of thy lusty days;\n",
            "  To say within thine own deep su\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7jGYzPurDEqr",
        "outputId": "5806555e-e0a9-4013-c0bc-eb30658d709e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5445609"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encode text data"
      ],
      "metadata": {
        "id": "tB5v3sUwDbeJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_characters = set(text)"
      ],
      "metadata": {
        "id": "U6UnELVaDHt4"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoder = dict(enumerate(all_characters))"
      ],
      "metadata": {
        "id": "qM0tuEweDJv8"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = {char: ind for ind,char in decoder.items()}"
      ],
      "metadata": {
        "id": "-e0WywOnDORq"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_text = np.array([encoder[char] for char in text])"
      ],
      "metadata": {
        "id": "S8Q0jO6WDPxx"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_text[:500]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ct5YT06DSAi",
        "outputId": "c33053b0-f84e-4fc7-837d-30662e00c4d0"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 7, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55,\n",
              "       55, 55, 55, 55, 55, 33,  7, 55, 55, 54, 44, 67,  2, 55, 31, 28,  8,\n",
              "       44, 25, 57, 34, 55, 46, 44, 25, 28, 34,  3, 44, 25, 57, 55, 74, 25,\n",
              "       55, 75, 25, 57,  8, 44, 25, 55,  8, 21, 46, 44, 25, 28, 57, 25, 52,\n",
              "        7, 55, 55, 82, 26, 28, 34, 55, 34, 26, 25, 44, 25,  9, 17, 55,  9,\n",
              "       25, 28,  3, 34, 17, 69, 57, 55, 44, 67, 57, 25, 55,  2,  8, 56, 26,\n",
              "       34, 55, 21, 25, 47, 25, 44, 55, 75,  8, 25, 52,  7, 55, 55, 37,  3,\n",
              "       34, 55, 28, 57, 55, 34, 26, 25, 55, 44,  8, 15, 25, 44, 55, 57, 26,\n",
              "       67,  3, 43, 75, 55,  9, 17, 55, 34,  8,  2, 25, 55, 75, 25, 46, 25,\n",
              "       28, 57, 25, 52,  7, 55, 55, 36,  8, 57, 55, 34, 25, 21, 75, 25, 44,\n",
              "       55, 26, 25,  8, 44, 55,  2,  8, 56, 26, 34, 55,  9, 25, 28, 44, 55,\n",
              "       26,  8, 57, 55,  2, 25,  2, 67, 44, 17, 50,  7, 55, 55, 37,  3, 34,\n",
              "       55, 34, 26, 67,  3, 55, 46, 67, 21, 34, 44, 28, 46, 34, 25, 75, 55,\n",
              "       34, 67, 55, 34, 26,  8, 21, 25, 55, 67, 74, 21, 55,  9, 44,  8, 56,\n",
              "       26, 34, 55, 25, 17, 25, 57, 52,  7, 55, 55, 54, 25, 25, 75, 69, 57,\n",
              "       34, 55, 34, 26, 17, 55, 43,  8, 56, 26, 34, 69, 57, 55, 31, 43, 28,\n",
              "        2, 25, 55, 74,  8, 34, 26, 55, 57, 25, 43, 31, 61, 57,  3,  9, 57,\n",
              "       34, 28, 21, 34,  8, 28, 43, 55, 31,  3, 25, 43, 52,  7, 55, 55, 32,\n",
              "       28, 13,  8, 21, 56, 55, 28, 55, 31, 28,  2,  8, 21, 25, 55, 74, 26,\n",
              "       25, 44, 25, 55, 28,  9,  3, 21, 75, 28, 21, 46, 25, 55, 43,  8, 25,\n",
              "       57, 52,  7, 55, 55, 82, 26, 17, 55, 57, 25, 43, 31, 55, 34, 26, 17,\n",
              "       55, 31, 67, 25, 52, 55, 34, 67, 55, 34, 26, 17, 55, 57, 74, 25, 25,\n",
              "       34, 55, 57, 25, 43, 31, 55, 34, 67, 67, 55, 46, 44,  3, 25, 43, 50,\n",
              "        7, 55, 55, 82, 26, 67,  3, 55, 34, 26, 28, 34, 55, 28, 44, 34, 55,\n",
              "       21, 67, 74, 55, 34, 26, 25, 55, 74, 67, 44, 43, 75, 69, 57, 55, 31,\n",
              "       44, 25, 57, 26, 55, 67, 44, 21, 28,  2, 25, 21, 34, 52,  7, 55, 55,\n",
              "       51, 21, 75, 55, 67, 21, 43, 17, 55, 26, 25, 44, 28, 43, 75, 55, 34,\n",
              "       67, 55, 34, 26, 25, 55, 56, 28,  3, 75, 17, 55, 57, 15, 44,  8, 21,\n",
              "       56, 52,  7, 55, 55, 27,  8, 34, 26,  8, 21, 55, 34, 26,  8, 21, 25,\n",
              "       55, 67, 74, 21, 55,  9,  3])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# one hot encoding"
      ],
      "metadata": {
        "id": "PDs6Jl3FDjDZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def one_hot_encoder(encoded_text, num_uni_chars):\n",
        "  # Create a placeholder for zeros.\n",
        "    one_hot = np.zeros((encoded_text.size, num_uni_chars))\n",
        "\n",
        "    # Convert data type for later use with pytorch (errors if we dont!)\n",
        "    one_hot = one_hot.astype(np.float32)\n",
        "\n",
        "    # Using fancy indexing fill in the 1s at the correct index locations\n",
        "    one_hot[np.arange(one_hot.shape[0]), encoded_text.flatten()] = 1.0\n",
        "\n",
        "\n",
        "    # Reshape it so it matches the batch sahe\n",
        "    one_hot = one_hot.reshape((*encoded_text.shape, num_uni_chars))\n",
        "\n",
        "    return one_hot"
      ],
      "metadata": {
        "id": "uowPbOLZDULX"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "one_hot_encoder(np.array([1,2,0]),3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UiQK4MjVDpvb",
        "outputId": "d6c344fd-1ffe-4993-a13a-553640eba0c6"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 1., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [1., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating Training Batches\n"
      ],
      "metadata": {
        "id": "S83n5uOTDule"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example_text = np.arange(10)"
      ],
      "metadata": {
        "id": "eBe9HCJoDrir"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# If we wanted 5 batches\n",
        "example_text.reshape((5,-1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZyhL2WmyD3Yh",
        "outputId": "61247f5c-e602-43e6-d4e2-d36edc911106"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 1],\n",
              "       [2, 3],\n",
              "       [4, 5],\n",
              "       [6, 7],\n",
              "       [8, 9]])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_batches(encoded_text, samp_per_batch=10, seq_len=50):\n",
        "\n",
        "    '''\n",
        "    Generate (using yield) batches for training.\n",
        "\n",
        "    X: Encoded Text of length seq_len\n",
        "    Y: Encoded Text shifted by one\n",
        "\n",
        "    Example:\n",
        "\n",
        "    X:\n",
        "\n",
        "    [[1 2 3]]\n",
        "\n",
        "    Y:\n",
        "\n",
        "    [[ 2 3 4]]\n",
        "\n",
        "    encoded_text : Complete Encoded Text to make batches from\n",
        "    batch_size : Number of samples per batch\n",
        "    seq_len : Length of character sequence\n",
        "\n",
        "    '''\n",
        "\n",
        "    # Total number of characters per batch\n",
        "    # Example: If samp_per_batch is 2 and seq_len is 50, then 100\n",
        "    # characters come out per batch.\n",
        "    char_per_batch = samp_per_batch * seq_len\n",
        "\n",
        "\n",
        "    # Number of batches available to make\n",
        "    # Use int() to roun to nearest integer\n",
        "    num_batches_avail = int(len(encoded_text)/char_per_batch)\n",
        "\n",
        "    # Cut off end of encoded_text that\n",
        "    # won't fit evenly into a batch\n",
        "    encoded_text = encoded_text[:num_batches_avail * char_per_batch]\n",
        "\n",
        "\n",
        "    # Reshape text into rows the size of a batch\n",
        "    encoded_text = encoded_text.reshape((samp_per_batch, -1))\n",
        "\n",
        "\n",
        "    # Go through each row in array.\n",
        "    for n in range(0, encoded_text.shape[1], seq_len):\n",
        "\n",
        "        # Grab feature characters\n",
        "        x = encoded_text[:, n:n+seq_len]\n",
        "\n",
        "        # y is the target shifted over by 1\n",
        "        y = np.zeros_like(x)\n",
        "\n",
        "        #\n",
        "        try:\n",
        "            y[:, :-1] = x[:, 1:]\n",
        "            y[:, -1]  = encoded_text[:, n+seq_len]\n",
        "\n",
        "        # FOR POTENTIAL INDEXING ERROR AT THE END\n",
        "        except:\n",
        "            y[:, :-1] = x[:, 1:]\n",
        "            y[:, -1] = encoded_text[:, 0]\n",
        "\n",
        "        yield x, y"
      ],
      "metadata": {
        "id": "KLu59QO6D5bu"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_text = encoded_text[:20]"
      ],
      "metadata": {
        "id": "mECe0kBDD8mE"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "olAiMY57EAmK",
        "outputId": "0f3583bd-615c-4f19-efe0-26f845aa204b"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 7, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55,\n",
              "       55, 55, 55])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_generator = generate_batches(sample_text,samp_per_batch=2,seq_len=5)"
      ],
      "metadata": {
        "id": "JfmMPptPECjr"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Grab first batch\n",
        "x, y = next(batch_generator)"
      ],
      "metadata": {
        "id": "3r-dbZuZEEeW"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FteyeS7qEGVQ",
        "outputId": "4cd9e271-2340-41e6-a2b9-0c2a643045c9"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 7, 55, 55, 55, 55],\n",
              "       [55, 55, 55, 55, 55]])"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OOP85bhIEGp5",
        "outputId": "1290b912-8424-4d23-c02a-7082bfd317ab"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[55, 55, 55, 55, 55],\n",
              "       [55, 55, 55, 55, 55]])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.is_available()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bHrWqDHqEHMI",
        "outputId": "5440ff54-9cd6-40ec-c546-d610aad29112"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CharModel(nn.Module):\n",
        "\n",
        "    def __init__(self, all_chars, num_hidden=256, num_layers=4,drop_prob=0.5,use_gpu=False):\n",
        "\n",
        "\n",
        "        # SET UP ATTRIBUTES\n",
        "        super().__init__()\n",
        "        self.drop_prob = drop_prob\n",
        "        self.num_layers = num_layers\n",
        "        self.num_hidden = num_hidden\n",
        "        self.use_gpu = use_gpu\n",
        "\n",
        "        #CHARACTER SET, ENCODER, and DECODER\n",
        "        self.all_chars = all_chars\n",
        "        self.decoder = dict(enumerate(all_chars))\n",
        "        self.encoder = {char: ind for ind,char in decoder.items()}\n",
        "\n",
        "\n",
        "        self.lstm = nn.LSTM(len(self.all_chars), num_hidden, num_layers, dropout=drop_prob, batch_first=True)\n",
        "\n",
        "        self.dropout = nn.Dropout(drop_prob)\n",
        "\n",
        "        self.fc_linear = nn.Linear(num_hidden, len(self.all_chars))\n",
        "\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "\n",
        "\n",
        "        lstm_output, hidden = self.lstm(x, hidden)\n",
        "\n",
        "\n",
        "        drop_output = self.dropout(lstm_output)\n",
        "\n",
        "        drop_output = drop_output.contiguous().view(-1, self.num_hidden)\n",
        "\n",
        "\n",
        "        final_out = self.fc_linear(drop_output)\n",
        "\n",
        "\n",
        "        return final_out, hidden\n",
        "\n",
        "\n",
        "    def hidden_state(self, batch_size):\n",
        "        '''\n",
        "        Used as separate method to account for both GPU and CPU users.\n",
        "        '''\n",
        "\n",
        "        if self.use_gpu:\n",
        "\n",
        "            hidden = (torch.zeros(self.num_layers,batch_size,self.num_hidden).cuda(),\n",
        "                     torch.zeros(self.num_layers,batch_size,self.num_hidden).cuda())\n",
        "        else:\n",
        "            hidden = (torch.zeros(self.num_layers,batch_size,self.num_hidden),\n",
        "                     torch.zeros(self.num_layers,batch_size,self.num_hidden))\n",
        "\n",
        "        return hidden\n",
        ""
      ],
      "metadata": {
        "id": "z4OvnVQrEJy6"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating the LSTM Model"
      ],
      "metadata": {
        "id": "5-GtcKDfEv3n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CharModel(nn.Module):\n",
        "\n",
        "    def __init__(self, all_chars, num_hidden=256, num_layers=4,drop_prob=0.5,use_gpu=False):\n",
        "\n",
        "\n",
        "        # SET UP ATTRIBUTES\n",
        "        super().__init__()\n",
        "        self.drop_prob = drop_prob\n",
        "        self.num_layers = num_layers\n",
        "        self.num_hidden = num_hidden\n",
        "        self.use_gpu = use_gpu\n",
        "\n",
        "        #CHARACTER SET, ENCODER, and DECODER\n",
        "        self.all_chars = all_chars\n",
        "        self.decoder = dict(enumerate(all_chars))\n",
        "        self.encoder = {char: ind for ind,char in decoder.items()}\n",
        "\n",
        "\n",
        "        self.lstm = nn.LSTM(len(self.all_chars), num_hidden, num_layers, dropout=drop_prob, batch_first=True)\n",
        "\n",
        "        self.dropout = nn.Dropout(drop_prob)\n",
        "\n",
        "        self.fc_linear = nn.Linear(num_hidden, len(self.all_chars))\n",
        "\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "\n",
        "\n",
        "        lstm_output, hidden = self.lstm(x, hidden)\n",
        "\n",
        "\n",
        "        drop_output = self.dropout(lstm_output)\n",
        "\n",
        "        drop_output = drop_output.contiguous().view(-1, self.num_hidden)\n",
        "\n",
        "\n",
        "        final_out = self.fc_linear(drop_output)\n",
        "\n",
        "\n",
        "        return final_out, hidden\n",
        "\n",
        "\n",
        "    def hidden_state(self, batch_size):\n",
        "        '''\n",
        "        Used as separate method to account for both GPU and CPU users.\n",
        "        '''\n",
        "\n",
        "        if self.use_gpu:\n",
        "\n",
        "            hidden = (torch.zeros(self.num_layers,batch_size,self.num_hidden).cuda(),\n",
        "                     torch.zeros(self.num_layers,batch_size,self.num_hidden).cuda())\n",
        "        else:\n",
        "            hidden = (torch.zeros(self.num_layers,batch_size,self.num_hidden),\n",
        "                     torch.zeros(self.num_layers,batch_size,self.num_hidden))\n",
        "\n",
        "        return hidden\n",
        ""
      ],
      "metadata": {
        "id": "CXnkbnXUEsVU"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Instance of the Model"
      ],
      "metadata": {
        "id": "KrALfKkhE2_8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = CharModel(\n",
        "    all_chars=all_characters,\n",
        "    num_hidden=512,\n",
        "    num_layers=3,\n",
        "    drop_prob=0.5,\n",
        "    use_gpu=True,\n",
        ")"
      ],
      "metadata": {
        "id": "U_srPq1SE0H2"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_param  = []\n",
        "for p in model.parameters():\n",
        "    total_param.append(int(p.numel()))"
      ],
      "metadata": {
        "id": "5fgJ1mr9E7VD"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sum(total_param)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PYbTFsWlE87H",
        "outputId": "2b8a5aff-307b-4684-d68a-5ece3dabbb94"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5470292"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(encoded_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zmyMnB39E-kE",
        "outputId": "75b06be7-64fb-4d35-877f-db28bd6a722e"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5445609"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Optimizer and Loss"
      ],
      "metadata": {
        "id": "6N2O1Xo8FCd2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(),lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "0w5u4NLRFAL-"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# percentage of data to be used for training\n",
        "train_percent = 0.9"
      ],
      "metadata": {
        "id": "Mz_USeXPFE-m"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(encoded_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y4IOxlwwFJMn",
        "outputId": "4ca934d4-6f3c-4ed8-d59f-8d274205a779"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5445609"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "int(len(encoded_text) * (train_percent))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rZo8fn3sFMOF",
        "outputId": "03ece343-9649-44ca-e6e2-c962108173ef"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4901048"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_ind = int(len(encoded_text) * (train_percent))"
      ],
      "metadata": {
        "id": "UQf1gbmXFPCi"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = encoded_text[:train_ind]\n",
        "val_data = encoded_text[train_ind:]"
      ],
      "metadata": {
        "id": "gBB_KE-lFW6k"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training the Network\n",
        "model parameters"
      ],
      "metadata": {
        "id": "qfwSlAeGFa8X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## VARIABLES\n",
        "\n",
        "# Epochs to train for\n",
        "epochs = 50\n",
        "# batch size\n",
        "batch_size = 128\n",
        "\n",
        "# Length of sequence\n",
        "seq_len = 100\n",
        "\n",
        "# for printing report purposes\n",
        "# always start at 0\n",
        "tracker = 0\n",
        "\n",
        "# number of characters in text\n",
        "num_char = max(encoded_text)+1"
      ],
      "metadata": {
        "id": "z9Y1XRyMFYp6"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set model to train\n",
        "model.train()\n",
        "\n",
        "\n",
        "# Check to see if using GPU\n",
        "if model.use_gpu:\n",
        "    model.cuda()\n",
        "\n",
        "for i in range(epochs):\n",
        "\n",
        "    hidden = model.hidden_state(batch_size)\n",
        "\n",
        "\n",
        "    for x,y in generate_batches(train_data,batch_size,seq_len):\n",
        "\n",
        "        tracker += 1\n",
        "\n",
        "        # One Hot Encode incoming data\n",
        "        x = one_hot_encoder(x,num_char)\n",
        "\n",
        "        # Convert Numpy Arrays to Tensor\n",
        "\n",
        "        inputs = torch.from_numpy(x)\n",
        "        targets = torch.from_numpy(y)\n",
        "\n",
        "        # Adjust for GPU if necessary\n",
        "\n",
        "        if model.use_gpu:\n",
        "\n",
        "            inputs = inputs.cuda()\n",
        "            targets = targets.cuda()\n",
        "\n",
        "        # Reset Hidden State\n",
        "        # If we dont' reset we would backpropagate through all training history\n",
        "        hidden = tuple([state.data for state in hidden])\n",
        "\n",
        "        model.zero_grad()\n",
        "\n",
        "        lstm_output, hidden = model.forward(inputs,hidden)\n",
        "        loss = criterion(lstm_output,targets.view(batch_size*seq_len).long())\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        # POSSIBLE EXPLODING GRADIENT PROBLEM!\n",
        "        # LET\"S CLIP JUST IN CASE\n",
        "        nn.utils.clip_grad_norm_(model.parameters(),max_norm=5)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "\n",
        "        ###################################\n",
        "        ### CHECK ON VALIDATION SET ######\n",
        "        #################################\n",
        "\n",
        "        if tracker % 25 == 0:\n",
        "\n",
        "            val_hidden = model.hidden_state(batch_size)\n",
        "            val_losses = []\n",
        "            model.eval()\n",
        "\n",
        "            for x,y in generate_batches(val_data,batch_size,seq_len):\n",
        "\n",
        "                # One Hot Encode incoming data\n",
        "                x = one_hot_encoder(x,num_char)\n",
        "\n",
        "\n",
        "                # Convert Numpy Arrays to Tensor\n",
        "\n",
        "                inputs = torch.from_numpy(x)\n",
        "                targets = torch.from_numpy(y)\n",
        "\n",
        "                # Adjust for GPU if necessary\n",
        "\n",
        "                if model.use_gpu:\n",
        "\n",
        "                    inputs = inputs.cuda()\n",
        "                    targets = targets.cuda()\n",
        "\n",
        "                # Reset Hidden State\n",
        "                # If we dont' reset we would backpropagate through\n",
        "                # all training history\n",
        "                val_hidden = tuple([state.data for state in val_hidden])\n",
        "\n",
        "                lstm_output, val_hidden = model.forward(inputs,val_hidden)\n",
        "                val_loss = criterion(lstm_output,targets.view(batch_size*seq_len).long())\n",
        "\n",
        "                val_losses.append(val_loss.item())\n",
        "\n",
        "            # Reset to training model after val for loop\n",
        "            model.train()\n",
        "\n",
        "            print(f\"Epoch: {i} Step: {tracker} Val Loss: {val_loss.item()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "hcsYlCv-Fhe5",
        "outputId": "dd57a06e-1e82-4572-ca68-564503b307ad"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 Step: 25 Val Loss: 3.2165865898132324\n",
            "Epoch: 0 Step: 50 Val Loss: 3.2079741954803467\n",
            "Epoch: 0 Step: 75 Val Loss: 3.197408437728882\n",
            "Epoch: 0 Step: 100 Val Loss: 3.016463041305542\n",
            "Epoch: 0 Step: 125 Val Loss: 2.890205144882202\n",
            "Epoch: 0 Step: 150 Val Loss: 2.730475425720215\n",
            "Epoch: 0 Step: 175 Val Loss: 2.6213436126708984\n",
            "Epoch: 0 Step: 200 Val Loss: 2.496274471282959\n",
            "Epoch: 0 Step: 225 Val Loss: 2.358293056488037\n",
            "Epoch: 0 Step: 250 Val Loss: 2.2601816654205322\n",
            "Epoch: 0 Step: 275 Val Loss: 2.18794846534729\n",
            "Epoch: 0 Step: 300 Val Loss: 2.1369974613189697\n",
            "Epoch: 0 Step: 325 Val Loss: 2.0799341201782227\n",
            "Epoch: 0 Step: 350 Val Loss: 2.0396652221679688\n",
            "Epoch: 0 Step: 375 Val Loss: 2.0066161155700684\n",
            "Epoch: 1 Step: 400 Val Loss: 1.9725703001022339\n",
            "Epoch: 1 Step: 425 Val Loss: 1.9368088245391846\n",
            "Epoch: 1 Step: 450 Val Loss: 1.8988516330718994\n",
            "Epoch: 1 Step: 475 Val Loss: 1.8748763799667358\n",
            "Epoch: 1 Step: 500 Val Loss: 1.8434561491012573\n",
            "Epoch: 1 Step: 525 Val Loss: 1.823357343673706\n",
            "Epoch: 1 Step: 550 Val Loss: 1.8001960515975952\n",
            "Epoch: 1 Step: 575 Val Loss: 1.7763476371765137\n",
            "Epoch: 1 Step: 600 Val Loss: 1.7575461864471436\n",
            "Epoch: 1 Step: 625 Val Loss: 1.746944546699524\n",
            "Epoch: 1 Step: 650 Val Loss: 1.736296534538269\n",
            "Epoch: 1 Step: 675 Val Loss: 1.7069209814071655\n",
            "Epoch: 1 Step: 700 Val Loss: 1.6887778043746948\n",
            "Epoch: 1 Step: 725 Val Loss: 1.687893271446228\n",
            "Epoch: 1 Step: 750 Val Loss: 1.6674805879592896\n",
            "Epoch: 2 Step: 775 Val Loss: 1.6512013673782349\n",
            "Epoch: 2 Step: 800 Val Loss: 1.636569619178772\n",
            "Epoch: 2 Step: 825 Val Loss: 1.627150535583496\n",
            "Epoch: 2 Step: 850 Val Loss: 1.6173107624053955\n",
            "Epoch: 2 Step: 875 Val Loss: 1.6025567054748535\n",
            "Epoch: 2 Step: 900 Val Loss: 1.5933806896209717\n",
            "Epoch: 2 Step: 925 Val Loss: 1.5827057361602783\n",
            "Epoch: 2 Step: 950 Val Loss: 1.5719717741012573\n",
            "Epoch: 2 Step: 975 Val Loss: 1.561040997505188\n",
            "Epoch: 2 Step: 1000 Val Loss: 1.5481425523757935\n",
            "Epoch: 2 Step: 1025 Val Loss: 1.541501522064209\n",
            "Epoch: 2 Step: 1050 Val Loss: 1.5332293510437012\n",
            "Epoch: 2 Step: 1075 Val Loss: 1.5238473415374756\n",
            "Epoch: 2 Step: 1100 Val Loss: 1.515676736831665\n",
            "Epoch: 2 Step: 1125 Val Loss: 1.5111486911773682\n",
            "Epoch: 3 Step: 1150 Val Loss: 1.512058138847351\n",
            "Epoch: 3 Step: 1175 Val Loss: 1.5021543502807617\n",
            "Epoch: 3 Step: 1200 Val Loss: 1.494148850440979\n",
            "Epoch: 3 Step: 1225 Val Loss: 1.4883170127868652\n",
            "Epoch: 3 Step: 1250 Val Loss: 1.4785759449005127\n",
            "Epoch: 3 Step: 1275 Val Loss: 1.4824222326278687\n",
            "Epoch: 3 Step: 1300 Val Loss: 1.4747461080551147\n",
            "Epoch: 3 Step: 1325 Val Loss: 1.4686753749847412\n",
            "Epoch: 3 Step: 1350 Val Loss: 1.463896632194519\n",
            "Epoch: 3 Step: 1375 Val Loss: 1.45955228805542\n",
            "Epoch: 3 Step: 1400 Val Loss: 1.455826997756958\n",
            "Epoch: 3 Step: 1425 Val Loss: 1.4519340991973877\n",
            "Epoch: 3 Step: 1450 Val Loss: 1.44571852684021\n",
            "Epoch: 3 Step: 1475 Val Loss: 1.44211745262146\n",
            "Epoch: 3 Step: 1500 Val Loss: 1.4416922330856323\n",
            "Epoch: 3 Step: 1525 Val Loss: 1.437747836112976\n",
            "Epoch: 4 Step: 1550 Val Loss: 1.4399958848953247\n",
            "Epoch: 4 Step: 1575 Val Loss: 1.4322696924209595\n",
            "Epoch: 4 Step: 1600 Val Loss: 1.4272124767303467\n",
            "Epoch: 4 Step: 1625 Val Loss: 1.4253405332565308\n",
            "Epoch: 4 Step: 1650 Val Loss: 1.4179669618606567\n",
            "Epoch: 4 Step: 1675 Val Loss: 1.417121410369873\n",
            "Epoch: 4 Step: 1700 Val Loss: 1.411545753479004\n",
            "Epoch: 4 Step: 1725 Val Loss: 1.41045343875885\n",
            "Epoch: 4 Step: 1750 Val Loss: 1.405280590057373\n",
            "Epoch: 4 Step: 1775 Val Loss: 1.403340458869934\n",
            "Epoch: 4 Step: 1800 Val Loss: 1.4028244018554688\n",
            "Epoch: 4 Step: 1825 Val Loss: 1.3980457782745361\n",
            "Epoch: 4 Step: 1850 Val Loss: 1.3984745740890503\n",
            "Epoch: 4 Step: 1875 Val Loss: 1.3915526866912842\n",
            "Epoch: 4 Step: 1900 Val Loss: 1.391859531402588\n",
            "Epoch: 5 Step: 1925 Val Loss: 1.395865797996521\n",
            "Epoch: 5 Step: 1950 Val Loss: 1.3874984979629517\n",
            "Epoch: 5 Step: 1975 Val Loss: 1.3872382640838623\n",
            "Epoch: 5 Step: 2000 Val Loss: 1.3820691108703613\n",
            "Epoch: 5 Step: 2025 Val Loss: 1.38478684425354\n",
            "Epoch: 5 Step: 2050 Val Loss: 1.3791102170944214\n",
            "Epoch: 5 Step: 2075 Val Loss: 1.3758255243301392\n",
            "Epoch: 5 Step: 2100 Val Loss: 1.37233304977417\n",
            "Epoch: 5 Step: 2125 Val Loss: 1.3716179132461548\n",
            "Epoch: 5 Step: 2150 Val Loss: 1.3667982816696167\n",
            "Epoch: 5 Step: 2175 Val Loss: 1.3690769672393799\n",
            "Epoch: 5 Step: 2200 Val Loss: 1.3710246086120605\n",
            "Epoch: 5 Step: 2225 Val Loss: 1.3683193922042847\n",
            "Epoch: 5 Step: 2250 Val Loss: 1.3623442649841309\n",
            "Epoch: 5 Step: 2275 Val Loss: 1.3649085760116577\n",
            "Epoch: 6 Step: 2300 Val Loss: 1.366387963294983\n",
            "Epoch: 6 Step: 2325 Val Loss: 1.3663249015808105\n",
            "Epoch: 6 Step: 2350 Val Loss: 1.358309268951416\n",
            "Epoch: 6 Step: 2375 Val Loss: 1.351953148841858\n",
            "Epoch: 6 Step: 2400 Val Loss: 1.3541935682296753\n",
            "Epoch: 6 Step: 2425 Val Loss: 1.353183627128601\n",
            "Epoch: 6 Step: 2450 Val Loss: 1.3508765697479248\n",
            "Epoch: 6 Step: 2475 Val Loss: 1.34845769405365\n",
            "Epoch: 6 Step: 2500 Val Loss: 1.3542362451553345\n",
            "Epoch: 6 Step: 2525 Val Loss: 1.3487985134124756\n",
            "Epoch: 6 Step: 2550 Val Loss: 1.348442554473877\n",
            "Epoch: 6 Step: 2575 Val Loss: 1.3476687669754028\n",
            "Epoch: 6 Step: 2600 Val Loss: 1.346285343170166\n",
            "Epoch: 6 Step: 2625 Val Loss: 1.3435168266296387\n",
            "Epoch: 6 Step: 2650 Val Loss: 1.3458997011184692\n",
            "Epoch: 7 Step: 2675 Val Loss: 1.3421701192855835\n",
            "Epoch: 7 Step: 2700 Val Loss: 1.3419314622879028\n",
            "Epoch: 7 Step: 2725 Val Loss: 1.3431925773620605\n",
            "Epoch: 7 Step: 2750 Val Loss: 1.3410630226135254\n",
            "Epoch: 7 Step: 2775 Val Loss: 1.3371269702911377\n",
            "Epoch: 7 Step: 2800 Val Loss: 1.3305294513702393\n",
            "Epoch: 7 Step: 2825 Val Loss: 1.3322174549102783\n",
            "Epoch: 7 Step: 2850 Val Loss: 1.3304762840270996\n",
            "Epoch: 7 Step: 2875 Val Loss: 1.3318006992340088\n",
            "Epoch: 7 Step: 2900 Val Loss: 1.3289598226547241\n",
            "Epoch: 7 Step: 2925 Val Loss: 1.331395149230957\n",
            "Epoch: 7 Step: 2950 Val Loss: 1.3278776407241821\n",
            "Epoch: 7 Step: 2975 Val Loss: 1.3284204006195068\n",
            "Epoch: 7 Step: 3000 Val Loss: 1.3238694667816162\n",
            "Epoch: 7 Step: 3025 Val Loss: 1.325808048248291\n",
            "Epoch: 7 Step: 3050 Val Loss: 1.320366621017456\n",
            "Epoch: 8 Step: 3075 Val Loss: 1.324988603591919\n",
            "Epoch: 8 Step: 3100 Val Loss: 1.3201994895935059\n",
            "Epoch: 8 Step: 3125 Val Loss: 1.3191791772842407\n",
            "Epoch: 8 Step: 3150 Val Loss: 1.3196853399276733\n",
            "Epoch: 8 Step: 3175 Val Loss: 1.3205580711364746\n",
            "Epoch: 8 Step: 3200 Val Loss: 1.3182687759399414\n",
            "Epoch: 8 Step: 3225 Val Loss: 1.3144513368606567\n",
            "Epoch: 8 Step: 3250 Val Loss: 1.3179521560668945\n",
            "Epoch: 8 Step: 3275 Val Loss: 1.3147382736206055\n",
            "Epoch: 8 Step: 3300 Val Loss: 1.3144865036010742\n",
            "Epoch: 8 Step: 3325 Val Loss: 1.315706729888916\n",
            "Epoch: 8 Step: 3350 Val Loss: 1.314405918121338\n",
            "Epoch: 8 Step: 3375 Val Loss: 1.3122612237930298\n",
            "Epoch: 8 Step: 3400 Val Loss: 1.3215101957321167\n",
            "Epoch: 8 Step: 3425 Val Loss: 1.3094786405563354\n",
            "Epoch: 9 Step: 3450 Val Loss: 1.3101601600646973\n",
            "Epoch: 9 Step: 3475 Val Loss: 1.3119333982467651\n",
            "Epoch: 9 Step: 3500 Val Loss: 1.3166197538375854\n",
            "Epoch: 9 Step: 3525 Val Loss: 1.3052852153778076\n",
            "Epoch: 9 Step: 3550 Val Loss: 1.311892032623291\n",
            "Epoch: 9 Step: 3575 Val Loss: 1.307410717010498\n",
            "Epoch: 9 Step: 3600 Val Loss: 1.306821584701538\n",
            "Epoch: 9 Step: 3625 Val Loss: 1.3057761192321777\n",
            "Epoch: 9 Step: 3650 Val Loss: 1.3095016479492188\n",
            "Epoch: 9 Step: 3675 Val Loss: 1.3064059019088745\n",
            "Epoch: 9 Step: 3700 Val Loss: 1.3059920072555542\n",
            "Epoch: 9 Step: 3725 Val Loss: 1.3050800561904907\n",
            "Epoch: 9 Step: 3750 Val Loss: 1.3036407232284546\n",
            "Epoch: 9 Step: 3775 Val Loss: 1.303318738937378\n",
            "Epoch: 9 Step: 3800 Val Loss: 1.3060693740844727\n",
            "Epoch: 10 Step: 3825 Val Loss: 1.3043750524520874\n",
            "Epoch: 10 Step: 3850 Val Loss: 1.3006625175476074\n",
            "Epoch: 10 Step: 3875 Val Loss: 1.3027164936065674\n",
            "Epoch: 10 Step: 3900 Val Loss: 1.299729824066162\n",
            "Epoch: 10 Step: 3925 Val Loss: 1.3004028797149658\n",
            "Epoch: 10 Step: 3950 Val Loss: 1.2959718704223633\n",
            "Epoch: 10 Step: 3975 Val Loss: 1.298816204071045\n",
            "Epoch: 10 Step: 4000 Val Loss: 1.2988998889923096\n",
            "Epoch: 10 Step: 4025 Val Loss: 1.2992571592330933\n",
            "Epoch: 10 Step: 4050 Val Loss: 1.296217679977417\n",
            "Epoch: 10 Step: 4075 Val Loss: 1.300631046295166\n",
            "Epoch: 10 Step: 4100 Val Loss: 1.296787977218628\n",
            "Epoch: 10 Step: 4125 Val Loss: 1.2971874475479126\n",
            "Epoch: 10 Step: 4150 Val Loss: 1.2969253063201904\n",
            "Epoch: 10 Step: 4175 Val Loss: 1.2987757921218872\n",
            "Epoch: 10 Step: 4200 Val Loss: 1.2944016456604004\n",
            "Epoch: 11 Step: 4225 Val Loss: 1.2953846454620361\n",
            "Epoch: 11 Step: 4250 Val Loss: 1.2964386940002441\n",
            "Epoch: 11 Step: 4275 Val Loss: 1.2943834066390991\n",
            "Epoch: 11 Step: 4300 Val Loss: 1.291642665863037\n",
            "Epoch: 11 Step: 4325 Val Loss: 1.287674069404602\n",
            "Epoch: 11 Step: 4350 Val Loss: 1.292209267616272\n",
            "Epoch: 11 Step: 4375 Val Loss: 1.292214035987854\n",
            "Epoch: 11 Step: 4400 Val Loss: 1.2925087213516235\n",
            "Epoch: 11 Step: 4425 Val Loss: 1.2879899740219116\n",
            "Epoch: 11 Step: 4450 Val Loss: 1.290388584136963\n",
            "Epoch: 11 Step: 4475 Val Loss: 1.2938385009765625\n",
            "Epoch: 11 Step: 4500 Val Loss: 1.2919299602508545\n",
            "Epoch: 11 Step: 4525 Val Loss: 1.292305588722229\n",
            "Epoch: 11 Step: 4550 Val Loss: 1.2921792268753052\n",
            "Epoch: 11 Step: 4575 Val Loss: 1.2853367328643799\n",
            "Epoch: 12 Step: 4600 Val Loss: 1.2908103466033936\n",
            "Epoch: 12 Step: 4625 Val Loss: 1.2931573390960693\n",
            "Epoch: 12 Step: 4650 Val Loss: 1.291849970817566\n",
            "Epoch: 12 Step: 4675 Val Loss: 1.2878608703613281\n",
            "Epoch: 12 Step: 4700 Val Loss: 1.2902836799621582\n",
            "Epoch: 12 Step: 4725 Val Loss: 1.2910953760147095\n",
            "Epoch: 12 Step: 4750 Val Loss: 1.287412166595459\n",
            "Epoch: 12 Step: 4775 Val Loss: 1.283745527267456\n",
            "Epoch: 12 Step: 4800 Val Loss: 1.2873469591140747\n",
            "Epoch: 12 Step: 4825 Val Loss: 1.2801764011383057\n",
            "Epoch: 12 Step: 4850 Val Loss: 1.2869961261749268\n",
            "Epoch: 12 Step: 4875 Val Loss: 1.29250168800354\n",
            "Epoch: 12 Step: 4900 Val Loss: 1.2855095863342285\n",
            "Epoch: 12 Step: 4925 Val Loss: 1.2816425561904907\n",
            "Epoch: 12 Step: 4950 Val Loss: 1.2807475328445435\n",
            "Epoch: 13 Step: 4975 Val Loss: 1.283854365348816\n",
            "Epoch: 13 Step: 5000 Val Loss: 1.285953402519226\n",
            "Epoch: 13 Step: 5025 Val Loss: 1.2857191562652588\n",
            "Epoch: 13 Step: 5050 Val Loss: 1.282425880432129\n",
            "Epoch: 13 Step: 5075 Val Loss: 1.286165714263916\n",
            "Epoch: 13 Step: 5100 Val Loss: 1.2867852449417114\n",
            "Epoch: 13 Step: 5125 Val Loss: 1.2815688848495483\n",
            "Epoch: 13 Step: 5150 Val Loss: 1.2816487550735474\n",
            "Epoch: 13 Step: 5175 Val Loss: 1.2833863496780396\n",
            "Epoch: 13 Step: 5200 Val Loss: 1.2867071628570557\n",
            "Epoch: 13 Step: 5225 Val Loss: 1.28580641746521\n",
            "Epoch: 13 Step: 5250 Val Loss: 1.2822129726409912\n",
            "Epoch: 13 Step: 5275 Val Loss: 1.2801284790039062\n",
            "Epoch: 13 Step: 5300 Val Loss: 1.281074047088623\n",
            "Epoch: 13 Step: 5325 Val Loss: 1.284554123878479\n",
            "Epoch: 14 Step: 5350 Val Loss: 1.2826097011566162\n",
            "Epoch: 14 Step: 5375 Val Loss: 1.2810120582580566\n",
            "Epoch: 14 Step: 5400 Val Loss: 1.282631278038025\n",
            "Epoch: 14 Step: 5425 Val Loss: 1.2819420099258423\n",
            "Epoch: 14 Step: 5450 Val Loss: 1.2801215648651123\n",
            "Epoch: 14 Step: 5475 Val Loss: 1.2784689664840698\n",
            "Epoch: 14 Step: 5500 Val Loss: 1.2739125490188599\n",
            "Epoch: 14 Step: 5525 Val Loss: 1.2728365659713745\n",
            "Epoch: 14 Step: 5550 Val Loss: 1.2748669385910034\n",
            "Epoch: 14 Step: 5575 Val Loss: 1.2770966291427612\n",
            "Epoch: 14 Step: 5600 Val Loss: 1.273818850517273\n",
            "Epoch: 14 Step: 5625 Val Loss: 1.2758328914642334\n",
            "Epoch: 14 Step: 5650 Val Loss: 1.273419976234436\n",
            "Epoch: 14 Step: 5675 Val Loss: 1.2739627361297607\n",
            "Epoch: 14 Step: 5700 Val Loss: 1.2802112102508545\n",
            "Epoch: 14 Step: 5725 Val Loss: 1.2762597799301147\n",
            "Epoch: 15 Step: 5750 Val Loss: 1.2738231420516968\n",
            "Epoch: 15 Step: 5775 Val Loss: 1.2760159969329834\n",
            "Epoch: 15 Step: 5800 Val Loss: 1.2755517959594727\n",
            "Epoch: 15 Step: 5825 Val Loss: 1.2746573686599731\n",
            "Epoch: 15 Step: 5850 Val Loss: 1.2806090116500854\n",
            "Epoch: 15 Step: 5875 Val Loss: 1.2786818742752075\n",
            "Epoch: 15 Step: 5900 Val Loss: 1.2746812105178833\n",
            "Epoch: 15 Step: 5925 Val Loss: 1.272382378578186\n",
            "Epoch: 15 Step: 5950 Val Loss: 1.2769395112991333\n",
            "Epoch: 15 Step: 5975 Val Loss: 1.2741098403930664\n",
            "Epoch: 15 Step: 6000 Val Loss: 1.2769227027893066\n",
            "Epoch: 15 Step: 6025 Val Loss: 1.2751305103302002\n",
            "Epoch: 15 Step: 6050 Val Loss: 1.273074746131897\n",
            "Epoch: 15 Step: 6075 Val Loss: 1.274653434753418\n",
            "Epoch: 15 Step: 6100 Val Loss: 1.2769209146499634\n",
            "Epoch: 16 Step: 6125 Val Loss: 1.2741658687591553\n",
            "Epoch: 16 Step: 6150 Val Loss: 1.2750868797302246\n",
            "Epoch: 16 Step: 6175 Val Loss: 1.2739760875701904\n",
            "Epoch: 16 Step: 6200 Val Loss: 1.2708852291107178\n",
            "Epoch: 16 Step: 6225 Val Loss: 1.2735753059387207\n",
            "Epoch: 16 Step: 6250 Val Loss: 1.2740113735198975\n",
            "Epoch: 16 Step: 6275 Val Loss: 1.270154356956482\n",
            "Epoch: 16 Step: 6300 Val Loss: 1.27510404586792\n",
            "Epoch: 16 Step: 6325 Val Loss: 1.272163987159729\n",
            "Epoch: 16 Step: 6350 Val Loss: 1.2732316255569458\n",
            "Epoch: 16 Step: 6375 Val Loss: 1.2706375122070312\n",
            "Epoch: 16 Step: 6400 Val Loss: 1.2704185247421265\n",
            "Epoch: 16 Step: 6425 Val Loss: 1.2680736780166626\n",
            "Epoch: 16 Step: 6450 Val Loss: 1.2721531391143799\n",
            "Epoch: 16 Step: 6475 Val Loss: 1.2762396335601807\n",
            "Epoch: 17 Step: 6500 Val Loss: 1.270564317703247\n",
            "Epoch: 17 Step: 6525 Val Loss: 1.2651845216751099\n",
            "Epoch: 17 Step: 6550 Val Loss: 1.2710992097854614\n",
            "Epoch: 17 Step: 6575 Val Loss: 1.2735300064086914\n",
            "Epoch: 17 Step: 6600 Val Loss: 1.2677959203720093\n",
            "Epoch: 17 Step: 6625 Val Loss: 1.266955852508545\n",
            "Epoch: 17 Step: 6650 Val Loss: 1.267918348312378\n",
            "Epoch: 17 Step: 6675 Val Loss: 1.2700963020324707\n",
            "Epoch: 17 Step: 6700 Val Loss: 1.271546721458435\n",
            "Epoch: 17 Step: 6725 Val Loss: 1.2703971862792969\n",
            "Epoch: 17 Step: 6750 Val Loss: 1.2692066431045532\n",
            "Epoch: 17 Step: 6775 Val Loss: 1.2689318656921387\n",
            "Epoch: 17 Step: 6800 Val Loss: 1.2685908079147339\n",
            "Epoch: 17 Step: 6825 Val Loss: 1.2693856954574585\n",
            "Epoch: 17 Step: 6850 Val Loss: 1.2718924283981323\n",
            "Epoch: 17 Step: 6875 Val Loss: 1.2676763534545898\n",
            "Epoch: 18 Step: 6900 Val Loss: 1.2695177793502808\n",
            "Epoch: 18 Step: 6925 Val Loss: 1.2659192085266113\n",
            "Epoch: 18 Step: 6950 Val Loss: 1.2658545970916748\n",
            "Epoch: 18 Step: 6975 Val Loss: 1.2661415338516235\n",
            "Epoch: 18 Step: 7000 Val Loss: 1.263940453529358\n",
            "Epoch: 18 Step: 7025 Val Loss: 1.2668297290802002\n",
            "Epoch: 18 Step: 7050 Val Loss: 1.268193244934082\n",
            "Epoch: 18 Step: 7075 Val Loss: 1.2656195163726807\n",
            "Epoch: 18 Step: 7100 Val Loss: 1.2703531980514526\n",
            "Epoch: 18 Step: 7125 Val Loss: 1.2675182819366455\n",
            "Epoch: 18 Step: 7150 Val Loss: 1.2660784721374512\n",
            "Epoch: 18 Step: 7175 Val Loss: 1.2678989171981812\n",
            "Epoch: 18 Step: 7200 Val Loss: 1.2641969919204712\n",
            "Epoch: 18 Step: 7225 Val Loss: 1.2676587104797363\n",
            "Epoch: 18 Step: 7250 Val Loss: 1.2664796113967896\n",
            "Epoch: 19 Step: 7275 Val Loss: 1.265473484992981\n",
            "Epoch: 19 Step: 7300 Val Loss: 1.2709612846374512\n",
            "Epoch: 19 Step: 7325 Val Loss: 1.2657400369644165\n",
            "Epoch: 19 Step: 7350 Val Loss: 1.2657718658447266\n",
            "Epoch: 19 Step: 7375 Val Loss: 1.2679954767227173\n",
            "Epoch: 19 Step: 7400 Val Loss: 1.2657954692840576\n",
            "Epoch: 19 Step: 7425 Val Loss: 1.2654032707214355\n",
            "Epoch: 19 Step: 7450 Val Loss: 1.2621983289718628\n",
            "Epoch: 19 Step: 7475 Val Loss: 1.2681128978729248\n",
            "Epoch: 19 Step: 7500 Val Loss: 1.2649654150009155\n",
            "Epoch: 19 Step: 7525 Val Loss: 1.2660706043243408\n",
            "Epoch: 19 Step: 7550 Val Loss: 1.264053463935852\n",
            "Epoch: 19 Step: 7575 Val Loss: 1.2654824256896973\n",
            "Epoch: 19 Step: 7600 Val Loss: 1.2641068696975708\n",
            "Epoch: 19 Step: 7625 Val Loss: 1.264696478843689\n",
            "Epoch: 20 Step: 7650 Val Loss: 1.2629406452178955\n",
            "Epoch: 20 Step: 7675 Val Loss: 1.2672001123428345\n",
            "Epoch: 20 Step: 7700 Val Loss: 1.2696810960769653\n",
            "Epoch: 20 Step: 7725 Val Loss: 1.2665492296218872\n",
            "Epoch: 20 Step: 7750 Val Loss: 1.2661112546920776\n",
            "Epoch: 20 Step: 7775 Val Loss: 1.2635188102722168\n",
            "Epoch: 20 Step: 7800 Val Loss: 1.2621333599090576\n",
            "Epoch: 20 Step: 7825 Val Loss: 1.2618769407272339\n",
            "Epoch: 20 Step: 7850 Val Loss: 1.2677267789840698\n",
            "Epoch: 20 Step: 7875 Val Loss: 1.2655704021453857\n",
            "Epoch: 20 Step: 7900 Val Loss: 1.265852451324463\n",
            "Epoch: 20 Step: 7925 Val Loss: 1.2638027667999268\n",
            "Epoch: 20 Step: 7950 Val Loss: 1.2638968229293823\n",
            "Epoch: 20 Step: 7975 Val Loss: 1.2684999704360962\n",
            "Epoch: 20 Step: 8000 Val Loss: 1.273659348487854\n",
            "Epoch: 21 Step: 8025 Val Loss: 1.266861081123352\n",
            "Epoch: 21 Step: 8050 Val Loss: 1.265160322189331\n",
            "Epoch: 21 Step: 8075 Val Loss: 1.2684316635131836\n",
            "Epoch: 21 Step: 8100 Val Loss: 1.2640926837921143\n",
            "Epoch: 21 Step: 8125 Val Loss: 1.2595123052597046\n",
            "Epoch: 21 Step: 8150 Val Loss: 1.2602077722549438\n",
            "Epoch: 21 Step: 8175 Val Loss: 1.2580832242965698\n",
            "Epoch: 21 Step: 8200 Val Loss: 1.258387565612793\n",
            "Epoch: 21 Step: 8225 Val Loss: 1.2612754106521606\n",
            "Epoch: 21 Step: 8250 Val Loss: 1.2643299102783203\n",
            "Epoch: 21 Step: 8275 Val Loss: 1.26131272315979\n",
            "Epoch: 21 Step: 8300 Val Loss: 1.2645974159240723\n",
            "Epoch: 21 Step: 8325 Val Loss: 1.2604470252990723\n",
            "Epoch: 21 Step: 8350 Val Loss: 1.259248971939087\n",
            "Epoch: 21 Step: 8375 Val Loss: 1.2604460716247559\n",
            "Epoch: 21 Step: 8400 Val Loss: 1.2641714811325073\n",
            "Epoch: 22 Step: 8425 Val Loss: 1.2586021423339844\n",
            "Epoch: 22 Step: 8450 Val Loss: 1.259597659111023\n",
            "Epoch: 22 Step: 8475 Val Loss: 1.2587093114852905\n",
            "Epoch: 22 Step: 8500 Val Loss: 1.2592371702194214\n",
            "Epoch: 22 Step: 8525 Val Loss: 1.258909821510315\n",
            "Epoch: 22 Step: 8550 Val Loss: 1.2573000192642212\n",
            "Epoch: 22 Step: 8575 Val Loss: 1.2595371007919312\n",
            "Epoch: 22 Step: 8600 Val Loss: 1.2598329782485962\n",
            "Epoch: 22 Step: 8625 Val Loss: 1.2600849866867065\n",
            "Epoch: 22 Step: 8650 Val Loss: 1.2646305561065674\n",
            "Epoch: 22 Step: 8675 Val Loss: 1.2640398740768433\n",
            "Epoch: 22 Step: 8700 Val Loss: 1.266614556312561\n",
            "Epoch: 22 Step: 8725 Val Loss: 1.2626084089279175\n",
            "Epoch: 22 Step: 8750 Val Loss: 1.2635444402694702\n",
            "Epoch: 22 Step: 8775 Val Loss: 1.2664344310760498\n",
            "Epoch: 23 Step: 8800 Val Loss: 1.2613545656204224\n",
            "Epoch: 23 Step: 8825 Val Loss: 1.2633756399154663\n",
            "Epoch: 23 Step: 8850 Val Loss: 1.2656763792037964\n",
            "Epoch: 23 Step: 8875 Val Loss: 1.2594317197799683\n",
            "Epoch: 23 Step: 8900 Val Loss: 1.2634155750274658\n",
            "Epoch: 23 Step: 8925 Val Loss: 1.2571853399276733\n",
            "Epoch: 23 Step: 8950 Val Loss: 1.2611732482910156\n",
            "Epoch: 23 Step: 8975 Val Loss: 1.2601356506347656\n",
            "Epoch: 23 Step: 9000 Val Loss: 1.261240005493164\n",
            "Epoch: 23 Step: 9025 Val Loss: 1.2610008716583252\n",
            "Epoch: 23 Step: 9050 Val Loss: 1.2581554651260376\n",
            "Epoch: 23 Step: 9075 Val Loss: 1.2601975202560425\n",
            "Epoch: 23 Step: 9100 Val Loss: 1.2588610649108887\n",
            "Epoch: 23 Step: 9125 Val Loss: 1.2594126462936401\n",
            "Epoch: 23 Step: 9150 Val Loss: 1.262307047843933\n",
            "Epoch: 24 Step: 9175 Val Loss: 1.263521671295166\n",
            "Epoch: 24 Step: 9200 Val Loss: 1.2576780319213867\n",
            "Epoch: 24 Step: 9225 Val Loss: 1.2618900537490845\n",
            "Epoch: 24 Step: 9250 Val Loss: 1.263452410697937\n",
            "Epoch: 24 Step: 9275 Val Loss: 1.2595707178115845\n",
            "Epoch: 24 Step: 9300 Val Loss: 1.2570253610610962\n",
            "Epoch: 24 Step: 9325 Val Loss: 1.2547236680984497\n",
            "Epoch: 24 Step: 9350 Val Loss: 1.261206865310669\n",
            "Epoch: 24 Step: 9375 Val Loss: 1.2591006755828857\n",
            "Epoch: 24 Step: 9400 Val Loss: 1.2588342428207397\n",
            "Epoch: 24 Step: 9425 Val Loss: 1.2607523202896118\n",
            "Epoch: 24 Step: 9450 Val Loss: 1.258576512336731\n",
            "Epoch: 24 Step: 9475 Val Loss: 1.257580280303955\n",
            "Epoch: 24 Step: 9500 Val Loss: 1.2591121196746826\n",
            "Epoch: 24 Step: 9525 Val Loss: 1.2614893913269043\n",
            "Epoch: 24 Step: 9550 Val Loss: 1.2607210874557495\n",
            "Epoch: 25 Step: 9575 Val Loss: 1.2570312023162842\n",
            "Epoch: 25 Step: 9600 Val Loss: 1.2605534791946411\n",
            "Epoch: 25 Step: 9625 Val Loss: 1.2604836225509644\n",
            "Epoch: 25 Step: 9650 Val Loss: 1.2588160037994385\n",
            "Epoch: 25 Step: 9675 Val Loss: 1.2539405822753906\n",
            "Epoch: 25 Step: 9700 Val Loss: 1.25429105758667\n",
            "Epoch: 25 Step: 9725 Val Loss: 1.2563830614089966\n",
            "Epoch: 25 Step: 9750 Val Loss: 1.2590076923370361\n",
            "Epoch: 25 Step: 9775 Val Loss: 1.2606725692749023\n",
            "Epoch: 25 Step: 9800 Val Loss: 1.2544810771942139\n",
            "Epoch: 25 Step: 9825 Val Loss: 1.2594945430755615\n",
            "Epoch: 25 Step: 9850 Val Loss: 1.2592982053756714\n",
            "Epoch: 25 Step: 9875 Val Loss: 1.2609485387802124\n",
            "Epoch: 25 Step: 9900 Val Loss: 1.2624480724334717\n",
            "Epoch: 25 Step: 9925 Val Loss: 1.2566901445388794\n",
            "Epoch: 26 Step: 9950 Val Loss: 1.25912344455719\n",
            "Epoch: 26 Step: 9975 Val Loss: 1.2639087438583374\n",
            "Epoch: 26 Step: 10000 Val Loss: 1.2641196250915527\n",
            "Epoch: 26 Step: 10025 Val Loss: 1.259904384613037\n",
            "Epoch: 26 Step: 10050 Val Loss: 1.2619701623916626\n",
            "Epoch: 26 Step: 10075 Val Loss: 1.2572152614593506\n",
            "Epoch: 26 Step: 10100 Val Loss: 1.2573281526565552\n",
            "Epoch: 26 Step: 10125 Val Loss: 1.255072832107544\n",
            "Epoch: 26 Step: 10150 Val Loss: 1.2565385103225708\n",
            "Epoch: 26 Step: 10175 Val Loss: 1.2543245553970337\n",
            "Epoch: 26 Step: 10200 Val Loss: 1.2559916973114014\n",
            "Epoch: 26 Step: 10225 Val Loss: 1.2585581541061401\n",
            "Epoch: 26 Step: 10250 Val Loss: 1.2565453052520752\n",
            "Epoch: 26 Step: 10275 Val Loss: 1.258151888847351\n",
            "Epoch: 26 Step: 10300 Val Loss: 1.2546766996383667\n",
            "Epoch: 27 Step: 10325 Val Loss: 1.2550952434539795\n",
            "Epoch: 27 Step: 10350 Val Loss: 1.259096622467041\n",
            "Epoch: 27 Step: 10375 Val Loss: 1.2605863809585571\n",
            "Epoch: 27 Step: 10400 Val Loss: 1.2545894384384155\n",
            "Epoch: 27 Step: 10425 Val Loss: 1.2579596042633057\n",
            "Epoch: 27 Step: 10450 Val Loss: 1.2548590898513794\n",
            "Epoch: 27 Step: 10475 Val Loss: 1.2544323205947876\n",
            "Epoch: 27 Step: 10500 Val Loss: 1.2532896995544434\n",
            "Epoch: 27 Step: 10525 Val Loss: 1.2573273181915283\n",
            "Epoch: 27 Step: 10550 Val Loss: 1.2603845596313477\n",
            "Epoch: 27 Step: 10575 Val Loss: 1.2607498168945312\n",
            "Epoch: 27 Step: 10600 Val Loss: 1.2618240118026733\n",
            "Epoch: 27 Step: 10625 Val Loss: 1.2564703226089478\n",
            "Epoch: 27 Step: 10650 Val Loss: 1.2596486806869507\n",
            "Epoch: 27 Step: 10675 Val Loss: 1.2595454454421997\n",
            "Epoch: 28 Step: 10700 Val Loss: 1.2602686882019043\n",
            "Epoch: 28 Step: 10725 Val Loss: 1.2629730701446533\n",
            "Epoch: 28 Step: 10750 Val Loss: 1.2613821029663086\n",
            "Epoch: 28 Step: 10775 Val Loss: 1.264224886894226\n",
            "Epoch: 28 Step: 10800 Val Loss: 1.2562110424041748\n",
            "Epoch: 28 Step: 10825 Val Loss: 1.2590619325637817\n",
            "Epoch: 28 Step: 10850 Val Loss: 1.2543632984161377\n",
            "Epoch: 28 Step: 10875 Val Loss: 1.256015658378601\n",
            "Epoch: 28 Step: 10900 Val Loss: 1.258103609085083\n",
            "Epoch: 28 Step: 10925 Val Loss: 1.2601826190948486\n",
            "Epoch: 28 Step: 10950 Val Loss: 1.2586978673934937\n",
            "Epoch: 28 Step: 10975 Val Loss: 1.260286808013916\n",
            "Epoch: 28 Step: 11000 Val Loss: 1.2555943727493286\n",
            "Epoch: 28 Step: 11025 Val Loss: 1.2531336545944214\n",
            "Epoch: 28 Step: 11050 Val Loss: 1.254145860671997\n",
            "Epoch: 28 Step: 11075 Val Loss: 1.2554261684417725\n",
            "Epoch: 29 Step: 11100 Val Loss: 1.2574697732925415\n",
            "Epoch: 29 Step: 11125 Val Loss: 1.2572729587554932\n",
            "Epoch: 29 Step: 11150 Val Loss: 1.2616991996765137\n",
            "Epoch: 29 Step: 11175 Val Loss: 1.256378412246704\n",
            "Epoch: 29 Step: 11200 Val Loss: 1.2572071552276611\n",
            "Epoch: 29 Step: 11225 Val Loss: 1.2565244436264038\n",
            "Epoch: 29 Step: 11250 Val Loss: 1.2571022510528564\n",
            "Epoch: 29 Step: 11275 Val Loss: 1.2530444860458374\n",
            "Epoch: 29 Step: 11300 Val Loss: 1.258104920387268\n",
            "Epoch: 29 Step: 11325 Val Loss: 1.259845495223999\n",
            "Epoch: 29 Step: 11350 Val Loss: 1.253717303276062\n",
            "Epoch: 29 Step: 11375 Val Loss: 1.261173963546753\n",
            "Epoch: 29 Step: 11400 Val Loss: 1.2567932605743408\n",
            "Epoch: 29 Step: 11425 Val Loss: 1.2600878477096558\n",
            "Epoch: 29 Step: 11450 Val Loss: 1.251710057258606\n",
            "Epoch: 30 Step: 11475 Val Loss: 1.2566308975219727\n",
            "Epoch: 30 Step: 11500 Val Loss: 1.25675368309021\n",
            "Epoch: 30 Step: 11525 Val Loss: 1.2581905126571655\n",
            "Epoch: 30 Step: 11550 Val Loss: 1.2528660297393799\n",
            "Epoch: 30 Step: 11575 Val Loss: 1.2564140558242798\n",
            "Epoch: 30 Step: 11600 Val Loss: 1.2507262229919434\n",
            "Epoch: 30 Step: 11625 Val Loss: 1.2484246492385864\n",
            "Epoch: 30 Step: 11650 Val Loss: 1.2527594566345215\n",
            "Epoch: 30 Step: 11675 Val Loss: 1.2537740468978882\n",
            "Epoch: 30 Step: 11700 Val Loss: 1.2522625923156738\n",
            "Epoch: 30 Step: 11725 Val Loss: 1.2512301206588745\n",
            "Epoch: 30 Step: 11750 Val Loss: 1.251631736755371\n",
            "Epoch: 30 Step: 11775 Val Loss: 1.2499761581420898\n",
            "Epoch: 30 Step: 11800 Val Loss: 1.2535738945007324\n",
            "Epoch: 30 Step: 11825 Val Loss: 1.2592946290969849\n",
            "Epoch: 31 Step: 11850 Val Loss: 1.2541919946670532\n",
            "Epoch: 31 Step: 11875 Val Loss: 1.255102276802063\n",
            "Epoch: 31 Step: 11900 Val Loss: 1.2551181316375732\n",
            "Epoch: 31 Step: 11925 Val Loss: 1.256006121635437\n",
            "Epoch: 31 Step: 11950 Val Loss: 1.2525982856750488\n",
            "Epoch: 31 Step: 11975 Val Loss: 1.253039002418518\n",
            "Epoch: 31 Step: 12000 Val Loss: 1.2513418197631836\n",
            "Epoch: 31 Step: 12025 Val Loss: 1.249466896057129\n",
            "Epoch: 31 Step: 12050 Val Loss: 1.2532453536987305\n",
            "Epoch: 31 Step: 12075 Val Loss: 1.2536777257919312\n",
            "Epoch: 31 Step: 12100 Val Loss: 1.2533661127090454\n",
            "Epoch: 31 Step: 12125 Val Loss: 1.2533072233200073\n",
            "Epoch: 31 Step: 12150 Val Loss: 1.2550218105316162\n",
            "Epoch: 31 Step: 12175 Val Loss: 1.2552247047424316\n",
            "Epoch: 31 Step: 12200 Val Loss: 1.2576299905776978\n",
            "Epoch: 32 Step: 12225 Val Loss: 1.2587558031082153\n",
            "Epoch: 32 Step: 12250 Val Loss: 1.2544668912887573\n",
            "Epoch: 32 Step: 12275 Val Loss: 1.2561050653457642\n",
            "Epoch: 32 Step: 12300 Val Loss: 1.2542804479599\n",
            "Epoch: 32 Step: 12325 Val Loss: 1.2532329559326172\n",
            "Epoch: 32 Step: 12350 Val Loss: 1.2519267797470093\n",
            "Epoch: 32 Step: 12375 Val Loss: 1.2533671855926514\n",
            "Epoch: 32 Step: 12400 Val Loss: 1.253830909729004\n",
            "Epoch: 32 Step: 12425 Val Loss: 1.253069519996643\n",
            "Epoch: 32 Step: 12450 Val Loss: 1.2586441040039062\n",
            "Epoch: 32 Step: 12475 Val Loss: 1.2534432411193848\n",
            "Epoch: 32 Step: 12500 Val Loss: 1.2530462741851807\n",
            "Epoch: 32 Step: 12525 Val Loss: 1.2518372535705566\n",
            "Epoch: 32 Step: 12550 Val Loss: 1.2520872354507446\n",
            "Epoch: 32 Step: 12575 Val Loss: 1.2557778358459473\n",
            "Epoch: 32 Step: 12600 Val Loss: 1.2521700859069824\n",
            "Epoch: 33 Step: 12625 Val Loss: 1.2534611225128174\n",
            "Epoch: 33 Step: 12650 Val Loss: 1.2565401792526245\n",
            "Epoch: 33 Step: 12675 Val Loss: 1.2550101280212402\n",
            "Epoch: 33 Step: 12700 Val Loss: 1.2532711029052734\n",
            "Epoch: 33 Step: 12725 Val Loss: 1.2579349279403687\n",
            "Epoch: 33 Step: 12750 Val Loss: 1.2503126859664917\n",
            "Epoch: 33 Step: 12775 Val Loss: 1.2515465021133423\n",
            "Epoch: 33 Step: 12800 Val Loss: 1.2530877590179443\n",
            "Epoch: 33 Step: 12825 Val Loss: 1.256485104560852\n",
            "Epoch: 33 Step: 12850 Val Loss: 1.2529246807098389\n",
            "Epoch: 33 Step: 12875 Val Loss: 1.2565053701400757\n",
            "Epoch: 33 Step: 12900 Val Loss: 1.2530226707458496\n",
            "Epoch: 33 Step: 12925 Val Loss: 1.2515748739242554\n",
            "Epoch: 33 Step: 12950 Val Loss: 1.256454586982727\n",
            "Epoch: 33 Step: 12975 Val Loss: 1.254805326461792\n",
            "Epoch: 34 Step: 13000 Val Loss: 1.255540132522583\n",
            "Epoch: 34 Step: 13025 Val Loss: 1.2538079023361206\n",
            "Epoch: 34 Step: 13050 Val Loss: 1.2524279356002808\n",
            "Epoch: 34 Step: 13075 Val Loss: 1.2531741857528687\n",
            "Epoch: 34 Step: 13100 Val Loss: 1.2565888166427612\n",
            "Epoch: 34 Step: 13125 Val Loss: 1.2483999729156494\n",
            "Epoch: 34 Step: 13150 Val Loss: 1.2556202411651611\n",
            "Epoch: 34 Step: 13175 Val Loss: 1.2531777620315552\n",
            "Epoch: 34 Step: 13200 Val Loss: 1.253185749053955\n",
            "Epoch: 34 Step: 13225 Val Loss: 1.2573193311691284\n",
            "Epoch: 34 Step: 13250 Val Loss: 1.2533584833145142\n",
            "Epoch: 34 Step: 13275 Val Loss: 1.2566779851913452\n",
            "Epoch: 34 Step: 13300 Val Loss: 1.2557063102722168\n",
            "Epoch: 34 Step: 13325 Val Loss: 1.2566012144088745\n",
            "Epoch: 34 Step: 13350 Val Loss: 1.2607665061950684\n",
            "Epoch: 35 Step: 13375 Val Loss: 1.2562131881713867\n",
            "Epoch: 35 Step: 13400 Val Loss: 1.256259560585022\n",
            "Epoch: 35 Step: 13425 Val Loss: 1.2591009140014648\n",
            "Epoch: 35 Step: 13450 Val Loss: 1.257215142250061\n",
            "Epoch: 35 Step: 13475 Val Loss: 1.256565809249878\n",
            "Epoch: 35 Step: 13500 Val Loss: 1.2539470195770264\n",
            "Epoch: 35 Step: 13525 Val Loss: 1.2512074708938599\n",
            "Epoch: 35 Step: 13550 Val Loss: 1.2576305866241455\n",
            "Epoch: 35 Step: 13575 Val Loss: 1.257408618927002\n",
            "Epoch: 35 Step: 13600 Val Loss: 1.259670615196228\n",
            "Epoch: 35 Step: 13625 Val Loss: 1.2546277046203613\n",
            "Epoch: 35 Step: 13650 Val Loss: 1.2566791772842407\n",
            "Epoch: 35 Step: 13675 Val Loss: 1.2543483972549438\n",
            "Epoch: 35 Step: 13700 Val Loss: 1.25287663936615\n",
            "Epoch: 35 Step: 13725 Val Loss: 1.2572938203811646\n",
            "Epoch: 35 Step: 13750 Val Loss: 1.2539961338043213\n",
            "Epoch: 36 Step: 13775 Val Loss: 1.2541651725769043\n",
            "Epoch: 36 Step: 13800 Val Loss: 1.2538127899169922\n",
            "Epoch: 36 Step: 13825 Val Loss: 1.2577574253082275\n",
            "Epoch: 36 Step: 13850 Val Loss: 1.2529268264770508\n",
            "Epoch: 36 Step: 13875 Val Loss: 1.2517926692962646\n",
            "Epoch: 36 Step: 13900 Val Loss: 1.2546253204345703\n",
            "Epoch: 36 Step: 13925 Val Loss: 1.2545311450958252\n",
            "Epoch: 36 Step: 13950 Val Loss: 1.2484965324401855\n",
            "Epoch: 36 Step: 13975 Val Loss: 1.2547444105148315\n",
            "Epoch: 36 Step: 14000 Val Loss: 1.255622148513794\n",
            "Epoch: 36 Step: 14025 Val Loss: 1.2545199394226074\n",
            "Epoch: 36 Step: 14050 Val Loss: 1.2578727006912231\n",
            "Epoch: 36 Step: 14075 Val Loss: 1.2574706077575684\n",
            "Epoch: 36 Step: 14100 Val Loss: 1.2605857849121094\n",
            "Epoch: 36 Step: 14125 Val Loss: 1.2538508176803589\n",
            "Epoch: 37 Step: 14150 Val Loss: 1.2564046382904053\n",
            "Epoch: 37 Step: 14175 Val Loss: 1.255884051322937\n",
            "Epoch: 37 Step: 14200 Val Loss: 1.2559294700622559\n",
            "Epoch: 37 Step: 14225 Val Loss: 1.252424955368042\n",
            "Epoch: 37 Step: 14250 Val Loss: 1.2553014755249023\n",
            "Epoch: 37 Step: 14275 Val Loss: 1.2526723146438599\n",
            "Epoch: 37 Step: 14300 Val Loss: 1.2543208599090576\n",
            "Epoch: 37 Step: 14325 Val Loss: 1.2478384971618652\n",
            "Epoch: 37 Step: 14350 Val Loss: 1.249863862991333\n",
            "Epoch: 37 Step: 14375 Val Loss: 1.2506908178329468\n",
            "Epoch: 37 Step: 14400 Val Loss: 1.2502624988555908\n",
            "Epoch: 37 Step: 14425 Val Loss: 1.2524197101593018\n",
            "Epoch: 37 Step: 14450 Val Loss: 1.2525252103805542\n",
            "Epoch: 37 Step: 14475 Val Loss: 1.254063367843628\n",
            "Epoch: 37 Step: 14500 Val Loss: 1.2579498291015625\n",
            "Epoch: 38 Step: 14525 Val Loss: 1.25421142578125\n",
            "Epoch: 38 Step: 14550 Val Loss: 1.256139874458313\n",
            "Epoch: 38 Step: 14575 Val Loss: 1.2592941522598267\n",
            "Epoch: 38 Step: 14600 Val Loss: 1.2612221240997314\n",
            "Epoch: 38 Step: 14625 Val Loss: 1.2560077905654907\n",
            "Epoch: 38 Step: 14650 Val Loss: 1.2548691034317017\n",
            "Epoch: 38 Step: 14675 Val Loss: 1.2510042190551758\n",
            "Epoch: 38 Step: 14700 Val Loss: 1.2526910305023193\n",
            "Epoch: 38 Step: 14725 Val Loss: 1.2540006637573242\n",
            "Epoch: 38 Step: 14750 Val Loss: 1.2522282600402832\n",
            "Epoch: 38 Step: 14775 Val Loss: 1.2528777122497559\n",
            "Epoch: 38 Step: 14800 Val Loss: 1.2560677528381348\n",
            "Epoch: 38 Step: 14825 Val Loss: 1.2562224864959717\n",
            "Epoch: 38 Step: 14850 Val Loss: 1.2543749809265137\n",
            "Epoch: 38 Step: 14875 Val Loss: 1.2536011934280396\n",
            "Epoch: 39 Step: 14900 Val Loss: 1.2566124200820923\n",
            "Epoch: 39 Step: 14925 Val Loss: 1.2554584741592407\n",
            "Epoch: 39 Step: 14950 Val Loss: 1.255561113357544\n",
            "Epoch: 39 Step: 14975 Val Loss: 1.2555558681488037\n",
            "Epoch: 39 Step: 15000 Val Loss: 1.2525672912597656\n",
            "Epoch: 39 Step: 15025 Val Loss: 1.2555639743804932\n",
            "Epoch: 39 Step: 15050 Val Loss: 1.253919243812561\n",
            "Epoch: 39 Step: 15075 Val Loss: 1.252898931503296\n",
            "Epoch: 39 Step: 15100 Val Loss: 1.2530148029327393\n",
            "Epoch: 39 Step: 15125 Val Loss: 1.2582595348358154\n",
            "Epoch: 39 Step: 15150 Val Loss: 1.2499018907546997\n",
            "Epoch: 39 Step: 15175 Val Loss: 1.2523036003112793\n",
            "Epoch: 39 Step: 15200 Val Loss: 1.2540611028671265\n",
            "Epoch: 39 Step: 15225 Val Loss: 1.2513576745986938\n",
            "Epoch: 39 Step: 15250 Val Loss: 1.2559363842010498\n",
            "Epoch: 39 Step: 15275 Val Loss: 1.251242756843567\n",
            "Epoch: 40 Step: 15300 Val Loss: 1.2511372566223145\n",
            "Epoch: 40 Step: 15325 Val Loss: 1.259407877922058\n",
            "Epoch: 40 Step: 15350 Val Loss: 1.2519304752349854\n",
            "Epoch: 40 Step: 15375 Val Loss: 1.252713680267334\n",
            "Epoch: 40 Step: 15400 Val Loss: 1.253800868988037\n",
            "Epoch: 40 Step: 15425 Val Loss: 1.2483786344528198\n",
            "Epoch: 40 Step: 15450 Val Loss: 1.2485636472702026\n",
            "Epoch: 40 Step: 15475 Val Loss: 1.248679757118225\n",
            "Epoch: 40 Step: 15500 Val Loss: 1.2515689134597778\n",
            "Epoch: 40 Step: 15525 Val Loss: 1.2494680881500244\n",
            "Epoch: 40 Step: 15550 Val Loss: 1.2534540891647339\n",
            "Epoch: 40 Step: 15575 Val Loss: 1.2541674375534058\n",
            "Epoch: 40 Step: 15600 Val Loss: 1.251939296722412\n",
            "Epoch: 40 Step: 15625 Val Loss: 1.2594451904296875\n",
            "Epoch: 40 Step: 15650 Val Loss: 1.254145622253418\n",
            "Epoch: 41 Step: 15675 Val Loss: 1.2566546201705933\n",
            "Epoch: 41 Step: 15700 Val Loss: 1.254917025566101\n",
            "Epoch: 41 Step: 15725 Val Loss: 1.25583815574646\n",
            "Epoch: 41 Step: 15750 Val Loss: 1.251794457435608\n",
            "Epoch: 41 Step: 15775 Val Loss: 1.2541545629501343\n",
            "Epoch: 41 Step: 15800 Val Loss: 1.2484561204910278\n",
            "Epoch: 41 Step: 15825 Val Loss: 1.2533835172653198\n",
            "Epoch: 41 Step: 15850 Val Loss: 1.2504466772079468\n",
            "Epoch: 41 Step: 15875 Val Loss: 1.2497146129608154\n",
            "Epoch: 41 Step: 15900 Val Loss: 1.2519866228103638\n",
            "Epoch: 41 Step: 15925 Val Loss: 1.250279188156128\n",
            "Epoch: 41 Step: 15950 Val Loss: 1.254032850265503\n",
            "Epoch: 41 Step: 15975 Val Loss: 1.2539780139923096\n",
            "Epoch: 41 Step: 16000 Val Loss: 1.257606029510498\n",
            "Epoch: 41 Step: 16025 Val Loss: 1.25856351852417\n",
            "Epoch: 42 Step: 16050 Val Loss: 1.257169485092163\n",
            "Epoch: 42 Step: 16075 Val Loss: 1.2575451135635376\n",
            "Epoch: 42 Step: 16100 Val Loss: 1.260127067565918\n",
            "Epoch: 42 Step: 16125 Val Loss: 1.255163311958313\n",
            "Epoch: 42 Step: 16150 Val Loss: 1.2526443004608154\n",
            "Epoch: 42 Step: 16175 Val Loss: 1.250569462776184\n",
            "Epoch: 42 Step: 16200 Val Loss: 1.2536542415618896\n",
            "Epoch: 42 Step: 16225 Val Loss: 1.2508152723312378\n",
            "Epoch: 42 Step: 16250 Val Loss: 1.2522666454315186\n",
            "Epoch: 42 Step: 16275 Val Loss: 1.2492351531982422\n",
            "Epoch: 42 Step: 16300 Val Loss: 1.2516558170318604\n",
            "Epoch: 42 Step: 16325 Val Loss: 1.2566384077072144\n",
            "Epoch: 42 Step: 16350 Val Loss: 1.2542853355407715\n",
            "Epoch: 42 Step: 16375 Val Loss: 1.254846453666687\n",
            "Epoch: 42 Step: 16400 Val Loss: 1.2560231685638428\n",
            "Epoch: 42 Step: 16425 Val Loss: 1.2578085660934448\n",
            "Epoch: 43 Step: 16450 Val Loss: 1.2567187547683716\n",
            "Epoch: 43 Step: 16475 Val Loss: 1.2549926042556763\n",
            "Epoch: 43 Step: 16500 Val Loss: 1.2538896799087524\n",
            "Epoch: 43 Step: 16525 Val Loss: 1.2495442628860474\n",
            "Epoch: 43 Step: 16550 Val Loss: 1.2516016960144043\n",
            "Epoch: 43 Step: 16575 Val Loss: 1.24984872341156\n",
            "Epoch: 43 Step: 16600 Val Loss: 1.250917673110962\n",
            "Epoch: 43 Step: 16625 Val Loss: 1.2523167133331299\n",
            "Epoch: 43 Step: 16650 Val Loss: 1.2562060356140137\n",
            "Epoch: 43 Step: 16675 Val Loss: 1.2514352798461914\n",
            "Epoch: 43 Step: 16700 Val Loss: 1.2534780502319336\n",
            "Epoch: 43 Step: 16725 Val Loss: 1.258690357208252\n",
            "Epoch: 43 Step: 16750 Val Loss: 1.2536532878875732\n",
            "Epoch: 43 Step: 16775 Val Loss: 1.2602707147598267\n",
            "Epoch: 43 Step: 16800 Val Loss: 1.2548558712005615\n",
            "Epoch: 44 Step: 16825 Val Loss: 1.256648063659668\n",
            "Epoch: 44 Step: 16850 Val Loss: 1.2571901082992554\n",
            "Epoch: 44 Step: 16875 Val Loss: 1.258502721786499\n",
            "Epoch: 44 Step: 16900 Val Loss: 1.251631498336792\n",
            "Epoch: 44 Step: 16925 Val Loss: 1.2548507452011108\n",
            "Epoch: 44 Step: 16950 Val Loss: 1.2476694583892822\n",
            "Epoch: 44 Step: 16975 Val Loss: 1.2513563632965088\n",
            "Epoch: 44 Step: 17000 Val Loss: 1.2536789178848267\n",
            "Epoch: 44 Step: 17025 Val Loss: 1.2507160902023315\n",
            "Epoch: 44 Step: 17050 Val Loss: 1.2494525909423828\n",
            "Epoch: 44 Step: 17075 Val Loss: 1.2531706094741821\n",
            "Epoch: 44 Step: 17100 Val Loss: 1.2549620866775513\n",
            "Epoch: 44 Step: 17125 Val Loss: 1.2504240274429321\n",
            "Epoch: 44 Step: 17150 Val Loss: 1.2543690204620361\n",
            "Epoch: 44 Step: 17175 Val Loss: 1.2574176788330078\n",
            "Epoch: 45 Step: 17200 Val Loss: 1.2511131763458252\n",
            "Epoch: 45 Step: 17225 Val Loss: 1.2572230100631714\n",
            "Epoch: 45 Step: 17250 Val Loss: 1.2532107830047607\n",
            "Epoch: 45 Step: 17275 Val Loss: 1.251396894454956\n",
            "Epoch: 45 Step: 17300 Val Loss: 1.2472480535507202\n",
            "Epoch: 45 Step: 17325 Val Loss: 1.2504849433898926\n",
            "Epoch: 45 Step: 17350 Val Loss: 1.2462631464004517\n",
            "Epoch: 45 Step: 17375 Val Loss: 1.2525737285614014\n",
            "Epoch: 45 Step: 17400 Val Loss: 1.2528371810913086\n",
            "Epoch: 45 Step: 17425 Val Loss: 1.2535195350646973\n",
            "Epoch: 45 Step: 17450 Val Loss: 1.2573617696762085\n",
            "Epoch: 45 Step: 17475 Val Loss: 1.2550050020217896\n",
            "Epoch: 45 Step: 17500 Val Loss: 1.252236008644104\n",
            "Epoch: 45 Step: 17525 Val Loss: 1.252182126045227\n",
            "Epoch: 45 Step: 17550 Val Loss: 1.2517025470733643\n",
            "Epoch: 46 Step: 17575 Val Loss: 1.2533466815948486\n",
            "Epoch: 46 Step: 17600 Val Loss: 1.2531490325927734\n",
            "Epoch: 46 Step: 17625 Val Loss: 1.2525660991668701\n",
            "Epoch: 46 Step: 17650 Val Loss: 1.2534502744674683\n",
            "Epoch: 46 Step: 17675 Val Loss: 1.2534821033477783\n",
            "Epoch: 46 Step: 17700 Val Loss: 1.2501282691955566\n",
            "Epoch: 46 Step: 17725 Val Loss: 1.24916672706604\n",
            "Epoch: 46 Step: 17750 Val Loss: 1.2483820915222168\n",
            "Epoch: 46 Step: 17775 Val Loss: 1.2507778406143188\n",
            "Epoch: 46 Step: 17800 Val Loss: 1.2514721155166626\n",
            "Epoch: 46 Step: 17825 Val Loss: 1.250037670135498\n",
            "Epoch: 46 Step: 17850 Val Loss: 1.251101016998291\n",
            "Epoch: 46 Step: 17875 Val Loss: 1.254305362701416\n",
            "Epoch: 46 Step: 17900 Val Loss: 1.2500290870666504\n",
            "Epoch: 46 Step: 17925 Val Loss: 1.2519476413726807\n",
            "Epoch: 46 Step: 17950 Val Loss: 1.2490265369415283\n",
            "Epoch: 47 Step: 17975 Val Loss: 1.250780701637268\n",
            "Epoch: 47 Step: 18000 Val Loss: 1.2998356819152832\n",
            "Epoch: 47 Step: 18025 Val Loss: 1.2630406618118286\n",
            "Epoch: 47 Step: 18050 Val Loss: 1.2555663585662842\n",
            "Epoch: 47 Step: 18075 Val Loss: 1.2482080459594727\n",
            "Epoch: 47 Step: 18100 Val Loss: 1.241780400276184\n",
            "Epoch: 47 Step: 18125 Val Loss: 1.242751955986023\n",
            "Epoch: 47 Step: 18150 Val Loss: 1.2448432445526123\n",
            "Epoch: 47 Step: 18175 Val Loss: 1.2453277111053467\n",
            "Epoch: 47 Step: 18200 Val Loss: 1.2452468872070312\n",
            "Epoch: 47 Step: 18225 Val Loss: 1.2478691339492798\n",
            "Epoch: 47 Step: 18250 Val Loss: 1.2509541511535645\n",
            "Epoch: 47 Step: 18275 Val Loss: 1.2468445301055908\n",
            "Epoch: 47 Step: 18300 Val Loss: 1.2537916898727417\n",
            "Epoch: 47 Step: 18325 Val Loss: 1.2514832019805908\n",
            "Epoch: 48 Step: 18350 Val Loss: 1.2502493858337402\n",
            "Epoch: 48 Step: 18375 Val Loss: 1.2502399682998657\n",
            "Epoch: 48 Step: 18400 Val Loss: 1.252998948097229\n",
            "Epoch: 48 Step: 18425 Val Loss: 1.2498308420181274\n",
            "Epoch: 48 Step: 18450 Val Loss: 1.2497411966323853\n",
            "Epoch: 48 Step: 18475 Val Loss: 1.249955654144287\n",
            "Epoch: 48 Step: 18500 Val Loss: 1.2469836473464966\n",
            "Epoch: 48 Step: 18525 Val Loss: 1.2489988803863525\n",
            "Epoch: 48 Step: 18550 Val Loss: 1.2487406730651855\n",
            "Epoch: 48 Step: 18575 Val Loss: 1.251586675643921\n",
            "Epoch: 48 Step: 18600 Val Loss: 1.2511972188949585\n",
            "Epoch: 48 Step: 18625 Val Loss: 1.2527203559875488\n",
            "Epoch: 48 Step: 18650 Val Loss: 1.2490909099578857\n",
            "Epoch: 48 Step: 18675 Val Loss: 1.2508931159973145\n",
            "Epoch: 48 Step: 18700 Val Loss: 1.2555903196334839\n",
            "Epoch: 49 Step: 18725 Val Loss: 1.2523517608642578\n",
            "Epoch: 49 Step: 18750 Val Loss: 1.25034499168396\n",
            "Epoch: 49 Step: 18775 Val Loss: 1.2523698806762695\n",
            "Epoch: 49 Step: 18800 Val Loss: 1.2511082887649536\n",
            "Epoch: 49 Step: 18825 Val Loss: 1.2515153884887695\n",
            "Epoch: 49 Step: 18850 Val Loss: 1.247024655342102\n",
            "Epoch: 49 Step: 18875 Val Loss: 1.248870611190796\n",
            "Epoch: 49 Step: 18900 Val Loss: 1.2528802156448364\n",
            "Epoch: 49 Step: 18925 Val Loss: 1.2520346641540527\n",
            "Epoch: 49 Step: 18950 Val Loss: 1.2522038221359253\n",
            "Epoch: 49 Step: 18975 Val Loss: 1.2517075538635254\n",
            "Epoch: 49 Step: 19000 Val Loss: 1.2535920143127441\n",
            "Epoch: 49 Step: 19025 Val Loss: 1.2540684938430786\n",
            "Epoch: 49 Step: 19050 Val Loss: 1.2517874240875244\n",
            "Epoch: 49 Step: 19075 Val Loss: 1.257283091545105\n",
            "Epoch: 49 Step: 19100 Val Loss: 1.2490932941436768\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model_name = 'example.net'"
      ],
      "metadata": {
        "id": "n7j3GI5zFk4H"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(),model_name)"
      ],
      "metadata": {
        "id": "ALPQHX-3YMdL"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Model"
      ],
      "metadata": {
        "id": "oIvyjhKqYRcp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = CharModel(\n",
        "    all_chars=all_characters,\n",
        "    num_hidden=512,\n",
        "    num_layers=3,\n",
        "    drop_prob=0.5,\n",
        "    use_gpu=True,\n",
        ")"
      ],
      "metadata": {
        "id": "PgJuawlUYPBa"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load(model_name))\n",
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LBg0QeWWYVU-",
        "outputId": "c5f942f1-f898-4f21-8c3b-e9552fc4f243"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-47-35d2fda4df86>:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(model_name))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CharModel(\n",
              "  (lstm): LSTM(84, 512, num_layers=3, batch_first=True, dropout=0.5)\n",
              "  (dropout): Dropout(p=0.5, inplace=False)\n",
              "  (fc_linear): Linear(in_features=512, out_features=84, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generating Predictions"
      ],
      "metadata": {
        "id": "e2WAaB8zYcCe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_next_char(model, char, hidden=None, k=1):\n",
        "\n",
        "        # Encode raw letters with model\n",
        "        encoded_text = model.encoder[char]\n",
        "\n",
        "        # set as numpy array for one hot encoding\n",
        "        # NOTE THE [[ ]] dimensions!!\n",
        "        encoded_text = np.array([[encoded_text]])\n",
        "\n",
        "        # One hot encoding\n",
        "        encoded_text = one_hot_encoder(encoded_text, len(model.all_chars))\n",
        "\n",
        "        # Convert to Tensor\n",
        "        inputs = torch.from_numpy(encoded_text)\n",
        "\n",
        "        # Check for CPU\n",
        "        if(model.use_gpu):\n",
        "            inputs = inputs.cuda()\n",
        "\n",
        "\n",
        "        # Grab hidden states\n",
        "        hidden = tuple([state.data for state in hidden])\n",
        "\n",
        "\n",
        "        # Run model and get predicted output\n",
        "        lstm_out, hidden = model(inputs, hidden)\n",
        "\n",
        "\n",
        "        # Convert lstm_out to probabilities\n",
        "        probs = F.softmax(lstm_out, dim=1).data\n",
        "\n",
        "\n",
        "\n",
        "        if(model.use_gpu):\n",
        "            # move back to CPU to use with numpy\n",
        "            probs = probs.cpu()\n",
        "\n",
        "\n",
        "        # k determines how many characters to consider\n",
        "        # for our probability choice.\n",
        "        # https://pytorch.org/docs/stable/torch.html#torch.topk\n",
        "\n",
        "        # Return k largest probabilities in tensor\n",
        "        probs, index_positions = probs.topk(k)\n",
        "\n",
        "\n",
        "        index_positions = index_positions.numpy().squeeze()\n",
        "\n",
        "        # Create array of probabilities\n",
        "        probs = probs.numpy().flatten()\n",
        "\n",
        "        # Convert to probabilities per index\n",
        "        probs = probs/probs.sum()\n",
        "\n",
        "        # randomly choose a character based on probabilities\n",
        "        char = np.random.choice(index_positions, p=probs)\n",
        "\n",
        "        # return the encoded value of the predicted char and the hidden state\n",
        "        return model.decoder[char], hidden"
      ],
      "metadata": {
        "id": "GqejA69OYXWw"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model, size, seed='The', k=1):\n",
        "\n",
        "\n",
        "\n",
        "    # CHECK FOR GPU\n",
        "    if(model.use_gpu):\n",
        "        model.cuda()\n",
        "    else:\n",
        "        model.cpu()\n",
        "\n",
        "    # Evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # begin output from initial seed\n",
        "    output_chars = [c for c in seed]\n",
        "\n",
        "    # intiate hidden state\n",
        "    hidden = model.hidden_state(1)\n",
        "\n",
        "    # predict the next character for every character in seed\n",
        "    for char in seed:\n",
        "        char, hidden = predict_next_char(model, char, hidden, k=k)\n",
        "\n",
        "    # add initial characters to output\n",
        "    output_chars.append(char)\n",
        "\n",
        "    # Now generate for size requested\n",
        "    for i in range(size):\n",
        "\n",
        "        # predict based off very last letter in output_chars\n",
        "        char, hidden = predict_next_char(model, output_chars[-1], hidden, k=k)\n",
        "\n",
        "        # add predicted character\n",
        "        output_chars.append(char)\n",
        "\n",
        "    # return string of predicted text\n",
        "    return ''.join(output_chars)"
      ],
      "metadata": {
        "id": "34v5mGZDYeeT"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(generate_text(model, 1000, seed='The ', k=3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wvN3FoU8YhuN",
        "outputId": "6eab41b0-9339-46fe-ff81-5c59dee06691"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The prophecies of two of the walls of the carping souls\n",
            "    That the state of thy fool and souls are bloody\n",
            "    That he hath bought thyself and me a man,\n",
            "    To straight be best and truth than thou shalt see,\n",
            "    And then, as to this charge, the seasons of him,\n",
            "    Too store of such a mere than how I saw\n",
            "    This foul displace the court of honour to his son,\n",
            "    The tribunes and the state, and the more serves\n",
            "    That she will be the state. If to be strange\n",
            "    Were such a soul to think, that the bosom of\n",
            "    The strength of men as the sun shall set her bed\n",
            "    To shallow stirs to see the world of this.\n",
            "    If thou hadst been thine eyes and there are strength\n",
            "    As with his self-a service and all manner\n",
            "    As this defence of hearts shall be an hour\n",
            "    To stay a man to bring thee with my heart.\n",
            "    Where's the more storm and to be treason fall'n?\n",
            "    What will they be not?\n",
            "  CLOTEN. With my deserving,\n",
            "    The more, as I had not to see this stroke,\n",
            "    The court and sour of him, and his de\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# torch.cuda.is_available()"
      ],
      "metadata": {
        "id": "uU6GlTyZYk1E"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Q0Av4N5BY17i"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}