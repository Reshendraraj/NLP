{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4e457e-d415-48ea-81de-c546505be042",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b08d23f5-efa8-4efb-9dbe-9c167c3df05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2eff507e-781a-4d93-bed8-ce9f46911a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# w = tf.Variable([.2])\n",
    "# b = tf.Variable([-.2])\n",
    "\n",
    "# x = tf.constant(1.0, dtype=tf.float32) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea31a338-dd8b-4fd2-b719-8a10d1947d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # linear_model =w*x+b\n",
    "# # init = tf.global_variables_initializer()\n",
    "\n",
    "# # Compute linear model\n",
    "# linear_model = w * x + b\n",
    "\n",
    "# # Initialize variables\n",
    "# init = tf.compat.v1.global_variables_initializer()\n",
    "\n",
    "# # Use tf.compat.v1.Session()\n",
    "# with tf.compat.v1.Session() as sess:\n",
    "#     sess.run(init)  # Initialize variables\n",
    "#     print(\"w:\", sess.run(w))\n",
    "#     print(\"b:\", sess.run(b))\n",
    "#     print(\"Linear Model:\", sess.run(linear_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8acb0fac-2fec-4c87-a428-eb90d8553c72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\konde\\AppData\\Local\\Temp\\ipykernel_7764\\36163279.py:3: The name tf.disable_eager_execution is deprecated. Please use tf.compat.v1.disable_eager_execution instead.\n",
      "\n",
      "w: [0.2]\n",
      "b: [-0.2]\n",
      "Linear Model: [0.]\n"
     ]
    }
   ],
   "source": [
    "# import tensorflow as tf\n",
    "\n",
    "tf.compat.v1.disable_eager_execution()  # Disable eager execution for TF 1.x behavior\n",
    "\n",
    "# Define variables\n",
    "w = tf.Variable([0.2], dtype=tf.float32)\n",
    "b = tf.Variable([-0.2], dtype=tf.float32)\n",
    "\n",
    "# Define constant\n",
    "x = tf.constant(1.0, dtype=tf.float32)\n",
    "\n",
    "# Compute linear model\n",
    "linear_model = w * x + b\n",
    "\n",
    "# Initialize variables\n",
    "init = tf.compat.v1.global_variables_initializer()\n",
    "\n",
    "# Use tf.compat.v1.Session()\n",
    "with tf.compat.v1.Session() as sess:\n",
    "    sess.run(init)  # Initialize variables\n",
    "    print(\"w:\", sess.run(w))\n",
    "    print(\"b:\", sess.run(b))\n",
    "    print(\"Linear Model:\", sess.run(linear_model))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ff71ec6-a6a8-40a2-ac4c-3f9e733180bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Output: [0.         0.2        0.40000004 0.6       ]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "tf.compat.v1.disable_eager_execution()  # Disable eager execution for TF 1.x behavior\n",
    "\n",
    "# Define variables\n",
    "w = tf.Variable([0.2], dtype=tf.float32)\n",
    "b = tf.Variable([-0.2], dtype=tf.float32)\n",
    "\n",
    "# Define input as a placeholder (TF 1.x only)\n",
    "x = tf.compat.v1.placeholder(dtype=tf.float32)\n",
    "\n",
    "# Compute linear model\n",
    "linear_model = w * x + b\n",
    "\n",
    "# Initialize variables\n",
    "init = tf.compat.v1.global_variables_initializer()\n",
    "\n",
    "# Use session\n",
    "with tf.compat.v1.Session() as sess:\n",
    "    sess.run(init)  # Initialize variables\n",
    "    print(\"Predicted Output:\", sess.run(linear_model, feed_dict={x: [1, 2, 3, 4]}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b335aae8-dd0c-462f-b716-24a024573b10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 20.16\n"
     ]
    }
   ],
   "source": [
    "# y = tf.constant(tf.float32)\n",
    "# squared_deltas = tf.squre(linear_model-y)\n",
    "# loss = tf.reduce_sum(squared_deltas)\n",
    "# print(sess.run(loss,{x:[1,2,3,4],y:[0,-1,-2,-3]}))\n",
    "# tf.compat.v1.disable_eager_execution()  # Disable eager execution for TF 1.x behavior\n",
    "\n",
    "# Define variables\n",
    "w = tf.Variable([0.2], dtype=tf.float32)\n",
    "b = tf.Variable([-0.2], dtype=tf.float32)\n",
    "# Define input x and y as placeholders\n",
    "x = tf.compat.v1.placeholder(dtype=tf.float32)\n",
    "y = tf.compat.v1.placeholder(dtype=tf.float32)\n",
    "\n",
    "# Compute linear model\n",
    "linear_model = w * x + b\n",
    "# Compute squared error\n",
    "\n",
    "squared_deltas = tf.square(linear_model - y)\n",
    "# Compute loss (sum of squared errors)\n",
    "loss = tf.reduce_sum(squared_deltas)\n",
    "# Initialize variables\n",
    "\n",
    "init = tf.compat.v1.global_variables_initializer()\n",
    "# Run session\n",
    "\n",
    "with tf.compat.v1.Session() as sess:\n",
    "    sess.run(init)  # Initialize variables\n",
    "    loss_value = sess.run(loss, feed_dict={x: [1, 2, 3, 4], y: [0, -1, -2, -3]})\n",
    "    print(\"Loss:\", loss_value)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf4ea9a2-ebfa-47e9-89ab-f0b0c6e56710",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\konde\\AppData\\Local\\Temp\\ipykernel_7764\\2041218943.py:21: The name tf.train.GradientDescentOptimizer is deprecated. Please use tf.compat.v1.train.GradientDescentOptimizer instead.\n",
      "\n",
      "Epoch 0: Loss = 20.15999984741211\n",
      "Epoch 100: Loss = 0.1247127428650856\n",
      "Epoch 200: Loss = 0.011205842718482018\n",
      "Epoch 300: Loss = 0.0010068798437714577\n",
      "Epoch 400: Loss = 9.04696062207222e-05\n",
      "Epoch 500: Loss = 8.12815960671287e-06\n",
      "Epoch 600: Loss = 7.300958486666786e-07\n",
      "Epoch 700: Loss = 6.571497124241432e-08\n",
      "Epoch 800: Loss = 5.90275206491242e-09\n",
      "Epoch 900: Loss = 5.33532329427544e-10\n",
      "Trained w: [-0.99999714], Trained b: [0.99999166]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# tf.compat.v1.disable_eager_execution()  # Disable eager execution for TF 1.x\n",
    "\n",
    "# Define placeholders for input and output\n",
    "x = tf.compat.v1.placeholder(dtype=tf.float32)\n",
    "y = tf.compat.v1.placeholder(dtype=tf.float32)\n",
    "\n",
    "# Define trainable variables\n",
    "w = tf.Variable([0.2], dtype=tf.float32)\n",
    "b = tf.Variable([-0.2], dtype=tf.float32)\n",
    "\n",
    "# Compute linear model\n",
    "linear_model = w * x + b\n",
    "\n",
    "# Compute loss (Mean Squared Error)\n",
    "loss = tf.reduce_sum(tf.square(linear_model - y))\n",
    "\n",
    "# Define optimizer\n",
    "learning_rate = 0.01\n",
    "optimizer = tf.compat.v1.train.GradientDescentOptimizer(learning_rate)\n",
    "train = optimizer.minimize(loss)\n",
    "\n",
    "# Initialize variables\n",
    "init = tf.compat.v1.global_variables_initializer()\n",
    "\n",
    "# Train the model\n",
    "epochs = 1000\n",
    "with tf.compat.v1.Session() as sess:\n",
    "    sess.run(init)  # Initialize variables\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        _, loss_value = sess.run([train, loss], feed_dict={x: [1, 2, 3, 4], y: [0, -1, -2, -3]})\n",
    "\n",
    "        # Print loss every 100 epochs\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch {epoch}: Loss = {loss_value}\")\n",
    "\n",
    "    # Get final trained values\n",
    "    trained_w, trained_b = sess.run([w, b])\n",
    "    print(f\"Trained w: {trained_w}, Trained b: {trained_b}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36f03a89-cc84-48a5-b8d3-d6814d32eb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import LabelEncoder,OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff28fbbc-30d4-45b3-ac54-908f4c61c746",
   "metadata": {},
   "source": [
    "# Algorithm used is perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "32745a5c-4446-4933-8386-afe364d5ae96",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\konde\\\\Downloads\\\\archive\\\\sonar.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mkonde\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDownloads\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124marchive\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124msonar.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\Downloads\\naviGATOR\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32m~\\Downloads\\naviGATOR\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\Downloads\\naviGATOR\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32m~\\Downloads\\naviGATOR\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[0;32m   1881\u001b[0m     f,\n\u001b[0;32m   1882\u001b[0m     mode,\n\u001b[0;32m   1883\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1884\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1885\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   1886\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[0;32m   1887\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1888\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1889\u001b[0m )\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\Downloads\\naviGATOR\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    874\u001b[0m             handle,\n\u001b[0;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    876\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    877\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    879\u001b[0m         )\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\konde\\\\Downloads\\\\archive\\\\sonar.csv'"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(r\"C:\\Users\\konde\\Downloads\\archive\\sonar.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc68c9b-2b91-4727-bcc5-fc3d9deb8999",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16cec99a-8cbc-4ade-83d8-f9c41a763d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# X = df[df.columns[0:60]].values\n",
    "# y= df[df.columns[60]]\n",
    "X = df.drop(columns=[\"R\"])\n",
    "y = df[\"R\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8db4cb1-b2bc-4460-80fc-11e401ba6fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "y_labelencoder = LabelEncoder()\n",
    "y = y_labelencoder.fit_transform(y)\n",
    "\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ec75eb-d656-46aa-8424-5c1c9c5e2fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "\n",
    "# Reshape y because OneHotEncoder expects a 2D array\n",
    "y = y.reshape(-1, 1)\n",
    "\n",
    "# Initialize OneHotEncoder\n",
    "one_hot_encoder = OneHotEncoder(sparse_output=False)\n",
    "\n",
    "# Apply One-Hot Encoding\n",
    "Y = one_hot_encoder.fit_transform(y)\n",
    "\n",
    "# Display the transformed y\n",
    "Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ab4a53-c0fc-4b15-a04d-53fdf5af7f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train,X_test,y_train,y_test = train_test_split(X,Y,test_size=0.2,random_state=42)\n",
    "\n",
    "# # Disable eager execution to use compatibility mode for TensorFlow 1.x\n",
    "# tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "# # Define constants\n",
    "# n_class = 2  # Number of output classes\n",
    "\n",
    "# # Placeholder for input data (None is for batch size, 60 is number of features)\n",
    "# x = tf.compat.v1.placeholder(tf.float32, [None, 60])  # Input data\n",
    "# y = tf.compat.v1.placeholder(tf.int32, [None])  # Labels\n",
    "\n",
    "# # Weights and biases (no hidden layer, single layer perceptron)\n",
    "# w = tf.Variable(tf.zeros([60, n_class]))  # Weights (60 features to 2 classes)\n",
    "# b = tf.Variable(tf.zeros([n_class]))  # Biases\n",
    "\n",
    "# # Model operation (logits - Linear transformation)\n",
    "# logits = tf.matmul(x, w) + b\n",
    "\n",
    "# # Define loss function (cross entropy for multi-class classification)\n",
    "# loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits))\n",
    "\n",
    "# # Use GradientDescentOptimizer to minimize the loss function\n",
    "# optimizer = tf.compat.v1.train.GradientDescentOptimizer(learning_rate=0.01).minimize(loss)\n",
    "\n",
    "# # Initialize all variables\n",
    "# init = tf.compat.v1.global_variables_initializer()\n",
    "\n",
    "# # Train the model using a session\n",
    "# with tf.compat.v1.Session() as sess:\n",
    "#     # Initialize variables\n",
    "#     sess.run(init)\n",
    "    \n",
    "#     # Number of epochs\n",
    "#     epochs = 100\n",
    "    \n",
    "#     for epoch in range(epochs):\n",
    "#         # Feed dictionary with training data\n",
    "#         _, current_loss = sess.run([optimizer, loss], feed_dict={x: X_train, y: y_train})\n",
    "        \n",
    "#         if (epoch + 1) % 10 == 0:  # Print loss every 10 epochs\n",
    "#             print(f'Epoch {epoch + 1}, Loss: {current_loss}')\n",
    "    \n",
    "#     # After training, you can evaluate the model or use it to make predictions\n",
    "#     # Here, we print the final weights and biases\n",
    "#     final_weights, final_biases = sess.run([w, b])\n",
    "#     print(f\"Final Weights: {final_weights}\")\n",
    "#     print(f\"Final Biases: {final_biases}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc1b94f-27bc-43f7-843d-70a5a6ad867d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Example data: Let's assume `X_train` is your input features (shape: [num_samples, 60])\n",
    "# And `y_train` is your labels (e.g., 0 for 'R' and 1 for 'M')\n",
    "# Replace this with your actual data\n",
    "X_train = np.random.rand(207, 60)  # Random data for demonstration\n",
    "y_train = np.random.randint(2, size=207)  # Random binary labels (0 or 1)\n",
    "\n",
    "# Disable eager execution to use compatibility mode for TensorFlow 1.x\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "# Define constants\n",
    "n_class = 2  # Number of output classes\n",
    "\n",
    "# Placeholder for input data (None is for batch size, 60 is number of features)\n",
    "x = tf.compat.v1.placeholder(tf.float32, [None, 60])  # Input data\n",
    "y = tf.compat.v1.placeholder(tf.int32, [None])  # Labels (integer class labels)\n",
    "\n",
    "# Weights and biases (no hidden layer, single layer perceptron)\n",
    "w = tf.Variable(tf.zeros([60, n_class]))  # Weights (60 features to 2 classes)\n",
    "b = tf.Variable(tf.zeros([n_class]))  # Biases\n",
    "\n",
    "# Model operation (logits - Linear transformation)\n",
    "logits = tf.matmul(x, w) + b\n",
    "\n",
    "# Define loss function (sparse softmax cross entropy for integer class labels)\n",
    "loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits))\n",
    "\n",
    "# Use GradientDescentOptimizer to minimize the loss function\n",
    "optimizer = tf.compat.v1.train.GradientDescentOptimizer(learning_rate=0.01).minimize(loss)\n",
    "\n",
    "# Initialize all variables\n",
    "init = tf.compat.v1.global_variables_initializer()\n",
    "\n",
    "# Train the model using a session\n",
    "with tf.compat.v1.Session() as sess:\n",
    "    # Initialize variables\n",
    "    sess.run(init)\n",
    "    \n",
    "    # Number of epochs\n",
    "    epochs = 100\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Feed dictionary with training data\n",
    "        _, current_loss = sess.run([optimizer, loss], feed_dict={x: X_train, y: y_train})\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:  # Print loss every 10 epochs\n",
    "            print(f'Epoch {epoch + 1}, Loss: {current_loss}')\n",
    "    \n",
    "    # After training, you can evaluate the model or use it to make predictions\n",
    "    # Here, we print the final weights and biases\n",
    "    final_weights, final_biases = sess.run([w, b])\n",
    "    print(f\"Final Weights: {final_weights}\")\n",
    "    print(f\"Final Biases: {final_biases}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04036240-f42d-452b-bfc5-ec0c6eb14597",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install kagglehub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e541fdd2-0307-4fe7-9d17-8125882d3925",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "tf.compat.v1.disable_eager_execution()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0998a1-e4d9-4919-b11e-a01c8556befd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# flower_types = os.listdir(r\"C:\\Users\\konde\\Downloads\\flowers\")\n",
    "# flower_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fcbcb82-2f2d-4856-9511-7458bbc27914",
   "metadata": {},
   "outputs": [],
   "source": [
    "flower_types = [f for f in os.listdir(r\"C:\\Users\\konde\\Downloads\\flowers\") if os.path.isdir(os.path.join(r\"C:\\Users\\konde\\Downloads\\flowers\", f))]\n",
    "print(flower_types)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0f4498-73e3-4023-9a2b-f6a47bcbdf1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327fac68-00c8-489c-adc7-d1b82d101420",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "\n",
    "path = r\"C:\\Users\\konde\\Downloads\\flowers\"\n",
    "im_size = 60\n",
    "images = []\n",
    "labels = []\n",
    "\n",
    "# Get list of folders (flower categories)\n",
    "flower_types = [f for f in os.listdir(path) if os.path.isdir(os.path.join(path, f))]\n",
    "print(\"Detected flower types:\", flower_types)\n",
    "\n",
    "for flower_type in flower_types:\n",
    "    data_path = os.path.join(path, flower_type)  # Folder path\n",
    "    \n",
    "    filenames = [file for file in os.listdir(data_path) if file.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "    print(f\"Reading from folder: {data_path}, Found {len(filenames)} images\")\n",
    "\n",
    "    for f in filenames:\n",
    "        img_path = os.path.join(data_path, f)\n",
    "        img = cv2.imread(img_path)\n",
    "\n",
    "        if img is not None:\n",
    "            img = cv2.resize(img, (im_size, im_size))\n",
    "            images.append(img)\n",
    "            labels.append(flower_type)\n",
    "        else:\n",
    "            print(f\"Error reading image: {img_path}\")\n",
    "\n",
    "print(f\"Total images loaded: {len(images)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54bb91eb-e9de-4bf4-9ae5-3734218d4a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "images.shape\n",
    "# Convert images to numpy array\n",
    "images = np.array(images)\n",
    "images = images/255\n",
    "images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c7f138-03af-4181-9245-a86a7310626c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder,OneHotEncoder\n",
    "x=images\n",
    "y=labels\n",
    "y_labelencoder = LabelEncoder()\n",
    "y = y_labelencoder.fit_transform(y)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d3dad1-cffb-42bc-a10e-f4ac81dfb0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "\n",
    "# Sample data (example of labels as a list)\n",
    "y = labels  # Labels (categorical)\n",
    "\n",
    "# Convert y to a NumPy array and reshape it\n",
    "y = np.array(y).reshape(-1, 1)\n",
    "\n",
    "# Initialize OneHotEncoder\n",
    "onehotencoder = OneHotEncoder()\n",
    "\n",
    "# Fit and transform y\n",
    "y_encoded = onehotencoder.fit_transform(y)\n",
    "\n",
    "# Convert to dense array if you need it\n",
    "Y = y_encoded.toarray()\n",
    "\n",
    "print(Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d0f237-94f8-4318-b5f4-b20dd49717dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=43)\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2171833b-f32e-4f00-9cbc-28cf36ad173d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train = np.reshape(x_train(1414,10800))\n",
    "# print(x_train.shape)\n",
    "\n",
    "x_train = x_train.reshape(1414, 10800)\n",
    "print(x_train.shape)\n",
    "x_test = x_test.reshape(354,10800)\n",
    "print(x_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2616e2ca-7066-46d4-aff8-b6cd4780093f",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "training_epochs =1000\n",
    "n_dim =60\n",
    "n_class =2\n",
    "\n",
    "x=tf.compat.v1.placeholder(tf.float32,[None,10800])\n",
    "w = tf.Variable(tf.zeros([10800,n_class]))\n",
    "b = tf.Variable(tf.zeros([n_class]))\n",
    "y = tf.compat.v1.placeholder(tf.float32,[None,n_class])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6720ad-8e54-43e2-b87a-3ded94a2e757",
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.compat.v1.global_variables_initializer()\n",
    "pred = tf.nn.softmax(tf.matmul(x, w) + b)\n",
    "tf.matmul(x, w) + b\n",
    "\n",
    "# # Define loss function (sparse softmax cross entropy for integer class labels)\n",
    "# loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits = pred,labels=y))\n",
    "\n",
    "# # Use GradientDescentOptimizer to minimize the loss function\n",
    "# optimizer = tf.compat.v1.train.GradientDescentOptimizer(learning_rate=0.01).minimize(loss)\n",
    "\n",
    "# Define loss function (softmax cross entropy for one-hot encoded labels)\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "\n",
    "# Use GradientDescentOptimizer to minimize the loss function\n",
    "optimizer = tf.compat.v1.train.GradientDescentOptimizer(learning_rate=0.01).minimize(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42384302-448b-45b0-9023-bcf78c6e3432",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64dc5800-4644-4985-a3c1-d79062de0587",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "# Get flower types (folders) in the dataset path\n",
    "path = r\"C:\\Users\\konde\\Downloads\\flowers\"\n",
    "flower_types = [f for f in os.listdir(path) if os.path.isdir(os.path.join(path, f))]\n",
    "print(\"Detected flower types:\", flower_types)\n",
    "\n",
    "im_size = 60\n",
    "images = []\n",
    "labels = []\n",
    "\n",
    "# Load and preprocess images\n",
    "for flower_type in flower_types:\n",
    "    data_path = os.path.join(path, flower_type)\n",
    "    filenames = [file for file in os.listdir(data_path) if file.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "    print(f\"Reading from folder: {data_path}, Found {len(filenames)} images\")\n",
    "    \n",
    "    for f in filenames:\n",
    "        img_path = os.path.join(data_path, f)\n",
    "        img = cv2.imread(img_path)\n",
    "        \n",
    "        if img is not None:\n",
    "            img = cv2.resize(img, (im_size, im_size))\n",
    "            images.append(img)\n",
    "            labels.append(flower_type)\n",
    "        else:\n",
    "            print(f\"Error reading image: {img_path}\")\n",
    "\n",
    "print(f\"Total images loaded: {len(images)}\")\n",
    "\n",
    "# Convert images to numpy array and normalize\n",
    "images = np.array(images) / 255.0\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(labels)\n",
    "y = y.reshape(-1, 1)\n",
    "\n",
    "onehotencoder = OneHotEncoder(sparse_output=False)\n",
    "Y = onehotencoder.fit_transform(y)\n",
    "\n",
    "# Split dataset\n",
    "x_train, x_test, y_train, y_test = train_test_split(images, Y, test_size=0.2, random_state=43)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n",
    "print(x_test.shape)\n",
    "\n",
    "# Reshape data for TensorFlow\n",
    "x_train = x_train.reshape(x_train.shape[0], -1)\n",
    "x_test = x_test.reshape(x_test.shape[0], -1)\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "\n",
    "# TensorFlow model\n",
    "learning_rate = 0.01\n",
    "training_epochs = 1000\n",
    "n_features = x_train.shape[1]\n",
    "n_classes = Y.shape[1]\n",
    "\n",
    "x = tf.compat.v1.placeholder(tf.float32, [None, n_features])\n",
    "y = tf.compat.v1.placeholder(tf.float32, [None, n_classes])\n",
    "\n",
    "w = tf.Variable(tf.zeros([n_features, n_classes]))\n",
    "b = tf.Variable(tf.zeros([n_classes]))\n",
    "\n",
    "pred = tf.matmul(x, w) + b\n",
    "\n",
    "# Loss function\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "\n",
    "# Optimizer\n",
    "optimizer = tf.compat.v1.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "\n",
    "# Training the model\n",
    "with tf.compat.v1.Session() as sess:\n",
    "    sess.run(tf.compat.v1.global_variables_initializer())\n",
    "    \n",
    "    for epoch in range(training_epochs):\n",
    "        _, current_loss = sess.run([optimizer, loss], feed_dict={x: x_train, y: y_train})\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'Epoch {epoch + 1}, Loss: {current_loss}')\n",
    "    \n",
    "    # Final model parameters\n",
    "    final_weights, final_biases = sess.run([w, b])\n",
    "    print(f\"Final Weights: {final_weights}\")\n",
    "    print(f\"Final Biases: {final_biases}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360801d5-feca-4fd6-b40f-5de5a45a15f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99a5b57-8257-49e4-a891-0d0c615312b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121ada7d-21cf-44ae-b3f3-1f220112963a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f215812d-6caa-4bd8-986f-3fe6a5bb851c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"C:\\Users\\konde\\Downloads\\TensorFlow_FILES\\TensorFlow_FILES\\DATA\\fake_reg.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff17318-75bc-46b3-9524-fb8e048717a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9cb428a-cf66-4bc6-8048-857ef58331bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e8e414-6b54-4f34-b829-6bd0b806a906",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1298ac1-f861-4b16-8317-d301441871d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df[['feature1','feature2']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756ee212-bea8-4b32-aab8-c22ec8b13612",
   "metadata": {},
   "outputs": [],
   "source": [
    "y=df['price'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa9c029-9170-49a0-be16-53d2f46fe4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91921b99-d1b6-4158-9d07-af040f69d683",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22ec3c8-e7de-4008-8cec-1eebee22df67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(MinMaxScaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd102ba-79c9-4d1c-9168-6265b8aaaa61",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7e049a-5590-4ebf-b98a-614e3f3a3929",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b7bdaa-1b69-4dc0-9c63-8cb1862a4d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96af2fd-3746-4d87-b9d1-a2e69e44ed7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52beb525-1565-425b-90a9-4336d562dcc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(Sequential)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19117b9a-c202-406d-b65f-fd5a577c192c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model =Sequential([Dense(4,activation='relu'),\n",
    "#                    Dense(2,activation='relu'),\n",
    "#                   Dense(1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13dcfad-920a-4c9c-bacd-94fbe0075e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "model =Sequential()\n",
    "model.add(Dense(4,activation='relu'))\n",
    "model.add(Dense(4,activation='relu'))\n",
    "model.add(Dense(4,activation='relu'))\n",
    "\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer = 'rmsprop',loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5fdf57-e0c7-494a-983e-e7f89d8d328c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x=X_train,y=y_train,epochs=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77203713-7d2e-4f90-8074-cfcc2b2f97bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_df = pd.DataFrame(model.history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e708cd4-97ea-41d2-90a0-391548a005a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_df.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adad18c3-ff17-4d73-be3a-9e59c89162fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_test,y_test,verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6cbd993-ba5d-4c47-aeb1-f2590d78edf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_train,y_train,verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16ffed4-3773-48a0-82e4-f6d6de5f685f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = model.predict(X_test)\n",
    "# test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c680ead-7d39-4e18-b341-9f8589472987",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = pd.DataFrame(y_test,columns=['Test Y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a379cc2a-ef78-4897-86f7-abc138e86939",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = pd.Series(test_predictions.reshape(200,))\n",
    "# test_predictions = pd.Series(test_predictions.values.reshape(200,))\n",
    "# test_predictions = pd.Series(test_predictions.to_numpy().reshape(200,))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2109b2de-42c2-4411-ba18-f7b2792f5f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8f60be-180a-48ff-ab58-0277eb1ba618",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = pd.concat([pred_df,test_predictions],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94fb3a0-c044-47ff-919f-909890066ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df.columns = ['Test Y','Model Predictions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c302d98d-5875-4b60-8188-699fd564a3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07921bc-14c3-49a4-8374-48fd578762b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x='Test Y',y='Model Predictions',data=pred_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1daef6ff-e76b-425a-91e2-e943fdb89704",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error,mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e16703-56aa-4d8c-8c03-ca38a765f264",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_absolute_error(pred_df['Test Y'],pred_df['Model Predictions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461cb9c3-fb09-4fae-82c2-4c5ab6e75212",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error(pred_df['Test Y'],pred_df['Model Predictions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b870680-97f7-4629-83ae-d4ea93489d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error(pred_df['Test Y'],pred_df['Model Predictions'])**0.5  # root mean squre error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55bccf2a-cf70-43ae-900c-2540b17e1d91",
   "metadata": {},
   "source": [
    "# Predicting on brand new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c783c1-8a94-4699-a4b3-78e1bb6633e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [[Feature1, Feature2]]\n",
    "new_gem = [[998,1000]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8a3944-e06b-4f60-b086-a22e531ea232",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't forget to scale!\n",
    "new_gem = scaler.transform(new_gem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62056c3-5b5b-4ec9-a33c-9dc1943925a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(new_gem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6548acf1-b838-4b3f-8f9b-52ff84256d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a6f966-fed5-4cd4-b26e-f59bd7e0ee00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save('my_model.h5')\n",
    "model.save('my_model.keras')  # Recommended format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e86297-0ea2-4868-9efa-83f835e6dba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.saving\n",
    "later_model = model.save('my_model.h5')  # creates a HDF5 file 'my_model.h5'\n",
    "# later_model = keras.saving.save_model(model, 'my_model.keras')  # Explicit Keras saving function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb477579-403a-4c24-a621-5d49358da181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# later_model.predict(new_gem)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e140b7-8102-4fcd-8950-91634db0b3de",
   "metadata": {},
   "source": [
    "# Regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ae3d34-9fd8-44eb-9349-a36a4fa67179",
   "metadata": {},
   "source": [
    "Feature Columns\n",
    "\n",
    "id - Unique ID for each home sold\n",
    "\n",
    "date - Date of the home sale\n",
    "\n",
    "price - Price of each home sold\n",
    "\n",
    "bedrooms - Number of bedrooms\n",
    "\n",
    "bathrooms - Number of bathrooms, where .5 accounts for a room with a toilet but no shower\n",
    "\n",
    "sqft_living - Square footage of the apartments interior living space\n",
    "\n",
    "sqft_lot - Square footage of the land space\n",
    "\n",
    "floors - Number of floors\n",
    "\n",
    "waterfront - A dummy variable for whether the apartment was overlooking the waterfront or not\n",
    "\n",
    "view - An index from 0 to 4 of how good the view of the property was\n",
    "\n",
    "condition - An index from 1 to 5 on the condition of the apartment,\n",
    "\n",
    "grade - An index from 1 to 13, where 1-3 falls short of building construction and design, 7 has an average level of construction and design, and 11-13 have a high quality level of construction and design.\n",
    "\n",
    "sqft_above - The square footage of the interior housing space that is above ground level\n",
    "\n",
    "sqft_basement - The square footage of the interior housing space that is below ground level\n",
    "\n",
    "yr_built - The year the house was initially built\n",
    "\n",
    "yr_renovated - The year of the house’s last renovation\n",
    "\n",
    "zipcode - What zipcode area the house is in\n",
    "\n",
    "lat - Lattitude\n",
    "\n",
    "long - Longitude\n",
    "\n",
    "sqft_living15 - The square footage of interior housing living space for the nearest 15 neighbors\n",
    "\n",
    "sqft_lot15 - The square footage of the land lots of the nearest 15 neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32927a98-b0bf-4284-ae91-c2490d2d10fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d889b69e-32eb-4b7b-b523-c8a10a5004a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd21dbb4-8423-4925-a94e-b06796352edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(r\"C:\\Users\\konde\\Downloads\\TensorFlow_FILES\\TensorFlow_FILES\\DATA\\kc_house_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8eb9ab-2342-4baf-a07c-ddf40d316145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32d26b3-89e0-4231-a176-fe6a3a0ed5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0070f05d-5ea2-4739-8914-765a24f37933",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901be134-a0a5-4b91-8881-082ffb1512a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,12))\n",
    "sns.histplot(df['price'],kde = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac0b291-0618-439a-9ba0-2e8956db4c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "# sns.countplot(df['bedrooms'])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1967cc70-7595-47bf-b395-1fd7dbb03cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x=df['bedrooms'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c76996b-d3f3-4790-938f-bb65d79ca078",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['date'],inplace=True)\n",
    "df.corr()['price'].sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f35c22-8855-4cc7-8c8b-99d02ccf1b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x='price',y='sqft_living',data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68549353-8f38-4c44-8c2a-bd01b61c88c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x='bedrooms',y='price',data=df,palette='rainbow',hue='bedrooms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7071ceac-4660-4334-9cfa-97d2ce144552",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x='price',y='long',data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54bfe6a6-322e-4bf4-9a8e-4b45d527d165",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x='price',y='lat',data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8502865-8d26-4c2f-aaf6-bd06f3f83a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x='long',y='lat',data=df,hue='price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7063d9e-ca2c-4a38-b9af-9137454ed638",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x='waterfront',y='price',data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53bb3a42-e892-44d5-99a7-cdc94c1180b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(r\"C:\\Users\\konde\\Downloads\\TensorFlow_FILES\\TensorFlow_FILES\\DATA\\kc_house_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab858d62-3c7d-4b3c-85d8-8adc80aa0c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e83cf56-fe5c-4117-ae5e-7bc0b5b92137",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('id',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f065efc7-bd56-4906-9949-c4e911168b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de5469a-5d8c-449f-91fc-6f49c5986564",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date']=pd.to_datetime(df['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836d0224-f9d0-4b97-852a-9360df457a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c762eef2-906f-4549-93dd-a50a22c31845",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def year_extraction(date):\n",
    "#     return date.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a145f7d-a2d7-4364-8b65-e8f1e372fcf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['year'] = df['date'].apply(lambda date:date.year)\n",
    "# df['year']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b425fda0-fb85-4e76-ab98-ef664e4bf21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['year'] = df['date'].dt.year\n",
    "df['month'] = df['date'].dt.month\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45299948-b664-4564-9f3b-e06669696584",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5776cc42-620d-4016-851b-683b3ef2f0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x='month',y='price',data=df,palette='coolwarm',hue='month')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc74acdf-5e2a-4518-b952-fe3adb593d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('month').mean()['price'].plot(legend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1d9abe-6aa0-4d8a-9dec-ec0a35f203b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(['year']).mean()['price'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a1cdf7-7f88-43bf-94b0-846b139f6691",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.drop('date',inplace=True,axis=1)\n",
    "# df.drop('zipcode',axis=1)\n",
    "df = df.drop(columns=['zipcode'],axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ee6ec8-53ee-4e1f-a4d6-65a3793b18b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f425bc5-7e07-4de3-a37a-63dd74f697cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df.drop('price',axis=1).values\n",
    "y= df['price'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2024c2ad-8576-48ef-8044-031a9ba332aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d154687e-2ae5-4442-aaa8-21f064b61f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19af1df3-4411-4786-af49-2311fc4f3c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b90d37c-4996-4f54-817b-cfb4f04caf0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b881951f-38cd-45a4-8121-4c31e5c001b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()  \n",
    "X_train = scaler.fit_transform(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0407c38-9c5c-4bc3-89ae-6b7c4d0607f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test= scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663e3bcc-915e-49d4-8f30-4b5f8a4fa2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571eb795-38f3-4f73-a8d2-082be75b9908",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78fa2d33-f8c0-4c9f-b89d-8229c41201dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(19,activation='relu'))\n",
    "model.add(Dense(19,activation='relu'))\n",
    "model.add(Dense(19,activation='relu'))\n",
    "model.add(Dense(19,activation='relu'))\n",
    "\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer='adam',loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0754f930-1850-45ed-8397-9d240afeea7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    x=X_train,\n",
    "    y=y_train,\n",
    "    validation_data=(X_test,y_test),\n",
    "    batch_size=128,epochs=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28abfefd-9460-4f2a-b2df-a48487da9209",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = pd.DataFrame(model.history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68fad7b7-94c7-4adf-befd-b2f9b42ac457",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992dd0f1-b78b-46db-904a-dd2a8f43dfa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error,mean_absolute_error,explained_variance_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827018a0-2c01-4c96-ac57-fc607e39dbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97f514a-12a2-4995-9b28-7ccfb4eaddf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error(y_test,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5007bd28-810b-4246-b924-e58f761a79dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(mean_squared_error(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a1469c-08f2-4169-b353-b8eaf2561144",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_absolute_error(y_test,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815a00cc-44c5-4a8d-95bf-1748ee64eba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['price'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069086fa-d47b-4bfe-934a-036ace8692ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "5.402966e+05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db64d995-afcf-4c3a-aa0b-1ccfea418847",
   "metadata": {},
   "outputs": [],
   "source": [
    "explained_variance_score(y_test,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfdee979-9a22-49a7-b2c9-e62dc601a80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(y_test,predictions)\n",
    "plt.plot(y_test,y_test,'y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4cfd87-6f61-43a8-9843-38acd685c4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_house = df.drop('price',axis=1).iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f9f807-2f05-4862-8694-e523ae917ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_house"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6d4cd7-ad7c-42cf-b7dc-cdf3e1ca5ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_house.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a788f48-40d2-4382-b8b5-47b6c159a83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_house = scaler.transform(single_house.values.reshape(-1,19))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f648cdb-f4f1-4173-90b7-6c755587b1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(single_house)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622acdfc-9bbd-4464-8a45-52ea31eb31f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93fb5d37-835f-426b-986c-a7753f81ab6c",
   "metadata": {},
   "source": [
    "# Tesnorflow classification task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9e4a2d-266f-4c62-832d-4a3fa1cfd832",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b353b46-17b7-4d26-808f-f34b9ccc8c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"C:\\Users\\konde\\Downloads\\TensorFlow_FILES\\TensorFlow_FILES\\DATA\\cancer_classification.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0979bd55-2a1b-4406-b2b0-b179753ce98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8780e2f7-80f8-4b78-82d4-3b7b9d3dd541",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a735360-e6a6-4d85-888a-79f96bd1679f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4125e2db-2846-4e77-b61a-72cb4dcacbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='benign_0__mal_1',data=df)   # well balanced data for classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da45883d-b572-463f-9975-b730fdf40554",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.corr()['benign_0__mal_1'][:-1].sort_values().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c902a5-6ab4-42bc-952d-7ea9acb22fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(df.corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6d99e7-9dbb-448e-936c-941d90af9523",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df.drop('benign_0__mal_1',axis=1).values\n",
    "y=df['benign_0__mal_1'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad271cb-a7d2-4be5-a295-345833c39d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2110beab-e8b2-462a-acea-18c43e4ac9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.25,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62b0859-fd97-4dbd-a3b0-7600137f1630",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903a24c5-6a07-4822-8f66-fd0becd25a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e89249-ab64-4d10-aafd-8e575ff27683",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = scaler.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5441fd7-d4d2-4960-ab8e-4df5145298fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775bd815-4257-4a9c-93d4-083dc0e6e55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86dc9c57-a173-4852-a126-99a69c1772be",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03b8f80-0fb4-4fc5-be7a-1c43464e5e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "model =Sequential()\n",
    "model.add(Dense(30,activation = 'relu'))\n",
    "\n",
    "model.add(Dense(15,activation = 'relu'))\n",
    "# binary classification\n",
    "model.add(Dense(1,activation = 'sigmoid'))\n",
    "\n",
    "model.compile(optimizer ='adam',loss = 'binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f983af-b3f7-41f7-b35f-af3ad42e66c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.fit(x= X_train,y= y_train,epochs=600,validation_data=(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f229b011-05c4-4d55-9f30-d7579b1d702f",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = pd.DataFrame(model.history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b6c1d7-cee7-490b-9eea-7f09767c51a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc53b904-4fe9-4859-87b8-118ac07b26c8",
   "metadata": {},
   "source": [
    "# Early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08415a24-a226-427b-97b0-395ca4abc4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model =Sequential()\n",
    "model.add(Dense(30,activation = 'relu'))\n",
    "\n",
    "model.add(Dense(15,activation = 'relu'))\n",
    "# binary classification\n",
    "model.add(Dense(1,activation = 'sigmoid'))\n",
    "\n",
    "model.compile(optimizer ='adam',loss = 'binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ae2b03-b4cb-444e-aa81-d74a0045e6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff22603-4597-42d9-bdff-746b1535cf96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(EarlyStopping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37502dd8-f874-44cb-94d6-d7a7630450fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = EarlyStopping(monitor = 'val_loss',mode='min',patience=25,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d8cf8b-6972-4e62-a0d9-bdd637ff8823",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.fit(x=X_train,y=y_train,epochs) \n",
    "model.fit(x= X_train,y= y_train,epochs=600,validation_data=(X_test,y_test),\n",
    "         callbacks=[early_stop])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33cda6d-0d02-4c1c-b7f0-e12c19885a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = pd.DataFrame(model.history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06abd70e-107e-4d20-85f4-b3ac8f68f86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd57007-0209-426a-8653-dc44e2cf9385",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2300fe-8124-42dd-afca-20209665f02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model =Sequential()\n",
    "model.add(Dense(30,activation = 'relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(15,activation = 'relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# binary classification\n",
    "model.add(Dense(1,activation = 'sigmoid'))\n",
    "\n",
    "model.compile(optimizer ='adam',loss = 'binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340ff87e-f884-434b-9760-1ae2db0cb40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x= X_train,y= y_train,epochs=600,validation_data=(X_test,y_test),\n",
    "         callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b309b905-8858-44f9-8df6-1ed9db3622e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_loss = pd.DataFrame(model.history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e8c5bb-d205-4ab4-b1db-c23f8dbb746e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_loss.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7dc6585-50ae-4171-a4a4-8f624a3d5d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.predict_Classes(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f498d0f-8636-49ac-9a1f-de2614172a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130dd261-2333-494c-bd5f-2dd735d19b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b108e26e-c078-4d92-82be-bf0157811a85",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf356007-8318-4dee-9e8d-9d22395679b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix,classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a9a9d9-6e8c-49a1-846d-ee80af66d9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(classification_report(y_pred,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e51f0c-7b66-4123-ac0f-92ca0b31f759",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Convert probabilities to binary class labels\n",
    "y_pred_classes = (y_pred > 0.5).astype(\"int32\")\n",
    "\n",
    "# Generate the classification report\n",
    "print(classification_report(y_test, y_pred_classes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78531cfe-df0d-4dbe-b7ea-28ee46fb5400",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_test, y_pred_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d136bb5c-7433-4684-a009-e62c367b1768",
   "metadata": {},
   "source": [
    "# Classification project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eadae29-bfb8-48c3-af58-3b882d643e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd35531-523a-40ea-a1e1-a04cbc3dc774",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"C:\\Users\\konde\\Downloads\\TensorFlow_FILES\\TensorFlow_FILES\\DATA\\lending_club_loan_two.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c122440-4de6-47ce-a17b-f4e0b15dca2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04aebe07-3726-4a6b-bdae-92886569365b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0adc7fba-17da-4a40-8147-23d4bd1555ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='loan_status',data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e139319-16ac-491f-bf58-90a392729ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,4))\n",
    "sns.histplot(df['loan_amnt'],kde=False,bins=40)\n",
    "plt.xlim(0,45000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2549656-58e0-465e-9ceb-573a401d7b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.corr(pd.numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8121088-1d10-4800-9736-340343837e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x='loan_status',y='loan_amnt',data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a37d6d7-ccbc-4058-9532-dc94e3152efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert 'term' to numeric (extract months)\n",
    "# df['term'] = df['term'].apply(lambda x: int(x.split()[0]))\n",
    "\n",
    "# # Now compute correlation\n",
    "# df.corr()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9fca18b-c93f-4b1a-b382-bb5916b2ac92",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x='installment',y='loan_amnt',data=df,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512ee6db-6dfb-475d-8444-8d0d8d12d962",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x='loan_status',y='loan_amnt',data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2da16a-d924-4b35-b3d9-df4e778027da",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('loan_status')['loan_amnt'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43aa674f-d711-4331-8b1c-92c7b5237626",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(df['grade'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56a139b-09ac-4a45-a4d9-495d1fdea31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(df['sub_grade'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54d0a6e-041b-414d-b9df-cdc0736f0551",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='grade',data=df,hue='loan_status')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f3e765-34f7-4acf-9fde-2175034d013a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,4))\n",
    "subgrade_order = sorted(df['sub_grade'].unique())\n",
    "sns.countplot(x='sub_grade',data=df,order = subgrade_order,palette='coolwarm' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9053c630-1c8d-4063-9882-ad8984148337",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,4))\n",
    "subgrade_order = sorted(df['sub_grade'].unique())\n",
    "sns.countplot(x='sub_grade',data=df,order = subgrade_order,palette='coolwarm' ,hue='loan_status')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358afb4f-de72-4911-9244-5c21f1e59d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_and_g = df[(df['grade']=='G') | (df['grade']=='F')]\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "subgrade_order = sorted(f_and_g['sub_grade'].unique())\n",
    "sns.countplot(x='sub_grade',data=f_and_g,order = subgrade_order,hue='loan_status')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be529f6e-3bca-4b5f-8014-a1e102144b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['loan_status'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9acbd9fd-8bec-4ab2-82b5-3e03dac1e16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['loan_repaid'] = df['loan_status'].map({'Fully Paid':1,'Charged Off':0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e040020-725d-4c79-a0d4-787800254f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['loan_repaid','loan_status']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b341fa-3a7f-435f-b189-905851a7a52c",
   "metadata": {},
   "source": [
    "# PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c98da86-c5e4-4242-9bf5-5ea2ac6671c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276f89a3-9f0f-41db-acbd-2a047ca4d813",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919cfd76-acfd-4a37-a552-dca4f7bb250b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed468915-944c-4aaa-92d2-bf2fd461b73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "100* df.isnull().sum()/len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61d732c-1a55-450a-8207-100c98ff770f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['emp_title'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0202a2f-86c8-4448-b28e-15e1c5513f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['emp_title'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e26ff6-7536-4942-a0bd-62425af64bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('emp_title',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d3fcc8-a321-4ba3-8eaf-64537891c5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(df['emp_length'].dropna().unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535742e4-ee98-4139-ac8f-6c184c778ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_length_order = [ '< 1 year',\n",
    "                      '1 year',\n",
    "                     '2 years',\n",
    "                     '3 years',\n",
    "                     '4 years',\n",
    "                     '5 years',\n",
    "                     '6 years',\n",
    "                     '7 years',\n",
    "                     '8 years',\n",
    "                     '9 years',\n",
    "                     '10+ years']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457b01e4-a139-4953-af5d-2061c6384972",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,4))\n",
    "\n",
    "sns.countplot(x='emp_length',data=df,order=emp_length_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf765ea-20b3-47e4-aaa1-8be190b609b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,4))\n",
    "sns.countplot(x='emp_length',data=df,order=emp_length_order,hue='loan_status')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edac995a-9c3f-4b95-b083-05e67f9a49df",
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_co = df[df['loan_status']==\"Charged Off\"].groupby(\"emp_length\").count()['loan_status']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4020df5-c412-4d3b-ad94-5c256a9df175",
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_fp = df[df['loan_status']==\"Fully Paid\"].groupby(\"emp_length\").count()['loan_status']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e26b669-0057-4a75-98b0-e1d17417f6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_len = emp_co/emp_fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33d0031-70cf-476b-8a17-aac5ebbe174f",
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4344bd92-88dc-464a-9123-d89d8f423176",
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_len.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53bcd46a-18ea-4d50-b08c-60ab39f62e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('emp_length',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9173be84-8a57-4593-9cf2-ba063b823b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d858b077-d392-421e-a0e9-7587d5d64b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['purpose'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0f0910-b8b3-43b0-bcb1-92ef80babf2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['title'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f05d7c1-6098-4d97-98fe-fc9da450dcda",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('title',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae62e31-9901-4470-9d01-2bed36aa1e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['mort_acc'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5ee78a-7b11-49f5-8868-a14937dc4ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Correlation with the mort_acc column\")\n",
    "# df.corr()['mort_acc'].sort_values\n",
    "# Select only numeric columns\n",
    "df_numeric = df.select_dtypes(include=['number'])\n",
    "\n",
    "# Compute correlation with 'mort_acc' column\n",
    "correlation = df_numeric.corr()['mort_acc'].sort_values()\n",
    "print(\"Correlation with the mort_acc column\")\n",
    "print(correlation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3147ba8-4823-44db-ba88-e7996e439145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Mean of mort_acc column per total_acc\")\n",
    "# df.groupby('total_acc').mean()['mort_acc']\n",
    "\n",
    "# Ensure that 'total_acc' and 'mort_acc' are numeric\n",
    "df['total_acc'] = pd.to_numeric(df['total_acc'], errors='coerce')\n",
    "df['mort_acc'] = pd.to_numeric(df['mort_acc'], errors='coerce')\n",
    "\n",
    "# Group by 'total_acc' and compute the mean of 'mort_acc'\n",
    "total_acc_avg = df.groupby('total_acc')['mort_acc'].mean()\n",
    "\n",
    "print(\"Mean of mort_acc column per total_acc\")\n",
    "(total_acc_avg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a2a856-4dbd-4c6e-8ce7-a03543a0f2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_mort_acc(total_acc,mort_acc):\n",
    "    '''\n",
    "    Accepts the total_acc and mort_acc values for the row.\n",
    "    Checks if the mort_acc is NaN , if so, it returns the avg mort_acc value\n",
    "    for the corresponding total_acc value for that row.\n",
    "    \n",
    "    total_acc_avg here should be a Series or dictionary containing the mapping of the\n",
    "    groupby averages of mort_acc per total_acc values.\n",
    "    '''\n",
    "    if np.isnan(mort_acc):\n",
    "        return total_acc_avg[total_acc]\n",
    "    else:\n",
    "        return mort_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa842a2-ed2b-45df-adb4-81919a231e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['mort_acc'] = df.apply(lambda x: fill_mort_acc(x['total_acc'], x['mort_acc']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725a70b8-55c0-4dcc-a932-0aa487517600",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c0ac52-cd46-4d3d-995c-4f476a1f74b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987ded08-a7c8-4e5e-bbaa-d565f9d7e2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d4648b-89d6-4dfa-a403-bffe286c10b2",
   "metadata": {},
   "source": [
    "# Categorical Variables and Dummy Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34afc5b-6f1e-41aa-8e6a-22e88c5ed645",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select_dtypes(['object']).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300819a3-7db8-48ec-a7ca-028b23cdedd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['term'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2f5aae-55bc-40eb-b90b-a5b28db82290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Or just use .map()\n",
    "# df['term'] = df['term'].apply(lambda term: int(term[:3]))\n",
    "# Convert the 'term' column to string type and extract the two-digit numeric values\n",
    "df['term_numeric'] = df['term'].astype(str)\n",
    "df['term'] = df['term_numeric'].str.extract('(\\d{2})').astype(float)\n",
    "\n",
    "# Display the result\n",
    "print(df[['term', 'term_numeric']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb3bb8e-c352-4538-bb79-cbe54c7cfa3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('grade',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d696c3b-7670-431c-944e-2429a358e5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "subgrade_dummies = pd.get_dummies(df['sub_grade'],drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3039f5f0-d5e9-4964-8304-bff1226635d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df.drop('sub_grade',axis=1),subgrade_dummies],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06ed610-9e01-4638-8a6d-e405b20200f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5089831-82c0-4884-8331-c6dddb2988e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select_dtypes(['object']).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352d7033-754e-4236-9772-6066fdadfb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummies = pd.get_dummies(df[['verification_status', 'application_type','initial_list_status','purpose' ]],drop_first=True)\n",
    "df = df.drop(['verification_status', 'application_type','initial_list_status','purpose'],axis=1)\n",
    "df = pd.concat([df,dummies],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa708dc-330c-4c4c-9757-e3ca6c85f984",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['home_ownership'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57e69da-1217-49c2-b059-d434b47dd69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['home_ownership']=df['home_ownership'].replace(['NONE', 'ANY'], 'OTHER')\n",
    "\n",
    "dummies = pd.get_dummies(df['home_ownership'],drop_first=True)\n",
    "df = df.drop('home_ownership',axis=1)\n",
    "df = pd.concat([df,dummies],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2f9ee5-45c7-4d6f-95b8-54509fcaa746",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['zip_code'] = df['address'].apply(lambda address:address[-5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4dc2b61-0d59-4a72-9bc5-537a8d000c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummies = pd.get_dummies(df['zip_code'],drop_first=True)\n",
    "df = df.drop(['zip_code','address'],axis=1)\n",
    "df = pd.concat([df,dummies],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e77066-f3ed-48a5-927a-5c0cb8ab9b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('issue_d',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4d0a88-4a72-4bec-acf2-62395f6fff9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['earliest_cr_year'] = df['earliest_cr_line'].apply(lambda date:int(date[-4:]))\n",
    "df = df.drop('earliest_cr_line',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65dbaee-3ec1-4f60-9fa1-9b6c2b99e782",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select_dtypes(['object']).columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b601aca-b880-4269-81b8-5ae7fa5fa716",
   "metadata": {},
   "source": [
    "# Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8a09f4-2be1-4375-9e30-4291468fdd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a87b7f-30f4-494d-96f1-34e3f0fbc485",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('loan_status',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de667a9f-d26c-4d3b-a4ab-e76c3f03ebc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('loan_repaid',axis=1).values\n",
    "y = df['loan_repaid'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad86ff7e-0da1-47e8-b37f-ca8df96d7adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71ca392-4af2-46ad-90e3-0b78aa2b0a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e55a46-8939-483b-be09-ab1673707152",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93db21d-79d4-4c99-a5ad-b6953d886de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = scaler.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac942923-a803-4b12-9b26-2207bd3d4198",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d3ffe9-2bae-4330-9edc-64ae193192e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation,Dropout\n",
    "from tensorflow.keras.constraints import max_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e32f80a-bac6-496d-9884-5604dcd0b129",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e812822a-09dd-4ea0-a9d3-e375d7f0200a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# input layer\n",
    "model.add(Dense(78,  activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# hidden layer\n",
    "model.add(Dense(39, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# hidden layer\n",
    "model.add(Dense(19, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# output layer\n",
    "model.add(Dense(units=1,activation='sigmoid'))\n",
    "\n",
    "# Compile model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5590e9de-19bf-4bf9-b329-6d1f1d12c8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x=X_train, \n",
    "          y=y_train, \n",
    "          epochs=25,\n",
    "          batch_size=256,\n",
    "          validation_data=(X_test, y_test), \n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e99bc5-d0aa-494f-bcfc-8df8106fce60",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = pd.DataFrame(model.history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb33f707-8619-4542-bc88-9e733deaf2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses[['loss','val_loss']].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5048e00-c02b-49b6-8687-2e1f030d4ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report,confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5824853b-9805-4816-947b-9354c3108866",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab975ce0-e5d4-4391-b109-019c82bbee4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = (predictions >= 0.5).astype(int)\n",
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378acbc3-fb5c-492f-9a77-25b5b45adc2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31ae2aa-655c-4c9d-9723-067cf0c9184a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694d1668-9e7d-41f1-b678-0e5dab4965d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example of loading data (adjust if needed)\n",
    "# df = pd.read_csv(r'C:\\Users\\konde\\Downloads\\TensorFlow_FILES\\TensorFlow_FILES\\DATA\\lending_club_loan_two.csv')\n",
    "\n",
    "# # Drop any irrelevant columns (example: address or application type)\n",
    "# df.drop(columns=['address', 'application_type'], inplace=True,axis=1)\n",
    "\n",
    "# # Handle missing values (if any)\n",
    "# df.fillna(df.mean(), inplace=True)\n",
    "\n",
    "# # Convert categorical variables to numerical using LabelEncoder\n",
    "# categorical_columns = ['term', 'grade', 'sub_grade', 'emp_title', 'home_ownership']\n",
    "\n",
    "# le = LabelEncoder()\n",
    "# for col in categorical_columns:\n",
    "#     df[col] = le.fit_transform(df[col])\n",
    "\n",
    "# # Convert the target variable to binary classification (if applicable)\n",
    "# # Assuming you're predicting whether a loan is approved or not\n",
    "# # Create a binary target column for classification (1: approved, 0: not approved)\n",
    "# df['loan_approved'] = np.random.randint(0, 2, df.shape[0])  # Replace with actual logic for your target\n",
    "\n",
    "# # Separate features (X) and target (y)\n",
    "# X = df.drop(columns=['loan_approved'])\n",
    "# y = df['loan_approved']\n",
    "\n",
    "# # Split the data into training and testing sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Scale the features (important for neural networks)\n",
    "# scaler = StandardScaler()\n",
    "# X_train = scaler.fit_transform(X_train)\n",
    "# X_test = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d93c63-4080-4700-bfd9-a030a2a7c8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Build a Sequential model\n",
    "# model = Sequential()\n",
    "\n",
    "# # Add layers to the model (Dense layers)\n",
    "# model.add(Dense(64, input_dim=X_train.shape[1], activation='relu'))\n",
    "# model.add(Dense(32, activation='relu'))\n",
    "# model.add(Dense(16, activation='relu'))\n",
    "# model.add(Dense(1, activation='sigmoid'))  # Binary classification: 0 or 1\n",
    "\n",
    "# # Compile the model with binary cross-entropy loss and Adam optimizer\n",
    "# model.compile(optimizer=Adam(), loss=BinaryCrossentropy(), metrics=['accuracy'])\n",
    "\n",
    "# # View the model summary\n",
    "# model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec601de-bc4f-4233-bbc2-b28ab92ae335",
   "metadata": {},
   "source": [
    "# TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d435db30-9230-49cd-bd1d-b96f549cba97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81030883-beaa-4002-a5e0-e1da26637afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(r\"C:\\Users\\konde\\Downloads\\TensorFlow_FILES\\TensorFlow_FILES\\DATA\\cancer_classification.csv\")\n",
    "df=pd.read_csv('Downloads/TensorFlow_FILES/TensorFlow_FILES/DATA/cancer_classification.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04fc55d8-c6c7-4d6c-96da-4d7d135ecdd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ec9554-1163-44cc-833d-1eba52efbd25",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df.drop('benign_0__mal_1',axis=1).values\n",
    "y=df['benign_0__mal_1'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f090560-f27d-4154-b928-0b7284aaa1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93a71c4-6e50-444c-b76f-d56b1cd7757b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.25,random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4475aa-b56a-437a-93d6-f2ce52666131",
   "metadata": {},
   "source": [
    "# scaling the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1a9d8f-5a80-4aac-a96d-f1670fc8cca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78903a8-dd4a-4450-92a7-4603e8cf92da",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a26e085-e88f-4906-b49a-64001e4f0c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7913be-51b9-44e8-9625-1f87b2632c28",
   "metadata": {},
   "source": [
    "# Creating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9335afb6-f3cd-49cd-ab07-a9771717704f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Dropout,Activation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7d2ebf-13ce-4403-814b-cb0dcc30e5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping,TensorBoard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d90afe-19fb-4d09-988f-8f9763fb2a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1539d6-935b-4e20-90a2-52a3a1f9a22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b463dd-4ed6-44e2-b290-afe96f49dca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db21ae8-8650-4b4b-9c35-2ca50831330a",
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime.now().strftime('%y-%m-%d--%H%M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42af0ff7-0722-489a-9260-7eaa9970c472",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_directory = 'logs\\\\fit'\n",
    "board = TensorBoard(log_dir=log_directory,histogram_freq=1,\n",
    "                   write_graph=True,\n",
    "                   write_images=True,\n",
    "                   update_freq='epoch',\n",
    "                   profile_batch=2,\n",
    "                   embeddings_freq=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197a8a5a-09f5-4dd8-aeb4-b4aff7b757ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(units=30,activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(units=15,activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(units=1,activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bafb712-906f-4e53-a331-f2f60090b9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x=X_train, \n",
    "          y=y_train, \n",
    "          epochs=600,\n",
    "          validation_data=(X_test, y_test), verbose=1,\n",
    "          callbacks=[early_stop,board]\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50c3ef0-61a8-47e9-8949-32dde6aeb5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(log_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd5ba15-9fc6-4785-99cf-4050b7a63eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303467b5-7378-4a22-84ab-8ffa1e1da1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark import SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42edf98b-7728-4b86-a353-85e878938a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df44d96a-cad4-4790-9a41-87f7205db974",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19fa62b6-9a1e-4735-9034-83e19d559d00",
   "metadata": {},
   "source": [
    "# Lambda Expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8feb45a8-3757-410d-a161-738d29910b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def square(num):\n",
    "    result = num**2\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bcaf874-7527-43b5-8021-3657d7a78a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "square(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2c8e11-d59b-4901-9d1b-d1b7f2de53c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cube(num):\n",
    "    return(num**3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677234ba-a915-465f-855b-8031fa498a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "cube(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c148a9e7-de33-489a-a3d8-7ddafb0deb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cube(num):return(num**3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f543ad-dcfe-41b3-8bbe-bc1ef20a8ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cube(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0409186a-7c0f-4ae4-8332-3408e4fa011c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sq = lambda num:num**3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e724d1fa-4ee8-4aa2-8378-7d02237eda29",
   "metadata": {},
   "outputs": [],
   "source": [
    "cube(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337884c5-d52a-4150-bbbd-f250bf7f3614",
   "metadata": {},
   "outputs": [],
   "source": [
    "even = lambda num: num%2==0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35aaef3-44bf-4a44-a013-50fc022eec2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "even(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16313816-fa38-425b-82cb-be2691bebe43",
   "metadata": {},
   "outputs": [],
   "source": [
    "first = lambda s:s[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d969dd8-f03b-45f0-a4b1-f5ec2d541639",
   "metadata": {},
   "outputs": [],
   "source": [
    "first('rishi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2ebe97-ea12-4029-98a8-eb1f43adbaf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "rev = lambda s:s[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55fa8c3c-db4c-46e1-91ba-54e85aed8040",
   "metadata": {},
   "outputs": [],
   "source": [
    "rev('rishi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8973d54-5fee-4948-95bb-220c35c4d4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "rev('abcd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80886143-dff2-4f0a-aa1d-5eddbd069c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adder(x,y):return (x+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54abd51c-b7a1-4362-8ae6-3f20e6049dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "adder(10,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53723d02-2f7c-4a42-b920-e5d7996d4023",
   "metadata": {},
   "outputs": [],
   "source": [
    "adder = lambda x,y:x+y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6aca4ae-74fa-4da0-9884-7a4080e09454",
   "metadata": {},
   "outputs": [],
   "source": [
    "adder(2,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abcd9ec4-fbeb-4b94-894b-ebfdd994dd53",
   "metadata": {},
   "source": [
    "# pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6305f4-c76d-4a89-85b8-695853c790b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61cf6a6-955c-4265-b76c-62b5f4c28420",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sc = SparkContext()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5822a813-c8a9-4a7e-a347-3286f1ea6ee7",
   "metadata": {},
   "source": [
    "# pytorch\n",
    "## tensor basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961c46c8-423f-4e43-a77c-4b24a93bc9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2715a1a-1ebc-4c0f-93f7-e4e0de1344a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6ce55b-cff0-4491-983a-0141c60fddf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b4e274-bc1b-42d9-9fc7-03560553cd93",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.array([1,2,3,4,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125ec456-e71b-4274-b273-fc2ae8a710e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3f8d3d-cc91-41fe-a5d4-69ac30d89595",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d37505-116f-43a3-b63b-0ba83d365d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c1ad79-a871-4c9e-a494-824dee006b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.from_numpy(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8309d90-eb6b-42ae-8246-65acc1529c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ae9458-7057-4cfb-80ce-6ba1e247068a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.as_tensor(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7533e4ad-1779-4101-8e8a-08a22326ebd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f0a23d-177b-41d1-864b-862bee4f957a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# arr = np.arange(0,12).reshape(4,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a05490d-82b6-4d0d-89b1-eb24260b9e4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff84d2b-fa4f-4af4-98bf-1a259c095de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr2 = np.arange(0.,12.).reshape(4,3)\n",
    "print(arr2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8167838b-94da-46f6-9e7f-3d9e6d7afb29",
   "metadata": {},
   "outputs": [],
   "source": [
    "x2 = torch.from_numpy(arr2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31be40a5-8702-4036-9817-abe7e33675ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80135685-2dcd-4095-a79a-1e8bc7a5ec49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# arr[0]=99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7deea17c-7881-45dd-89f4-44776787ec13",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.array([1,2,3,4,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9726b412-8cb6-47fa-8e75-27265086ab25",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr[0]=99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971d0183-762b-4e73-8f54-f90e28ea92b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76713f67-3feb-4d44-989d-39af650188e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b2cc26-c999-474d-837c-3d22d96bc4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95cca822-8738-49f7-b7af-7d3535a0179f",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_arr = np.arange(0,10)\n",
    "my_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0c5572-d263-4862-8b7e-aff76544be09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62eb090-368b-40e6-8921-5e4228af1721",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_other_tensor= torch.from_numpy(my_arr)\n",
    "my_tensor= torch.tensor(my_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef25b824-2064-4aee-bd30-92919cc740ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4dc23a5-e9aa-4e8a-9ad1-e4050ea785c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_arr[0]=99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190eea47-5e19-4c0b-aab7-d5cedf65505e",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7081bfcb-d803-43d0-bffb-6bd6abf091ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e941a3-4dcb-4027-b1bf-9a2af4e3daec",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_other_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd5e25c-5465-4e86-96ee-d557e3a6ab63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d3def4-601c-42be-a79c-3e6f8a52dd75",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_arr = np.array([1,2,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed48c89-1356-4123-8182-858a2b6a95c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_arr.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393b35a2-9f1b-4bac-9728-efdbf79cf0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor(new_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c8af24-9163-4f11-8758-f743dd50affc",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_tensor = torch.Tensor(new_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d1d5e1-1233-4e7a-a154-7874bbf2f727",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_tensor.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8297c3-142f-425d-9c97-82179f5ceec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.empty(2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a156b2-c4c1-4452-a6c1-c0b3d72fdfa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = torch.empty(4,2)\n",
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb62415-a976-4a09-85fc-f86e2e309eae",
   "metadata": {},
   "outputs": [],
   "source": [
    " torch.zeros(4,3,dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229a699f-f980-4284-a60a-0e36ad0641db",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.arange(0,18,2).reshape(3,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3313ddbb-6c75-4c9b-ab7a-99e02d696f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.linspace(0,18,12).reshape(3,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c06b27-1f8e-4f32-86bc-1a770b4d8586",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_tensor = torch.tensor([1,2,3])\n",
    "my_tensor.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87a765f-1ac7-4660-9c48-8d76dbf2f959",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_tensor= my_tensor.type(torch.int32)\n",
    "my_tensor                                      # changing 64 to 32 bit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1816d2d9-6266-400a-bb82-6c7c0faced0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.rand(4,3)  # values gives betwwn 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb4a12e-0f27-446b-983e-d977690fbc46",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.randn(4,3)   # gives normal distribution and mean is zero and standard devaition is 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6a2470-de30-4f07-b4e2-681ff13554b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.randint(low=0,high=10,size=(5,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e72c2ff-0ccc-4cfa-8b39-50bca2990970",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=torch.zeros(2,5)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ea024b-fbdc-41e5-9ea9-e25f4f5f5ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.rand_like(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19255620-96e0-48b7-932c-5e3bb1d1eb3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.randn_like(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641485b3-2415-48a8-89e2-dd31fafd6c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.randint_like(x,low=0,high=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579d3499-0077-4cb4-a73c-6bafb36063e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "torch.rand(2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb5f489-dbe7-4a1a-a080-87f76e34c3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.rand(2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4029e51-1191-42d6-b551-959d5b4e5972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.rand(2,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8043f3f-2eb7-4743-b3f5-f63f3357932d",
   "metadata": {},
   "source": [
    "# Tensor operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca643353-2d0c-4743-b78e-71f0dab025a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=torch.arange(6).reshape(3,2)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312c42e5-fd2a-4fde-9f86-d36d1580663c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x[1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57d7308-7596-4ff8-a626-79f130ce3bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "x[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dfbc22a-cd31-4c85-ae20-f1304a66adef",
   "metadata": {},
   "outputs": [],
   "source": [
    "x[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea514fa2-3aec-49c9-9071-a1718ab093cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "x =torch.arange(10)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc497b9-484a-4d64-bc21-59d7af76b2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.view(2,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ad7f98-a99e-42b7-8cbe-1aadb5b68639",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=torch.tensor([1.,2.,3.])\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389665a0-85b1-4845-bfd0-b27153e71204",
   "metadata": {},
   "outputs": [],
   "source": [
    "b=torch.tensor([4.,5.,6])\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1fc9df8-d50f-4f84-b60e-33553d41ade6",
   "metadata": {},
   "outputs": [],
   "source": [
    "a*b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a906d4e6-315a-4268-87df-c51f7b4cd4e4",
   "metadata": {},
   "source": [
    "# dot product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb8cb47-421f-4926-8151-5a0a8ff909ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=torch.tensor([1.,2.,3.])\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef156f2e-cdb3-4415-b5f6-6e6d2f291e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "b=torch.tensor([4.,5.,6])\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30fa323-5abd-411e-915f-c1bca8859f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "a*b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486b12f2-c74d-45d3-b92d-8e44b5cf92a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "a.dot(b)  #dot product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f39de1e-8cca-4af6-bb0c-3d109593ac24",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=torch.tensor([[0,1,2],[3,4,5]])\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249c5f4f-986b-4888-960c-ffbe456d3607",
   "metadata": {},
   "outputs": [],
   "source": [
    "b= torch.tensor([[6,7],[8,9],[10,11]])\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43745869-5d89-4884-beae-136b5315fd59",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.mm(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3dff129-8882-4b4a-9186-9fca3e096bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "a@b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c182f09c-2036-4c02-a768-a3a81fc97432",
   "metadata": {},
   "source": [
    "L2 or Euclidian Norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e29a47-db83-4c8b-b3e3-4e33a17c241d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([2.,5.,8.,14.])\n",
    "x.norm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb27f69a-0389-4689-a13b-2e6404a47019",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "torch.manual_seed(42);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26663f6-fee8-4b88-b320-923ecd014cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.random.randint(0,5,6)\n",
    "arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08a7a99-31ee-4f00-9f17-8f8f36bf1274",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=torch.from_numpy(arr)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ba0a18-afa3-4dd5-9dab-c86cdfc735d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x.type(torch.int64)\n",
    "x.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12bf72c6-00f8-4d1a-a92f-44a72d91283d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x= x.reshape(3,2)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a7ac7e-6590-4215-b073-ccf435f7d480",
   "metadata": {},
   "outputs": [],
   "source": [
    "x[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45b7cc0-a2c8-4af4-a01f-029d9005547b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556f9a35-6d19-4d53-95f7-16796f9f4781",
   "metadata": {},
   "outputs": [],
   "source": [
    "y= torch.randint(0,5,(2,3))\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf42cb52-4f2b-4ba7-bd6b-919ee30e8fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.mm(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6bb073-c2f5-4272-a33b-0e53ce28acf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a099864-13d8-413f-9ec4-9c18eb704303",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=torch.tensor(2.0,requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17633d6-828b-48e0-8fa2-f3c2185e81e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y= 2*x**4 + x**3 + 3*x**2 +5*x +1\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4045fd9b-88c1-46fa-8e39-bd8b377b5f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec362234-73c5-400d-af66-33ad467b3a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda09e6d-0df3-4e35-a1e4-95cf6a57adbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777d8e27-ed3e-4fe6-ad7c-d49d05211005",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=torch.tensor([[1.,2.,3.,],[3.,2.,1.]],requires_grad=True)   # this is input layer\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508a8cd8-1370-49cf-b1ff-5b52bb6e37a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = 3*x + 2 # this is one layer (hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06251d25-415a-45ef-b2ee-cb0b42a62efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa9c8a3-1e43-476c-8790-2e119d48f431",
   "metadata": {},
   "outputs": [],
   "source": [
    "z= 2*y**2     #this is another layer (another hidden layer)\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17108dc9-2cb2-4403-982c-894d76f17f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = z.mean()   # output layer\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc76502-f387-43fb-a374-3eaf2862bb76",
   "metadata": {},
   "source": [
    "# back propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547dbc72-d644-4e55-a9c4-e51883812405",
   "metadata": {},
   "outputs": [],
   "source": [
    "out.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eacd957-de58-4e9b-b6a3-6a34908a1c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5256f820-1563-415c-b6d9-3f5a1f0d01c3",
   "metadata": {},
   "source": [
    "# Linear Regression with pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c76fd22-9ca2-43c5-864c-cca66fe2745e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68b83ac-a1b8-45c0-8285-9063c7af3b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.linspace(1,50,50).reshape(-1,1)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e156b0-ee52-417f-966d-6ee716cf0ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(71)\n",
    "e = torch.randint(-8,9,(50,1),dtype= torch.float)\n",
    "e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aea201d-895a-486e-8593-360ec48bb324",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = 2*X +1+e\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a730b0b-59d4-40d3-b482-deae6b9a626d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x=X.numpy(),y=y.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f09f3bf-8c55-4255-b0e4-deeae69c4364",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(59)\n",
    "model=nn.Linear(in_features=1,out_features=1)\n",
    "print(model.weight)\n",
    "print(model.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218a31f2-2068-4d7e-82b4-5c18d3befe3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self,in_features,out_features):\n",
    "        super().__init__()\n",
    "        self.linear=nn.Linear(in_features,out_features)\n",
    "    def forward(self,x):\n",
    "        y_pred=self.linear(x)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15bd460-9910-43c6-9ec4-cf42d1b5827e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(59)\n",
    "model=Model(1,1)\n",
    "print(Model)\n",
    "print(model.linear.weight.item())\n",
    "print(model.linear.bias.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f17b52-1c02-470f-b1b9-77a0136c8162",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(name, '\\t', param.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2933d878-5831-4f19-ad03-e52696dc5f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([2.0])\n",
    "print(model.forward(x)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b30a68-ad2b-456c-a79c-2a79630264cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = np.linspace(0.0,50.0,50)\n",
    "x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b4a0d4-0075-4633-a65b-39d0bce452b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = 0.1059\n",
    "b1 = 0.9637\n",
    "y1 = w1*x1+b1\n",
    "y1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b13089-d547-4931-86ef-84143f7a8760",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x=X.numpy(),y=y.numpy())\n",
    "plt.plot(x1,y1,'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ca34b0-0454-43b4-bbdd-e6cb0a5f080a",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe03996-8a57-44d5-b465-75a0bf3a167a",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "losses = []\n",
    "\n",
    "for i in range(epochs):\n",
    "    i=i+1\n",
    "    #predicting on forward pass\n",
    "    y_pred = model.forward(X)\n",
    "    #calculating loss\n",
    "    loss = criterion(y,y_pred)\n",
    "    #record the error\n",
    "    losses.append(loss)\n",
    "    print(f'epoch: {i:2}  loss: {loss.item():10.8f}  weight: {model.linear.weight.item():10.8f}  bias: {model.linear.bias.item():10.8f}') \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef0e5ca-5c56-401c-9dc3-bf9521a7bb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(range(epochs),losses)\n",
    "# plt.ylabel('Loss')\n",
    "# plt.xlabel('epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d64f33-f87a-4a88-b939-528571dedf23",
   "metadata": {},
   "outputs": [],
   "source": [
    "w1,b1 = model.linear.weight.item(), model.linear.bias.item()\n",
    "print(f'Current weight: {w1:.8f}, Current bias: {b1:.8f}')\n",
    "print()\n",
    "\n",
    "y1 = x1*w1 + b1\n",
    "print(x1)\n",
    "print(y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439cc6a0-23a2-431a-9ae1-e4778aa5b20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X.numpy(), y.numpy())\n",
    "plt.plot(x1,y1,'r')\n",
    "plt.title('Current Model')\n",
    "plt.ylabel('y')\n",
    "plt.xlabel('x');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9dcc41-f870-40f9-b9bd-95d6d2de193a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789e7c92-22b9-428a-a453-b098e3eaf9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c9e215-582b-4180-88bf-81df50652759",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(r\"C:\\Users\\konde\\Downloads\\PYTORCH_NOTEBOOKS\\PYTORCH_NOTEBOOKS\\Data\\iris.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c39c01f-d7b5-4a7f-bc99-c397cdfdb089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945bf3a5-95bd-41c1-8c8c-ade455eb9a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dfe64bc-a5c1-41e8-b310-fb2ce3901bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0bd42c-bd40-4484-986a-5edb753009a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(10,7))\n",
    "# fig.tight_layout()\n",
    "\n",
    "# plots = [(0,1),(2,3),(0,2),(1,3)]\n",
    "# colors = ['b', 'r', 'g']\n",
    "# labels = ['Iris setosa','Iris virginica','Iris versicolor']\n",
    "\n",
    "# for i, ax in enumerate(axes.flat):\n",
    "#     for j in range(3):\n",
    "#         x = df.columns[plots[i][0]]\n",
    "#         y = df.columns[plots[i][1]]\n",
    "#         ax.scatter(df[df['target']==j][x], df[df['target']==j][y], color=colors[j])\n",
    "#         ax.set(xlabel=x, ylabel=y)\n",
    "\n",
    "# fig.legend(labels=labels, loc=3, bbox_to_anchor=(1.0,0.85))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595f0bc0-7e20-449a-b9b5-e74b6102d110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4f277e-d288-4ed0-af80-c8f9db3d239d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e6593a-56e0-4b1c-815c-7a9e8fbdb826",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b55f73-6879-4bdb-bf8b-43d2b8203404",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# # Assuming df is your DataFrame\n",
    "# X = df.iloc[:, :-1]  # Features (all columns except last)\n",
    "# y = df.iloc[:, -1]   # Target (last column)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb86c06-5dba-4706-960b-bb723c0b4817",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a87a63-500e-4d59-8f6d-1c915db37ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.isnull().sum())  # Ensure no missing values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2d27f7-0a9e-4440-a0a7-cbdd91972fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)  # Removes NaNs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b21d47-f016-4415-b4dc-ccc20898f96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define features and labels\n",
    "X = df.drop('target', axis=1)  # Features (pandas DataFrame)\n",
    "y = df['target']  # Labels (pandas Series)\n",
    "\n",
    "# Split dataset into 80% training and 20% testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Print shapes to verify\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3bbe5a-1c6c-4cc8-b22c-063fa21bc92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = torch.FloatTensor(X_train.values)\n",
    "# X_test = torch.FloatTensor(X_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6ebe33-cf56-4e6b-b38e-5e6ae8acded8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73fa4931-4b15-4104-ae0d-e38337c7dcbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "\n",
    "# Assume df is the pandas DataFrame\n",
    "X = df.drop('target', axis=1)  # Features\n",
    "y = df['target']  # Labels\n",
    "\n",
    "# Split dataset into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert features to PyTorch tensors\n",
    "X_train = torch.tensor(X_train.to_numpy(), dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test.to_numpy(), dtype=torch.float32)\n",
    "\n",
    "# Convert target labels to PyTorch tensors (for classification)\n",
    "y_train = torch.tensor(y_train.values, dtype=torch.long).reshape(-1, 1)\n",
    "y_test = torch.tensor(y_test.values, dtype=torch.long).reshape(-1, 1)\n",
    "\n",
    "# Print shape to verify\n",
    "print(X_train.shape, y_train.shape)  # Expected: (120, 4) (120, 1)\n",
    "print(X_test.shape, y_test.shape)    # Expected: (30, 4) (30, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6339136e-48e4-4214-bf28-4310add9276f",
   "metadata": {},
   "source": [
    "method 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8faf14b-9880-489a-b321-d497211c5978",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset,DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80dc8824-76d3-4fdd-8dc8-be3e05da0a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788ecffe-d6eb-4d9c-9566-30dd03629ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.drop('target', axis=1).values # Features\n",
    "labels= df['target'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405e4c76-bceb-4e15-bc9a-9687817b456e",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = TensorDataset(torch.FloatTensor(data),torch.LongTensor(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6275a219-96d7-4907-b689-c7566bb0723f",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6e287f-d241-460f-ba5b-0f4dc05f9a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67a03ca-d032-4311-bec3-c5437f99d4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in iris:\n",
    "#     print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ac4ba2-e9d8-48d0-b7a3-fc3c2b742cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_loader = DataLoader(iris, batch_size=105, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d51556f-2b1c-4053-9f97-112613c015aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i_batch, sample_batched in enumerate(iris_loader):\n",
    "#     print(i_batch, sample_batched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cdab957-300f-4314-b1c2-75a2e2adf366",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in iris_loader:\n",
    "    print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa95269-7490-47a2-bd47-cbbecd8307ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1216cea-ec66-450c-9725-b98b186ceeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Model(nn.Module):\n",
    "#     def __init__(self,in_features = 4,h1=8,h2=9,out_features=3):\n",
    "#         super().__init__()\n",
    "#         self.fc1=nn.Linear(in_features,h1)\n",
    "#         self.h1 =nn.Linear(h1,h2)\n",
    "#         self.out=nn.Linear(h2,out_features)\n",
    "\n",
    "#     def forward(self,x):\n",
    "#         x=F.relu(self.fc1(x))\n",
    "#         x=F.relu(self.fc2(x))\n",
    "#         x=self.out(x)\n",
    "#         return x\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, in_features=4, h1=8, h2=9, out_features=3):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_features, h1)  # First layer\n",
    "        self.h1 = nn.Linear(h1, h2)  # Second layer (Renamed from fc2)\n",
    "        self.out = nn.Linear(h2, out_features)  # Output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))  # ✅ First layer\n",
    "        x = F.relu(self.h1(x))  # ✅ Corrected: Use self.h1 instead of self.fc2\n",
    "        x = self.out(x)  # ✅ Output layer\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e472cd-0db5-4952-95c3-aabaa5361833",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(32)\n",
    "model = Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c21bfd-74d5-4218-b36d-4d587f6e09c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2301c000-9e1a-447f-b9f8-8bd168b931b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"C:\\Users\\konde\\Downloads\\PYTORCH_NOTEBOOKS\\PYTORCH_NOTEBOOKS\\Data\\iris.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36b3202-1e5b-4398-a46f-369aebdb5196",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354ac3dd-beff-41a1-98cc-b42bf923a637",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2c2a87-be54-4e0d-b344-4e031848e450",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(10,7))\n",
    "fig.tight_layout()\n",
    "\n",
    "plots = [(0,1),(2,3),(0,2),(1,3)]\n",
    "colors = ['b', 'r', 'g']\n",
    "labels = ['Iris setosa','Iris virginica','Iris versicolor']\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    for j in range(3):\n",
    "        x = df.columns[plots[i][0]]\n",
    "        y = df.columns[plots[i][1]]\n",
    "        ax.scatter(df[df['target']==j][x], df[df['target']==j][y], color=colors[j])\n",
    "        ax.set(xlabel=x, ylabel=y)\n",
    "\n",
    "fig.legend(labels=labels, loc=3, bbox_to_anchor=(1.0,0.85))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2531ba-a6cd-442c-84ad-2eb4d3d96241",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('target',axis=1).values\n",
    "y=df['target'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67dd73b0-09c5-4255-8a23-924475dc6b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7020b188-d88d-45a8-877b-029844d4faf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbff1f7d-6e8d-46a1-a263-ceb80a7cc5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.FloatTensor(X_train)\n",
    "X_test = torch.FloatTensor(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8650decc-704f-437b-adff-f3bd01f7b845",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train=torch.LongTensor(y_train)\n",
    "y_test =torch.LongTensor(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7aeb12a-ef11-4e73-ad0e-f38d70eedf32",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a58fba-2d20-48e2-b83e-ee7a5384da80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852a7bf1-19cb-4817-896c-01beb87a1ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=100\n",
    "losses=[]\n",
    "\n",
    "for i in range(epochs):\n",
    "    # forward and prediction\n",
    "    y_pred = model.forward(X_train)\n",
    "    # calculate loss\n",
    "    loss = criterion(y_pred,y_train)\n",
    "    # record loss\n",
    "    losses.append(loss.detach().numpy())\n",
    "    if i%10==0:\n",
    "        print(f'Epoch{i} and loss is:{loss}')\n",
    "    #Backpropagation\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14006aa6-153f-4415-98c7-0fc6e7cb6ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(epochs),losses)\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d134a9-1d43-4909-847c-5e200cc98264",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    y_val = model.forward(X_test)\n",
    "    loss = criterion(y_val, y_test)\n",
    "print(f'{loss:.8f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24fe9962-dbf0-481a-8f9d-83bb67514017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97dfcb76-830d-4c80-8180-521dd625c240",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct=0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i,data in enumerate(X_test):\n",
    "        y_val = model.forward(data)\n",
    "        print(f'{i+1}.) {str(y_val.argmax().item())} {y_test[i]}')\n",
    "        \n",
    "        if y_val.argmax().item()==y_test[i]:\n",
    "            print(f'we got{correct}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa57b26-0b26-4a72-9789-9ddba396b632",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),'my_model_iris.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431ef611-f350-413e-aa07-636f1d567871",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = Model()\n",
    "new_model.load_state_dict(torch.load('my_model_iris.pt'))\n",
    "new_model.eval()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf17bce-4502-4bf0-9065-f74f8f8bbca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "unknown_iris= torch.tensor([5.6,3.7,2.2,0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297e5512-033c-40d9-81d3-114fc5ad91af",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(10,7))\n",
    "fig.tight_layout()\n",
    "\n",
    "plots = [(0,1),(2,3),(0,2),(1,3)]\n",
    "colors = ['b', 'r', 'g']\n",
    "labels = ['Iris setosa','Iris virginica','Iris versicolor','Mystery iris']\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    for j in range(3):\n",
    "        x = df.columns[plots[i][0]]\n",
    "        y = df.columns[plots[i][1]]\n",
    "        ax.scatter(df[df['target']==j][x], df[df['target']==j][y], color=colors[j])\n",
    "        ax.set(xlabel=x, ylabel=y)\n",
    "        \n",
    "    # Add a plot for our mystery iris:\n",
    "    ax.scatter(unknown_iris[plots[i][0]],unknown_iris[plots[i][1]], color='y')\n",
    "    \n",
    "fig.legend(labels=labels, loc=3, bbox_to_anchor=(1.0,0.85))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315a24b2-6d59-41c4-833c-2702e292dc5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    print(new_model(unknown_iris))\n",
    "    print()\n",
    "    print(labels[new_model(unknown_iris).argmax()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636350dc-597f-4ead-ab85-f44249af0a90",
   "metadata": {},
   "source": [
    "# pytoch project Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af14367b-a3ed-4cfe-b14a-ff39b15ed623",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f8e67b-3e07-4ae0-aa2b-0dcecafa0ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"C:\\Users\\konde\\OneDrive\\PYTORCH_NOTEBOOKS\\PYTORCH_NOTEBOOKS\\Data\\NYCTaxiFares.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4d4efc-92b6-472c-afa2-bb550f099064",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3087fe2-0877-49de-bbd2-5d9125e91618",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['fare_amount'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e358488-31ad-4127-bdaa-e2a26744a330",
   "metadata": {},
   "outputs": [],
   "source": [
    "def haversine_distance(df, lat1, long1, lat2, long2):\n",
    "    \"\"\"\n",
    "    Calculates the haversine distance between 2 sets of GPS coordinates in df\n",
    "    \"\"\"\n",
    "    r = 6371  # average radius of Earth in kilometers\n",
    "       \n",
    "    phi1 = np.radians(df[lat1])\n",
    "    phi2 = np.radians(df[lat2])\n",
    "    \n",
    "    delta_phi = np.radians(df[lat2]-df[lat1])\n",
    "    delta_lambda = np.radians(df[long2]-df[long1])\n",
    "     \n",
    "    a = np.sin(delta_phi/2)**2 + np.cos(phi1) * np.cos(phi2) * np.sin(delta_lambda/2)**2\n",
    "    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1-a))\n",
    "    d = (r * c) # in kilometers\n",
    "\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef530e4a-5c35-41d2-ab81-925103ecc5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['dist_km'] = haversine_distance(df,'pickup_latitude', 'pickup_longitude', 'dropoff_latitude', 'dropoff_longitude')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2d9a2f-114d-43d9-b3ee-cfac486adab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['pickup_datetime'] = pd.to_datetime(df['pickup_datetime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63af82af-19c5-4727-8b1a-04d56e8a3abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc11e7dd-ccf3-4f55-b13e-887560c33b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['EDTdate']=df['pickup_datetime']-pd.Timedelta(hours=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83ce80a-ec47-4a93-8176-7bd1ec79308f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Hour'] = df['EDTdate'].dt.hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a66297-e80f-48f4-9e3e-e63b4bcf2cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['AMorPM'] = np.where(df['Hour']<12,'am','pm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180a266b-4c4c-4179-9c80-d75b8b7f3fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Weekday'] = df['EDTdate'].dt.strftime(\"%a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fec4d3a-7677-4696-8e53-9508f0a1b351",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b4bc28-6e04-4439-b66d-b07734a98063",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = ['Hour', 'AMorPM', 'Weekday']\n",
    "cont_cols = ['pickup_latitude', 'pickup_longitude', 'dropoff_latitude', 'dropoff_longitude', 'passenger_count', 'dist_km']\n",
    "y_col = ['fare_amount']  # this column contains the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631c82b0-6c15-4c27-8450-7c5e7494d67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.info()\n",
    "# df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3141d640-5698-4e08-aded-7f4288a67792",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cat in cat_cols:\n",
    "    df[cat]=df[cat].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df669b0a-410e-4646-be63-84534e7a8cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2518ccc3-8a66-4661-a94b-9b714daaaa03",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Hour'].head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539f5841-0181-4ef8-9181-643a492e1b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['AMorPM'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf13dc2-1c55-4104-8788-0c97f1a270d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['AMorPM'].cat.categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace2a410-c1d8-439f-95db-456041d79115",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['AMorPM'].head().cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e37ffbe-b4e0-46c8-ac14-919f59ce13a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Weekday'].cat.categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1d1ca7-2733-4856-a3b2-ea88120214db",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Weekday'].cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56eb986-d9f9-47ab-96b5-a82f1360ca17",
   "metadata": {},
   "outputs": [],
   "source": [
    "hr = df['Hour'].cat.codes.values\n",
    "ampm = df['AMorPM'].cat.codes.values\n",
    "wkdy = df['Weekday'].cat.codes.values\n",
    "cats = np.stack([hr, ampm, wkdy], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d9e881-3467-4829-8d60-ad96551e132d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fdd7d62-6f8c-47dc-8da3-225d60cdb12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cats = np.stack([df[col].cat.codes.values for col in cat_cols],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d30199-165b-44e6-beb4-102b665ab22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3949f18-5926-49a7-9239-18861e977b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "conts = np.stack([df[col].values for col in cont_cols],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8b9283-6d4b-411b-9c4d-57c287c5cae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "conts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87dc2f2-2f5b-4435-b97f-e4c9cb8ff490",
   "metadata": {},
   "outputs": [],
   "source": [
    "cats = torch.tensor(cats, dtype=torch.int64)\n",
    "conts = torch.tensor(conts,dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e6ec6d-ac68-4888-a7bc-db5b5e20665f",
   "metadata": {},
   "outputs": [],
   "source": [
    "conts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8d4dab-db2c-401e-8655-bab21412504a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa357222-557e-4793-a7ff-cf286a24173b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.tensor(df[y_col].values,dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3286ff2-4fcc-413a-a062-de34a459a5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_szs = [len(df[col].cat.categories)for col in cat_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbfa628-f2f8-4b6a-a419-0c579e380e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_szs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfbcf43-ebdd-4833-bcc1-562c39734f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_szs =[(size,min(50,(size+1)//2))for size in cat_szs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b278da-c0fb-4960-9710-ef99c72ff0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_szs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6408e5e7-caf4-4df1-be34-160242559524",
   "metadata": {},
   "outputs": [],
   "source": [
    "catz = cats[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd0e7fc-cc19-44c6-9207-aeb3976a8ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "catz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66effeeb-a5ae-4be5-9e6c-cf718d9034a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "selfembeds = nn.ModuleList([nn.Embedding(ni,nf)for ni,nf in emb_szs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9acc9fb-7840-4d50-80d3-bb87e528f72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "selfembeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10163ce4-6537-42a3-9201-f5c273c3238c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This happens inside the forward() method\n",
    "embeddingz = []\n",
    "for i,e in enumerate(selfembeds):\n",
    "    embeddingz.append(e(catz[:,i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe28a64-797b-414c-9763-140e982e32d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddingz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa20ca14-c272-4275-80d8-450f7e73f111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We concatenate the embedding sections (12,1,4) into one (17)\n",
    "z = torch.cat(embeddingz, axis=1)\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e13c9d-bd82-43a2-9ede-e8360a34efdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "selfembdrop = nn.Dropout(0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff91edaf-be80-471b-90d4-d26e28302ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = selfembdrop(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4694bebe-4c70-47a5-aed1-f141933b039d",
   "metadata": {},
   "outputs": [],
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f92309-0162-4c41-9715-07f40e20db22",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabularModel(nn.Module):\n",
    "\n",
    "    def __init__(self, emb_szs, n_cont, out_sz, layers, p=0.5):\n",
    "        super().__init__()\n",
    "        self.embeds = nn.ModuleList([nn.Embedding(ni, nf) for ni,nf in emb_szs])\n",
    "        self.emb_drop = nn.Dropout(p)\n",
    "        self.bn_cont = nn.BatchNorm1d(n_cont)\n",
    "        \n",
    "        layerlist = []\n",
    "        n_emb = sum((nf for ni,nf in emb_szs))\n",
    "        n_in = n_emb + n_cont\n",
    "        \n",
    "        for i in layers:\n",
    "            layerlist.append(nn.Linear(n_in,i)) \n",
    "            layerlist.append(nn.ReLU(inplace=True))\n",
    "            layerlist.append(nn.BatchNorm1d(i))\n",
    "            layerlist.append(nn.Dropout(p))\n",
    "            n_in = i\n",
    "        layerlist.append(nn.Linear(layers[-1],out_sz))\n",
    "            \n",
    "        self.layers = nn.Sequential(*layerlist)\n",
    "    \n",
    "    def forward(self, x_cat, x_cont):\n",
    "        embeddings = []\n",
    "        for i,e in enumerate(self.embeds):\n",
    "            embeddings.append(e(x_cat[:,i]))\n",
    "        x = torch.cat(embeddings, 1)\n",
    "        x = self.emb_drop(x)\n",
    "        \n",
    "        x_cont = self.bn_cont(x_cont)\n",
    "        x = torch.cat([x, x_cont], 1)\n",
    "        x = self.layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd677522-7031-4ac7-9703-1789ce9f5f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(33)\n",
    "model = TabularModel(emb_szs, conts.shape[1], 1, [200,100], p=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47736d0c-a173-490e-b4e9-690efdd21a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8978b88-fe14-43f7-a2b2-13af85dbbfe3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850d4cd3-91d1-4280-be1e-6c1f3bb8ac76",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()  # we'll convert this to RMSE later\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffbe4ff-d6a9-4430-b082-9c9032e065d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9633c2f-7ea8-4d8b-aa60-b3088f51f3a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facb9a2e-4e43-4154-a042-c408322d45d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff69056-c1d4-49de-bf56-56a92878529e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F          # adds some efficiency\n",
    "from torch.utils.data import DataLoader  # lets us load data in batches\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9e4033-1f61-46f0-92d1-126014f0226f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6436f9c6-2761-4871-97ec-aef8d20514b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.ToTensor()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5cf40c4-c21f-4094-8b67-2b72df521b43",
   "metadata": {},
   "source": [
    "# load the train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2129784-4302-421d-b4ec-3b428a01eb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = datasets.MNIST(root=r\"C:\\Users\\konde\\OneDrive\\PYTORCH_NOTEBOOKS\\PYTORCH_NOTEBOOKS\",train=True,download=True,transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9414c3-f90c-4e6f-bde2-a18cbda10f5c",
   "metadata": {},
   "source": [
    "# load the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94fb47b-93fe-4306-9c9e-a3c1a7e0fccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ae7d75-196a-46bf-a97d-35c14df94fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = datasets.MNIST(root=r\"C:\\Users\\konde\\OneDrive\\PYTORCH_NOTEBOOKS\\PYTORCH_NOTEBOOKS\", train=False, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b9c97b-d5fe-4442-8689-970a5080cc48",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e303c8db-256b-41d5-b37e-54bcea946a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5adf3ec-a6bc-4bf1-a6b3-3c9bc722da61",
   "metadata": {},
   "outputs": [],
   "source": [
    "image,label = train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea92824-356d-4868-a0f2-9b7dbc6db9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f35c1d-4ede-406e-8837-7ba410f37e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(train_data[0][0].reshape((28,28)));\n",
    "plt.imshow(image.reshape(28,28),cmap='gray')\n",
    "plt.imshow(image.reshape(28,28),cmap='gist_yarg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0df738-9b26-4a92-bb97-75b84c9ae69a",
   "metadata": {},
   "source": [
    "# Batch loading wuth dataloader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904875c1-197f-4ba8-812a-b915a53de829",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(101)  # for consistent results\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=100, shuffle=True)\n",
    "\n",
    "test_loader = DataLoader(test_data, batch_size=500, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524c8383-3f3b-4863-aa4f-b5bfaaf7b29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.utils import make_grid\n",
    "np.set_printoptions(formatter=dict(int=lambda x: f'{x:4}')) \n",
    "\n",
    "for images,labels in train_loader: \n",
    "    break\n",
    "\n",
    "# Print the first 12 labels\n",
    "print('Labels: ', labels[:12].numpy())\n",
    "\n",
    "# Print the first 12 images\n",
    "im = make_grid(images[:12], nrow=12)  # the default nrow is 8\n",
    "plt.figure(figsize=(10,4))\n",
    "# We need to transpose the images from CWH to WHC\n",
    "plt.imshow(np.transpose(im.numpy(), (1, 2, 0)));\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f0d431-6013-44f2-8af9-9a2d399fd30d",
   "metadata": {},
   "source": [
    "# Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b979a2-91a9-46a3-8233-8dacb6588637",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Multilayerperceptron(nn.Module):\n",
    "\n",
    "    def __init__(self,in_sz=784,out_sz=10,layers=[120,84]):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_sz,layers[0])\n",
    "        self.fc2 = nn.Linear(layers[0],layers[1])\n",
    "        self.fc3 = nn.Linear(layers[1],out_sz)\n",
    "    def forward(self,x):\n",
    "        X=F.relu(self.fc1(X))\n",
    "        X=F.relu(self.fc2(X))\n",
    "        X=self.fc3(X)\n",
    "        return F.log_softmax(X,dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15d2fb3-8fe7-406b-b2a7-32c5c213d2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(101)\n",
    "model = Multilayerperceptron()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e8e909-0ef1-43e4-a114-f3523608373e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    print(param.numel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50ef818-040d-4c92-a477-d3c841ba892a",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818f5cb9-52e6-4b9f-a68b-14ea128531c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# images.view(100,-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907a27c1-9f75-4cc1-a034-042d43527035",
   "metadata": {},
   "outputs": [],
   "source": [
    "images.view(100,-1).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a96e1d-498f-4240-898f-aa24d08020d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "epochs=10\n",
    "\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "trn_correct = []\n",
    "tst_correct= []\n",
    "\n",
    "for i in range(epochs):\n",
    "    trn_corr =0\n",
    "    tst_corr =0\n",
    "\n",
    "    for b,(X_train,y_train) in enumerate (train_loader):\n",
    "        b+=1\n",
    "        y_pred = model(X_train.view(100,-1))\n",
    "        loss = criterion(y_pred,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5dafd62-b437-4d3d-b584-9c4699352e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# import torch.nn.functional as F\n",
    "# from torch.utils.data import DataLoader\n",
    "\n",
    "# # ✅ Set a seed for reproducibility\n",
    "# torch.manual_seed(101)\n",
    "\n",
    "# # ✅ Load EMNIST dataset (Replace this with actual dataset)\n",
    "# train_loader = DataLoader(train_data, batch_size=100, shuffle=True)  # Training data\n",
    "# test_loader = DataLoader(test_data, batch_size=500, shuffle=False)   # Testing data\n",
    "\n",
    "# # ✅ Define a simple Multilayer Perceptron (MLP)\n",
    "# class MLP(nn.Module):\n",
    "#     def __init__(self, input_size=784, hidden1=120, hidden2=84, output_size=10):\n",
    "#         super().__init__()\n",
    "#         self.fc1 = nn.Linear(input_size, hidden1)  # First hidden layer\n",
    "#         self.fc2 = nn.Linear(hidden1, hidden2)     # Second hidden layer\n",
    "#         self.fc3 = nn.Linear(hidden2, output_size) # Output layer\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = F.relu(self.fc1(x))  # Activation function for layer 1\n",
    "#         x = F.relu(self.fc2(x))  # Activation function for layer 2\n",
    "#         x = self.fc3(x)          # Output (logits)\n",
    "#         return F.log_softmax(x, dim=1)  # Convert to probabilities\n",
    "\n",
    "# # ✅ Initialize model, loss function, and optimizer\n",
    "# model = MLP()\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# # ✅ Training & Testing Loop\n",
    "# epochs = 10\n",
    "# for epoch in range(epochs):\n",
    "#     train_correct = 0\n",
    "#     test_correct = 0\n",
    "\n",
    "#     # **Training Phase**\n",
    "#     for X_train, y_train in train_loader:\n",
    "#         X_train = X_train.view(X_train.shape[0], -1)  # Flatten input\n",
    "\n",
    "#         # Forward pass\n",
    "#         y_pred = model(X_train)\n",
    "#         loss = criterion(y_pred, y_train)\n",
    "\n",
    "#         # Backpropagation\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         # Compute correct predictions\n",
    "#         train_correct += (y_pred.argmax(1) == y_train).sum().item()\n",
    "\n",
    "#     # **Testing Phase (No Gradient Calculation)**\n",
    "#     with torch.no_grad():\n",
    "#         for X_test, y_test in test_loader:\n",
    "#             X_test = X_test.view(X_test.shape[0], -1)  # Flatten input\n",
    "#             y_pred_test = model(X_test)\n",
    "#             test_correct += (y_pred_test.argmax(1) == y_test).sum().item()\n",
    "\n",
    "#     # Print accuracy after each epoch\n",
    "#     train_acc = train_correct / len(train_loader.dataset) * 100\n",
    "#     test_acc = test_correct / len(test_loader.dataset) * 100\n",
    "#     print(f\"Epoch {epoch+1}: Train Accuracy = {train_acc:.2f}%, Test Accuracy = {test_acc:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04fe799-81bd-4e5b-a02c-6f4a377ce8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# ✅ Set a seed for reproducibility\n",
    "torch.manual_seed(101)\n",
    "\n",
    "# ✅ Load EMNIST dataset (Replace this with actual dataset)\n",
    "train_loader = DataLoader(train_data, batch_size=100, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=500, shuffle=False)\n",
    "\n",
    "# ✅ Define a simple Multilayer Perceptron (MLP)\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size=784, hidden1=120, hidden2=84, output_size=10):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden1)\n",
    "        self.fc2 = nn.Linear(hidden1, hidden2)\n",
    "        self.fc3 = nn.Linear(hidden2, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "# ✅ Initialize model, loss function, and optimizer\n",
    "model = MLP()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# ✅ Lists to store loss and accuracy values\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "\n",
    "# ✅ Training & Testing Loop\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    train_loss = 0\n",
    "    test_loss = 0\n",
    "    train_correct = 0\n",
    "    test_correct = 0\n",
    "\n",
    "    # **Training Phase**\n",
    "    model.train()  # Set model to training mode\n",
    "    for X_train, y_train in train_loader:\n",
    "        X_train = X_train.view(X_train.shape[0], -1)  # Flatten input\n",
    "\n",
    "        # Forward pass\n",
    "        y_pred = model(X_train)\n",
    "        loss = criterion(y_pred, y_train)\n",
    "        train_loss += loss.item()  # Store training loss\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Compute correct predictions\n",
    "        train_correct += (y_pred.argmax(1) == y_train).sum().item()\n",
    "\n",
    "    # **Testing Phase (No Gradient Calculation)**\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        for X_test, y_test in test_loader:\n",
    "            X_test = X_test.view(X_test.shape[0], -1)  # Flatten input\n",
    "            y_pred_test = model(X_test)\n",
    "            loss = criterion(y_pred_test, y_test)\n",
    "            test_loss += loss.item()  # Store test loss\n",
    "            test_correct += (y_pred_test.argmax(1) == y_test).sum().item()\n",
    "\n",
    "    # Compute average losses\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    avg_test_loss = test_loss / len(test_loader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "    test_losses.append(avg_test_loss)\n",
    "\n",
    "    # Compute accuracy\n",
    "    train_acc = train_correct / len(train_loader.dataset) * 100\n",
    "    test_acc = test_correct / len(test_loader.dataset) * 100\n",
    "    train_accuracies.append(train_acc)\n",
    "    test_accuracies.append(test_acc)\n",
    "\n",
    "    # Print accuracy and loss for each epoch\n",
    "    print(f\"Epoch {epoch+1}: Train Loss = {avg_train_loss:.4f}, Test Loss = {avg_test_loss:.4f}, \"\n",
    "          f\"Train Accuracy = {train_acc:.2f}%, Test Accuracy = {test_acc:.2f}%\")\n",
    "\n",
    "# ✅ Plot Training & Validation Loss\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)  # First plot\n",
    "plt.plot(train_losses, label=\"Training Loss\", marker='o')\n",
    "plt.plot(test_losses, label=\"Validation Loss\", marker='o')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training & Validation Loss\")\n",
    "plt.legend()\n",
    "\n",
    "# ✅ Plot Training & Validation Accuracy\n",
    "plt.subplot(1, 2, 2)  # Second plot\n",
    "plt.plot(train_accuracies, label=\"Training Accuracy\", marker='o')\n",
    "plt.plot(test_accuracies, label=\"Validation Accuracy\", marker='o')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy (%)\")\n",
    "plt.title(\"Training & Validation Accuracy\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()  # Adjust layout\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa448e3e-4069-43eb-8c0f-04c4c657ea5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# ✅ Load all test images at once (Batch size = 10,000)\n",
    "test_loader_all = DataLoader(test_data, batch_size=10000, shuffle=False)\n",
    "\n",
    "# ✅ Evaluate the model on unseen data\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()  # Set to evaluation mode\n",
    "    correct = 0\n",
    "    all_preds, all_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_test, y_test in test_loader:\n",
    "            X_test = X_test.view(len(X_test), -1)  # Flatten images\n",
    "            y_pred = model(X_test)\n",
    "            predicted_labels = y_pred.argmax(dim=1)\n",
    "\n",
    "            # Count correct predictions\n",
    "            correct += (predicted_labels == y_test).sum().item()\n",
    "\n",
    "            # Store predictions & true labels for confusion matrix\n",
    "            all_preds.extend(predicted_labels.cpu().numpy())\n",
    "            all_labels.extend(y_test.cpu().numpy())\n",
    "\n",
    "    # Compute test accuracy\n",
    "    total_samples = len(test_data)\n",
    "    test_accuracy = correct / total_samples * 100\n",
    "    print(f\"Test Accuracy: {correct}/{total_samples} = {test_accuracy:.2f}%\")\n",
    "\n",
    "    return all_preds, all_labels\n",
    "\n",
    "# ✅ Call the evaluation function\n",
    "preds, labels = evaluate_model(model, test_loader_all)\n",
    "\n",
    "# ✅ Plot the Confusion Matrix\n",
    "def plot_confusion_matrix(y_true, y_pred):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=range(10), yticklabels=range(10))\n",
    "    plt.xlabel(\"Predicted Label\")\n",
    "    plt.ylabel(\"True Label\")\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.show()\n",
    "\n",
    "# ✅ Display confusion matrix\n",
    "plot_confusion_matrix(labels, preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11fae37-ea66-4e24-8ea6-fecbed38c396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "# start_time = time.time()\n",
    "\n",
    "# epochs = 10\n",
    "# train_losses = []\n",
    "# test_losses = []\n",
    "# train_correct = []\n",
    "# test_correct = []\n",
    "\n",
    "# for i in range(epochs):\n",
    "#     trn_corr = 0\n",
    "#     tst_corr = 0\n",
    "    \n",
    "#     # Run the training batches\n",
    "#     for b, (X_train, y_train) in enumerate(train_loader):\n",
    "#         b+=1\n",
    "        \n",
    "#         # Apply the model\n",
    "#         y_pred = model(X_train.view(100, -1))  # Here we flatten X_train\n",
    "#         loss = criterion(y_pred, y_train)\n",
    " \n",
    "#         # Tally the number of correct predictions\n",
    "#         predicted = torch.max(y_pred.data, 1)[1]\n",
    "#         batch_corr = (predicted == y_train).sum()\n",
    "#         trn_corr += batch_corr\n",
    "        \n",
    "#         # Update parameters\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "        \n",
    "#         # Print interim results\n",
    "#         if b%200 == 0:\n",
    "#             print(f'epoch: {i:2}  batch: {b:4} [{100*b:6}/60000]  loss: {loss.item():10.8f}  \\\n",
    "# accuracy: {trn_corr.item()*100/(100*b):7.3f}%')\n",
    "    \n",
    "#     # Update train loss & accuracy for the epoch\n",
    "#     train_losses.append(loss)\n",
    "#     train_correct.append(trn_corr)\n",
    "        \n",
    "#     # Run the testing batches\n",
    "#     with torch.no_grad():\n",
    "#         for b, (X_test, y_test) in enumerate(test_loader):\n",
    "\n",
    "#             # Apply the model\n",
    "#             y_val = model(X_test.view(500, -1))  # Here we flatten X_test\n",
    "\n",
    "#             # Tally the number of correct predictions\n",
    "#             predicted = torch.max(y_val.data, 1)[1] \n",
    "#             tst_corr += (predicted == y_test).sum()\n",
    "    \n",
    "#     # Update test loss & accuracy for the epoch\n",
    "#     loss = criterion(y_val, y_test)\n",
    "#     test_losses.append(loss)\n",
    "#     test_correct.append(tst_corr)\n",
    "        \n",
    "# print(f'\\nDuration: {time.time() - start_time:.0f} seconds') # print the time elapsed            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8024720d-f2f1-4f98-9993-546e7c895049",
   "metadata": {},
   "source": [
    "# MNIST with CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7da610-a2a8-4ba7-8d8d-10897cc2e61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import make_grid\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1390811a-4a2f-4bc8-ae9a-b9d1a40bf990",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb46fc4-dc58-4c3c-8459-bc30f81121f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.ToTensor()\n",
    "train_data = datasets.MNIST(root=r\"C:\\Users\\konde\\OneDrive\\PYTORCH_NOTEBOOKS\\PYTORCH_NOTEBOOKS\",train=True, download=True, transform=transform)\n",
    "test_data = datasets.MNIST(r\"C:\\Users\\konde\\OneDrive\\PYTORCH_NOTEBOOKS\\PYTORCH_NOTEBOOKS\",train=False, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0825338f-8914-4450-8b79-b99f6cfd697e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1032a56d-361c-4c9d-8d00-f2b19e2311eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f51297-4445-42fc-907e-8fb83fe0faea",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_data, batch_size=10, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=10, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5642b3-13e0-4d86-ada0-6796c500656b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 color channel, 6 FILTERS (output features), 3by3 Kernal, 1 stride\n",
    "conv1 = nn.Conv2d(1,6,3,1)\n",
    "# 6 output features ,16 filters ,3by3 kernal ,1 stride\n",
    "conv2 = nn.Conv2d(6,16,3,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a307e4-67af-4c84-a637-dd16b4733fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Grab the first MNIST record\n",
    "for i, (X_train, y_train) in enumerate(train_data):\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39871d1-a396-4207-a725-82c8708f310b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = X_train.view(1,1,28,28)  # 1 batch ,1 color channel,width and height 28,28 respectively\n",
    "print(x.shape)          #-----> 4d batch (batch of 1 image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98bafee-90b2-4d15-9dc0-cf908e4486f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the first convolution/activation\n",
    "x = F.relu(conv1(x))\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e15321-7cc2-4f5d-a3a0-c5261dd623d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the first pooling layer\n",
    "x = F.max_pool2d(x, 2, 2)      # 2 is for 2by2 window,and 2 is stride\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06df0d90-36de-4e21-b17c-dcaaf0a80cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the second convolution/activation\n",
    "x = F.relu(conv2(x))\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968b6be7-75b0-41b2-bdfa-c3158bdfb143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the second pooling layer\n",
    "x = F.max_pool2d(x, 2, 2)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f568fa-c1f9-412a-978e-62abc15bf519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the data\n",
    "x = x.view(-1, 5*5*16)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc77a27-95bc-4b25-b80f-15e38ab35953",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionalNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1,6,3,1)\n",
    "        self.conv2 = nn.Conv2d(6,16,3,1)\n",
    "        self.fc1 = nn.Linear(5*5*16,120)\n",
    "        self.fc2 = nn.Linear(120,84)\n",
    "        self.fc3 = nn.Linear(84,10)\n",
    "        \n",
    "    def forward(self,X):\n",
    "        X =F.relu(self.conv1(X))\n",
    "        X =F.max_pool2d(X,2,2)\n",
    "        X =F.relu(self.conv2(X))\n",
    "        X =F.max_pool2d(X,2,2)\n",
    "        X = X.view(-1, 5*5*16)\n",
    "        X = F.relu(self.fc1(X))\n",
    "        X = F.relu(self.fc2(X))\n",
    "        X = self.fc3(X)\n",
    "        return F.log_softmax(X, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e05d60-7e09-442c-8af6-9c1b737c6189",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "model = ConvolutionalNetwork()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad0b42f-68a5-4d73-a22d-4d0ab1071596",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    print(param.numel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a6e478-f95e-47d5-9c63-12ee1c170b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0c7c1c-e64b-4503-b07a-7b959ebba9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Define the CNN model\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 3, 1)\n",
    "        self.fc1 = nn.Linear(5*5*16, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = x.view(-1, 5*5*16)  # Flatten\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "# # Load the MNIST dataset\n",
    "transform = transforms.ToTensor()\n",
    "train_data = datasets.MNIST(root=r\"C:\\Users\\konde\\OneDrive\\PYTORCH_NOTEBOOKS\\PYTORCH_NOTEBOOKS\",train=True, download=True, transform=transform)\n",
    "test_data = datasets.MNIST(r\"C:\\Users\\konde\\OneDrive\\PYTORCH_NOTEBOOKS\\PYTORCH_NOTEBOOKS\",train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=64, shuffle=False)\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "model = SimpleCNN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop with loss & accuracy tracking\n",
    "epochs = 5\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss = 0\n",
    "    train_correct = 0\n",
    "    \n",
    "    # Training\n",
    "    model.train()\n",
    "    for X_train, y_train in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(X_train)\n",
    "        loss = criterion(y_pred, y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        train_correct += (y_pred.argmax(1) == y_train).sum().item()\n",
    "    \n",
    "    # Testing\n",
    "    test_loss = 0\n",
    "    test_correct = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for X_test, y_test in test_loader:\n",
    "            y_val = model(X_test)\n",
    "            loss = criterion(y_val, y_test)\n",
    "            test_loss += loss.item()\n",
    "            test_correct += (y_val.argmax(1) == y_test).sum().item()\n",
    "\n",
    "    # Calculate average loss and accuracy\n",
    "    train_losses.append(train_loss / len(train_loader))\n",
    "    test_losses.append(test_loss / len(test_loader))\n",
    "    train_accuracies.append(train_correct / len(train_data) * 100)\n",
    "    test_accuracies.append(test_correct / len(test_data) * 100)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs} - \"\n",
    "          f\"Train Loss: {train_losses[-1]:.4f}, Train Acc: {train_accuracies[-1]:.2f}% - \"\n",
    "          f\"Test Loss: {test_losses[-1]:.4f}, Test Acc: {test_accuracies[-1]:.2f}%\")\n",
    "\n",
    "print(f\"\\nTraining Time: {time.time() - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c9f431-12f9-4735-b159-d34867c66625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Training & Testing Loss\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(test_losses, label='Test Loss')\n",
    "plt.title(\"Loss over Epochs\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "\n",
    "# Plot Training & Testing Accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_accuracies, label='Train Accuracy')\n",
    "plt.plot(test_accuracies, label='Test Accuracy')\n",
    "plt.title(\"Accuracy over Epochs\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy (%)\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77cf70e-50b7-44ce-a5f1-bf6d34a55fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Load the full test dataset at once\n",
    "test_loader = DataLoader(test_data, batch_size=len(test_data), shuffle=False)\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "with torch.no_grad():  # No gradients needed for testing\n",
    "    for X_test, y_test in test_loader:\n",
    "        y_pred = model(X_test)            # Forward pass\n",
    "        predicted = y_pred.argmax(1)       # Get the predicted class\n",
    "        correct = (predicted == y_test).sum().item()  # Count correct predictions\n",
    "\n",
    "# Print test accuracy\n",
    "total_samples = len(test_data)\n",
    "accuracy = correct * 100 / total_samples\n",
    "print(f'Test Accuracy: {correct}/{total_samples} = {accuracy:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea190d6-f569-4505-bdd8-c8722e1c3cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 2019\n",
    "plt.figure(figsize=(1,1))\n",
    "plt.imshow(test_data[x][0].reshape((28,28)), cmap=\"gist_yarg\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de7a5c1-9129-49f1-8cce-ef155ad81c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    new_pred = model(test_data[x][0].view(1,1,28,28)).argmax()\n",
    "print(\"Predicted value:\",new_pred.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a254a3-2ba1-451f-8952-3458957d5994",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(predicted.view(-1), y_test.view(-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d17ce63-dc54-4e2f-ba4e-fd5667cfe520",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# class SimpleCNN(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(SimpleCNN, self).__init__()\n",
    "        \n",
    "#         # 1st Convolution Layer: 1 input channel (grayscale), 6 output channels, 3x3 kernel\n",
    "#         self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=3, stride=1)\n",
    "\n",
    "#         # 2nd Convolution Layer: 6 input channels (from conv1), 16 output channels, 3x3 kernel\n",
    "#         self.conv2 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=3, stride=1)\n",
    "\n",
    "#         # Fully Connected Layers\n",
    "#         self.fc1 = nn.Linear(in_features=5*5*16, out_features=120)  # First FC layer\n",
    "#         self.fc2 = nn.Linear(in_features=120, out_features=84)  # Second FC layer\n",
    "#         self.fc3 = nn.Linear(in_features=84, out_features=10)  # Output layer (10 classes)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # Apply 1st Convolution + ReLU Activation\n",
    "#         x = F.relu(self.conv1(x))\n",
    "\n",
    "#         # Max Pooling (2x2)\n",
    "#         x = F.max_pool2d(x, kernel_size=2, stride=2)\n",
    "\n",
    "#         # Apply 2nd Convolution + ReLU Activation\n",
    "#         x = F.relu(self.conv2(x))\n",
    "\n",
    "#         # Max Pooling (2x2)\n",
    "#         x = F.max_pool2d(x, kernel_size=2, stride=2)\n",
    "\n",
    "#         # Flatten the feature maps before feeding into Fully Connected layers\n",
    "#         x = x.view(-1, 5*5*16)\n",
    "\n",
    "#         # Fully Connected Layer 1 + ReLU\n",
    "#         x = F.relu(self.fc1(x))\n",
    "\n",
    "#         # Fully Connected Layer 2 + ReLU\n",
    "#         x = F.relu(self.fc2(x))\n",
    "\n",
    "#         # Final Output Layer (Log Softmax for Classification)\n",
    "#         x = self.fc3(x)\n",
    "#         return F.log_softmax(x, dim=1)\n",
    "\n",
    "# # Create a model instance\n",
    "# model = SimpleCNN()\n",
    "\n",
    "# # Example Input (1 image, 1 channel, 28x28)\n",
    "# sample_input = torch.randn(1, 1, 28, 28)\n",
    "\n",
    "# # Forward Pass (Get model output)\n",
    "# output = model(sample_input)\n",
    "\n",
    "# # Print Output Shape\n",
    "# print(\"Output Shape:\", output.shape)  # Should be (1, 10) since there are 10 classes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b0841f-ba4c-4c60-ba1d-a49f0c20f42e",
   "metadata": {},
   "source": [
    "# CIFAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752192d4-4bca-42a9-ac92-ab9844dd9f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sn  # for heatmaps\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6769ac6e-02d8-49ff-9268-d370ca0e7f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Convert to tensor\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize pixel values\n",
    "])\n",
    "train_data = datasets.CIFAR10(root=r\"C:\\Users\\konde\\OneDrive\\PYTORCH_NOTEBOOKS\\PYTORCH_NOTEBOOKS\",train=True, download=True, transform=transform)\n",
    "test_data = datasets.CIFAR10(r\"C:\\Users\\konde\\OneDrive\\PYTORCH_NOTEBOOKS\\PYTORCH_NOTEBOOKS\",train=False, download=True, transform=transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4e1bed-7ea0-4b9a-919a-5a65de0c4ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07503132-4d42-4cbb-965e-da6c259792b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd402b56-eedb-43a5-a0a0-5be45b7dbfa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a1e5d3-d458-45f9-bfb7-497715667543",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['plane', '  car', ' bird', '  cat', ' deer', '  dog', ' frog', 'horse', ' ship', 'truck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc637cd2-d83d-4e5c-9654-6f585bde6575",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# np.set_printoptions(formatter=dict(int=lambda x: f'{x:5}')) # to widen the printed array\n",
    "\n",
    "# # Grab the first batch of 10 images\n",
    "# for images,labels in train_loader: \n",
    "#     break\n",
    "\n",
    "# # Print the labels\n",
    "# print('Label:', labels.numpy())\n",
    "# print('Class: ', *np.array([class_names[i] for i in labels]))\n",
    "\n",
    "# # Print the images\n",
    "# im = make_grid(images, nrow=5)  # the default nrow is 8\n",
    "# plt.figure(figsize=(10,4))\n",
    "# plt.imshow(np.transpose(im.numpy(), (1, 2, 0)));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3f7ad2-f785-41a7-8999-8fa011f1d534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# im = (im + 1) / 2  # Rescale from [-1,1] to [0,1]\n",
    "# plt.imshow(np.transpose(im.numpy(), (1, 2, 0)))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a215dddf-babf-4f2a-a619-9b6281e10581",
   "metadata": {},
   "outputs": [],
   "source": [
    "for images,labels in train_loader:\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7f8c89-ced0-4737-9059-4c41896ee02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "im =make_grid(images,nrow=5)\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.imshow(np.transpose(im.numpy(),(1,2,0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8efd8d7-a7c1-44fa-848a-2d78af95e586",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf22b1f1-acfe-4eb5-aa12-8008307ec97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class SimpleCNN(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(SimpleCNN, self).__init__()\n",
    "#         self.conv1 = nn.Conv2d(3, 16, 3, padding=1)  # Conv Layer 1: 3 input channels (RGB), 16 filters, 3x3 kernel\n",
    "#         self.conv2 = nn.Conv2d(16, 32, 3, padding=1)  # Conv Layer 2: 16 input channels, 32 filters, 3x3 kernel\n",
    "#         self.fc1 = nn.Linear(8*8*32, 128)  # Fully Connected Layer 1: Flattened 8x8x32 to 128 neurons\n",
    "#         self.fc2 = nn.Linear(128, 64)  # Fully Connected Layer 2: 128 → 64 neurons\n",
    "#         self.fc3 = nn.Linear(64, 10)  # Output Layer: 64 → 10 (for 10 CIFAR-10 classes)\n",
    "        \n",
    "#     def forward(self,x):\n",
    "#         def forward(self, x):\n",
    "#             x = F.relu(self.conv1(x))  # Apply Conv1 + ReLU activation\n",
    "#             x = F.max_pool2d(x, 2, 2)  # Apply 2x2 Max Pooling\n",
    "#             x = F.relu(self.conv2(x))  # Apply Conv2 + ReLU activation\n",
    "#             x = F.max_pool2d(x, 2, 2)  # Apply 2x2 Max Pooling\n",
    "#             x = x.view(-1, 8*8*32)  # Flatten (reshape tensor)\n",
    "#             x = F.relu(self.fc1(x))  # Fully Connected Layer 1 + ReLU\n",
    "#             x = F.relu(self.fc2(x))  # Fully Connected Layer 2 + ReLU\n",
    "#             x = self.fc3(x)  # Output Layer (logits)\n",
    "#             return F.log_softmax(x, dim=1)  # Apply Log Softmax for classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48b31b2-c8ff-4e1a-b791-177666daedfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Initialize Model, Loss Function, and Optimizer\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Use GPU if available\n",
    "# model = SimpleCNN().to(device)\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fb8835-7167-46e1-8f95-bd4c5c331dfc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9be6bd-6988-4c99-8233-ffc989fb9fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# 1️⃣ Load the CIFAR-10 Dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Convert to tensor\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize pixel values\n",
    "])\n",
    "\n",
    "# train_data = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "# test_data = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=64, shuffle=False)\n",
    "\n",
    "# 2️⃣ Define the CNN Model\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, 3, padding=1)  # Conv Layer 1: 3 input channels (RGB), 16 filters, 3x3 kernel\n",
    "        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)  # Conv Layer 2: 16 input channels, 32 filters, 3x3 kernel\n",
    "        self.fc1 = nn.Linear(8*8*32, 128)  # Fully Connected Layer 1: Flattened 8x8x32 to 128 neurons\n",
    "        self.fc2 = nn.Linear(128, 64)  # Fully Connected Layer 2: 128 → 64 neurons\n",
    "        self.fc3 = nn.Linear(64, 10)  # Output Layer: 64 → 10 (for 10 CIFAR-10 classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))  # Apply Conv1 + ReLU activation\n",
    "        x = F.max_pool2d(x, 2, 2)  # Apply 2x2 Max Pooling\n",
    "        x = F.relu(self.conv2(x))  # Apply Conv2 + ReLU activation\n",
    "        x = F.max_pool2d(x, 2, 2)  # Apply 2x2 Max Pooling\n",
    "        x = x.view(-1, 8*8*32)  # Flatten (reshape tensor)\n",
    "        x = F.relu(self.fc1(x))  # Fully Connected Layer 1 + ReLU\n",
    "        x = F.relu(self.fc2(x))  # Fully Connected Layer 2 + ReLU\n",
    "        x = self.fc3(x)  # Output Layer (logits)\n",
    "        return F.log_softmax(x, dim=1)  # Apply Log Softmax for classification\n",
    "\n",
    "# 3️⃣ Initialize Model, Loss Function, and Optimizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Use GPU if available\n",
    "model = SimpleCNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 4️⃣ Training the Model\n",
    "epochs = 10\n",
    "train_losses, test_losses = [], []\n",
    "train_accuracies, test_accuracies = [], []\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss, train_correct = 0, 0\n",
    "    \n",
    "    model.train()  # Set model to training mode\n",
    "    for X_train, y_train in train_loader:\n",
    "        X_train, y_train = X_train.to(device), y_train.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(X_train)  # Forward pass\n",
    "        loss = criterion(y_pred, y_train)  # Compute loss\n",
    "        loss.backward()  # Backpropagation\n",
    "        optimizer.step()  # Update weights\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        train_correct += (y_pred.argmax(1) == y_train).sum().item()\n",
    "\n",
    "    test_loss, test_correct = 0, 0\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        for X_test, y_test in test_loader:\n",
    "            X_test, y_test = X_test.to(device), y_test.to(device)\n",
    "            y_val = model(X_test)\n",
    "            loss = criterion(y_val, y_test)\n",
    "            test_loss += loss.item()\n",
    "            test_correct += (y_val.argmax(1) == y_test).sum().item()\n",
    "\n",
    "    # Store loss and accuracy\n",
    "    train_losses.append(train_loss / len(train_loader))\n",
    "    test_losses.append(test_loss / len(test_loader))\n",
    "    train_accuracies.append(train_correct / len(train_data) * 100)\n",
    "    test_accuracies.append(test_correct / len(test_data) * 100)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs} - \"\n",
    "          f\"Train Loss: {train_losses[-1]:.4f}, Train Acc: {train_accuracies[-1]:.2f}% - \"\n",
    "          f\"Test Loss: {test_losses[-1]:.4f}, Test Acc: {test_accuracies[-1]:.2f}%\")\n",
    "\n",
    "print(f\"\\nTraining Time: {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "# 5️⃣ Plot Loss & Accuracy\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "# Loss Plot\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label=\"Train Loss\")\n",
    "plt.plot(test_losses, label=\"Test Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss Over Epochs\")\n",
    "plt.legend()\n",
    "\n",
    "# Accuracy Plot\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_accuracies, label=\"Train Accuracy\")\n",
    "plt.plot(test_accuracies, label=\"Test Accuracy\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy (%)\")\n",
    "plt.title(\"Accuracy Over Epochs\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# 6️⃣ Evaluate Model on Entire Test Set\n",
    "test_load_all = torch.utils.data.DataLoader(test_data, batch_size=len(test_data), shuffle=False)\n",
    "\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    for X_test, y_test in test_load_all:\n",
    "        X_test, y_test = X_test.to(device), y_test.to(device)\n",
    "        y_val = model(X_test)\n",
    "        predicted = y_val.argmax(1)\n",
    "        correct += (predicted == y_test).sum().item()\n",
    "\n",
    "print(f\"\\nFinal Test Accuracy: {correct}/{len(test_data)} = {correct * 100 / len(test_data):.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42dc054-87a1-4be2-b89c-c278fb5ad1a8",
   "metadata": {},
   "source": [
    "# Realworld Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9510ccd-224a-48d8-a5b8-ee8a9e53d8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1232a761-6705-42d4-9359-f7bd8017b73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "\n",
    "# Filter harmless warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7aa084d-2579-494a-ad0a-57b8d6d74c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with Image.open(r\"C:\\Users\\konde\\Downloads\\CATS_DOGS\\CATS_DOGS\\train\\DOG\\987.jpg\") as im:\n",
    "    display(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37da7508-227b-4fce-869f-f30c044aef22",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"C:\\Users\\konde\\Downloads\\CATS_DOGS\\CATS_DOGS\"\n",
    "img_names = []\n",
    "for folders,subfolders,filenames in os.walk(path):\n",
    "    for img in filenames:\n",
    "        img_names.append(folders+'/'+img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83425959-bb60-4da5-8b3f-73bac0cdc590",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Images: ',len(img_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ce3f01-5e58-44fa-9598-541492064c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start by creating a list\n",
    "img_sizes = []\n",
    "rejected = []\n",
    "\n",
    "for item in img_names:\n",
    "    try:\n",
    "        with Image.open(item) as img:\n",
    "            img_sizes.append(img.size)\n",
    "    except:\n",
    "        rejected.append(item)\n",
    "        \n",
    "print(f'Images:  {len(img_sizes)}')\n",
    "print(f'Rejects: {len(rejected)}')\n",
    "\n",
    "# from PIL import Image\n",
    "# from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# img_sizes = []\n",
    "# rejected = []\n",
    "\n",
    "# def process_image(item):\n",
    "#     try:\n",
    "#         with Image.open(item) as img:\n",
    "#             img.verify()\n",
    "#             return (item, img.size)\n",
    "#     except:\n",
    "#         return (item, None)\n",
    "\n",
    "# with ThreadPoolExecutor() as executor:\n",
    "#     results = executor.map(process_image, img_names)\n",
    "\n",
    "# for item, size in results:\n",
    "#     if size:\n",
    "#         img_sizes.append(size)\n",
    "#     else:\n",
    "#         rejected.append(item)\n",
    "\n",
    "# print(f'Images:  {len(img_sizes)}')\n",
    "# print(f'Rejects: {len(rejected)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f7275d-296b-4188-86fa-6cadebbd2d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(img_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc23f07a-d6a3-463a-9aef-397941aa867d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30d21a4-38a1-4d32-9c7b-9b7f654738d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[0].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec77fb4-65fa-4816-97b3-76f21e31f72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[1].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c38440-1557-42c3-b18d-a5819b7600f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dog = Image.open(r\"C:\\Users\\konde\\Downloads\\CATS_DOGS\\CATS_DOGS\\train\\DOG\\986.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0353a6e-7504-454f-9ed4-6a39a0295d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(dog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f80a33a-63f7-47bd-be0e-de00bc2b9761",
   "metadata": {},
   "outputs": [],
   "source": [
    "dog.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15d0eca-340e-420f-8261-a53b9d7f8ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "r, g, b = dog.getpixel((0, 0))\n",
    "print(r,g,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c13eb7-f064-4d23-ae56-82f3752f2d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "im = transform(dog)\n",
    "print(im.shape)\n",
    "plt.imshow(np.transpose(im.numpy(), (1, 2, 0)));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80fc7cdc-df14-47fe-996b-2350f9d51e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57bd4946-80e7-49f8-82be-c0c266556bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((1000,500)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "im = transform(dog)\n",
    "print(im.shape)\n",
    "plt.imshow(np.transpose(im.numpy(), (1, 2, 0)));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04576435-3832-439a-94b2-016b35f4521b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f93ff87-f904-43da-8ced-410eaacce836",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize(250),\n",
    "    transforms.CenterCrop(250),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "im = transform(dog)\n",
    "print(im.shape)\n",
    "plt.imshow(np.transpose(im.numpy(), (1, 2, 0)));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f66ec2-78c7-4f08-b826-ea05234431fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db03a5f-7855-49c9-aee9-82b41acb779d",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(p=1),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "im = transform(dog)\n",
    "print(im.shape)\n",
    "plt.imshow(np.transpose(im.numpy(), (1, 2, 0)));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defa74a9-a4a8-4578-8d83-c789647a2cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc9ece8-0a60-4b1d-9240-2a75d9b70666",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.RandomRotation(45),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "im = transform(dog)\n",
    "print(im.shape)\n",
    "plt.imshow(np.transpose(im.numpy(), (1, 2, 0)));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19cd3fe0-d38d-476d-9f6e-6352b254c7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b035f8e-6e20-4657-8d7a-ff45b851e1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(p=1),  # normally we'd set p=0.5\n",
    "    transforms.RandomRotation(30),\n",
    "    transforms.Resize(224),\n",
    "    transforms.CenterCrop(224), \n",
    "    transforms.ToTensor()\n",
    "])\n",
    "im = transform(dog)\n",
    "print(im.shape)\n",
    "plt.imshow(np.transpose(im.numpy(), (1, 2, 0)));\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a237fd5b-01d8-4336-8776-d65602aec406",
   "metadata": {},
   "source": [
    "# Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931800b0-f2d3-49a1-b007-b3bc1fd60c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                         [0.229, 0.224, 0.225])\n",
    "])\n",
    "im = transform(dog)\n",
    "print(im.shape)\n",
    "plt.imshow(np.transpose(im.numpy(), (1, 2, 0)));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7a946a-c732-46e8-8167-d6dbca8f63d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330f1bc2-1369-490b-aced-d0d04896683f",
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_normalize = transforms.Normalize(\n",
    "    mean=[-0.485/0.229, -0.456/0.224, -0.406/0.225],\n",
    "    std=[1/0.229, 1/0.224, 1/0.225]\n",
    ")\n",
    "im_inv = inv_normalize(im)\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.imshow(np.transpose(im_inv.numpy(), (1, 2, 0)));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ce45ef-4dd0-40ab-a3e7-a9355350c4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8b6530-9c01-443b-b01a-9310ea3bd0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets,transforms,models\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480db5f6-3eee-4629-ad40-6f43e53bbc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.Resize(224),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                             [0.229, 0.224, 0.225]),\n",
    "    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79bf138-fc51-4f0f-9431-ebe4b8b1344a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                             [0.229, 0.224, 0.225]),\n",
    "    \n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18f91b6-1ec8-4129-af6b-9698840b9bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = r\"C:\\Users\\konde\\Downloads\\CATS_DOGS\\CATS_DOGS\"\n",
    "\n",
    "train_data = datasets.ImageFolder(os.path.join(root,'train'),transform = train_transform)\n",
    "test_data = datasets.ImageFolder(os.path.join(root,'test'),transform = test_transform)\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "train_loader = DataLoader(train_data,batch_size=10,shuffle=True)\n",
    "test_loader = DataLoader(test_data,batch_size=10)\n",
    "\n",
    "class_names = train_data.classes\n",
    "print(class_names)\n",
    "print(f'Training images available: {len(train_data)}')\n",
    "print(f'Testing images available:  {len(test_data)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f453e59-ed39-4a13-9134-2348d1c23305",
   "metadata": {},
   "outputs": [],
   "source": [
    "for images,labels in train_loader: \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025e23d9-3c74-4903-9a04-4d465628c3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01cf21c4-6223-43b3-98c1-046f14b88018",
   "metadata": {},
   "outputs": [],
   "source": [
    "im = make_grid(images, nrow=5) \n",
    "\n",
    "inv_normalize = transforms.Normalize(\n",
    "    mean=[-0.485/0.229, -0.456/0.224, -0.406/0.225],\n",
    "    std=[1/0.229, 1/0.224, 1/0.225]\n",
    ")\n",
    "im_inv = inv_normalize(im)\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.imshow(np.transpose(im_inv.numpy(), (1, 2, 0)));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c11e11-1103-466d-ba38-910dca71d2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9edb69-e9f1-466e-9da2-75f37b7cee71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5497fcb2-bf7a-4312-8c18-e409cb83c8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# Define a simple Convolutional Neural Network\n",
    "class ConvolutionalNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvolutionalNetwork, self).__init__()\n",
    "\n",
    "        # First convolutional layer (input: 3 channels, output: 6 channels, kernel size: 3x3)\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=6, kernel_size=3, stride=1)\n",
    "\n",
    "        # Second convolutional layer (input: 6 channels, output: 16 channels, kernel size: 3x3)\n",
    "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=3, stride=1)\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(16 * 54 * 54, 120)  # Adjusted for 128x128 image input\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 2)  # Output layer (2 classes: Cat & Dog)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply first convolution, then ReLU activation, then max pooling\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, kernel_size=2, stride=2)\n",
    "\n",
    "        # Apply second convolution, then ReLU activation, then max pooling\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, kernel_size=2, stride=2)\n",
    "\n",
    "        # Flatten the feature maps into a vector for the fully connected layers\n",
    "        x = x.view(x.size(0), -1)  # Reshapes tensor to (batch_size, flattened_size,32,2352 respectively)\n",
    "\n",
    "        # Fully connected layers with ReLU activation\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "\n",
    "        # Final layer (logits output)\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        # Apply log softmax for classification\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "# Create an instance of the model\n",
    "CNNmodel = ConvolutionalNetwork()\n",
    "torch.manual_seed(101)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(CNNmodel.parameters(),lr=0.001)\n",
    "# Print model architecture\n",
    "print(CNNmodel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc0c64d-0887-4ada-9429-a7e5bc6f8fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (((224-2)/2)-2)/2 = 54.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc00d43-4b4f-46d5-9688-c2b311f9c617",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8092804c-107b-4532-b3a9-c01406c7c7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in CNNmodel.parameters():\n",
    "    print(p.numel())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf89582-3f56-4786-b105-a31b2203b12c",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ee06cc-22cd-4421-8636-0ceabcf99e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time  # Import time module to measure execution time\n",
    "start_time = time.time()  # Start the timer\n",
    "\n",
    "# Number of training epochs\n",
    "epochs = 3  \n",
    "\n",
    "# Maximum batches to process for training and testing\n",
    "max_trn_batch = 800      # 800 *10batch = 8000 images \n",
    "max_tst_batch = 300      # 300 *10batch = 3000 images\n",
    "\n",
    "# Lists to store loss and accuracy metrics\n",
    "train_losses = []  \n",
    "test_losses = []  \n",
    "train_correct = []  \n",
    "test_correct = []  \n",
    "\n",
    "# Loop through the number of epochs\n",
    "for i in range(epochs):  \n",
    "    trn_corr = 0  # Initialize correct predictions counter for training\n",
    "    tst_corr = 0  # Initialize correct predictions counter for testing\n",
    "\n",
    "    # Run the training batches\n",
    "    for b, (X_train, y_train) in enumerate(train_loader):  \n",
    "        \n",
    "        # Stop training after reaching the max batch limit\n",
    "        if b == max_trn_batch:  \n",
    "            break  \n",
    "        b += 1  # Increase batch count\n",
    "\n",
    "        # Apply the model on training data\n",
    "        y_pred = CNNmodel(X_train)  \n",
    "        loss = criterion(y_pred, y_train)  # Calculate loss\n",
    "\n",
    "        # Get predictions (index of max logit)\n",
    "        predicted = torch.max(y_pred.data, 1)[1]  \n",
    "        batch_corr = (predicted == y_train).sum()  # Count correct predictions in batch\n",
    "        trn_corr += batch_corr  # Add batch correct count to total\n",
    "\n",
    "        # Backpropagation (update model parameters)\n",
    "        optimizer.zero_grad()  # Reset gradients\n",
    "        loss.backward()  # Compute gradients\n",
    "        optimizer.step()  # Update model weights\n",
    "\n",
    "        # Print progress every 200 batches\n",
    "        if b % 200 == 0:  \n",
    "            print(f'epoch: {i:2}  batch: {b:4} [{10*b:6}/8000]  loss: {loss.item():10.8f}  \\\n",
    "accuracy: {trn_corr.item()*100/(10*b):7.3f}%')\n",
    "\n",
    "    # Store training loss and accuracy\n",
    "    train_losses.append(loss)  \n",
    "    train_correct.append(trn_corr)  \n",
    "\n",
    "    # Run the testing batches without updating gradients\n",
    "    with torch.no_grad():  \n",
    "        for b, (X_test, y_test) in enumerate(test_loader):  \n",
    "            \n",
    "            # Stop testing after reaching the max batch limit\n",
    "            if b == max_tst_batch:  \n",
    "                break  \n",
    "\n",
    "            # Apply the model on test data\n",
    "            y_val = CNNmodel(X_test)  \n",
    "\n",
    "            # Get predictions and count correct ones\n",
    "            predicted = torch.max(y_val.data, 1)[1]  \n",
    "            tst_corr += (predicted == y_test).sum()  \n",
    "\n",
    "    # Compute test loss\n",
    "    loss = criterion(y_val, y_test)  \n",
    "    test_losses.append(loss)  \n",
    "    test_correct.append(tst_corr)  \n",
    "\n",
    "# Print total execution time\n",
    "total_time = time.time() - start_time\n",
    "print(f'\\nDuration: {total_time/60:.0f} seconds')  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b1f35b-dc46-42e1-9745-d179363bf384",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'\\nDuration: {total_time/60} minutes')  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c9c458-87f9-4471-8b8f-ee3b81a48bab",
   "metadata": {},
   "source": [
    "# save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221bbdc2-2cf3-4376-92c2-0442d08f396f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(CNNmodel.state_dict(), 'CustomImageCNNModel.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce187f02-37d6-4227-8f6b-06da93cde5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(train_losses, label='training loss')\n",
    "# plt.plot(test_losses, label='validation loss')\n",
    "# plt.title('Loss at the end of each epoch')\n",
    "# plt.legend();\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Convert tensors to NumPy arrays before plotting\n",
    "train_losses_np = [loss.detach().numpy() for loss in train_losses]\n",
    "test_losses_np = [loss.detach().numpy() for loss in test_losses]\n",
    "\n",
    "plt.plot(train_losses_np, label='Training Loss')\n",
    "plt.plot(test_losses_np, label='Validation Loss')\n",
    "plt.title('Loss at the End of Each Epoch')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef8f9c6-1dbf-493a-8728-7c41b3333061",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot([t/80 for t in train_correct], label='training accuracy')\n",
    "# plt.plot([t/30 for t in test_correct], label='validation accuracy')\n",
    "# plt.title('Accuracy at the end of each epoch')\n",
    "# plt.legend();\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Convert tensor lists to NumPy arrays\n",
    "train_acc_np = [acc.detach().numpy() for acc in train_correct]\n",
    "test_acc_np = [acc.detach().numpy() for acc in test_correct]\n",
    "\n",
    "# Convert to percentage accuracy\n",
    "# train_acc_percent = [acc / len(train_loader.dataset) * 100 for acc in train_acc_np]\n",
    "# test_acc_percent = [acc / len(test_loader.dataset) * 100 for acc in test_acc_np]\n",
    "\n",
    "# Plot training and test accuracy\n",
    "# plt.plot(train_acc_np) #label='Training Accuracy')\n",
    "# plt.plot(test_acc_np) #label='Validation Accuracy')\n",
    "\n",
    "# plt.title('Training vs. Validation Accuracy')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.ylabel('Accuracy (%)')\n",
    "# plt.legend()\n",
    "# #plt.ylim(0,100)\n",
    "# plt.show()\n",
    "plt.plot(train_acc_np,label='training accuracy')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9146f92c-1701-4f56-ab5e-6d314ed5c325",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(test_acc_np,label = 'validation acuuracy')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37276a2e-c816-4a39-8b09-d866e9dd93d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbaa5861-6b32-499a-93a3-62276c3233cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time  \n",
    "# import torch  \n",
    "\n",
    "# start_time = time.time()  # Start timer  \n",
    "\n",
    "# epochs = 3  \n",
    "# max_trn_batch = 800  \n",
    "# max_tst_batch = 300  \n",
    "\n",
    "# train_losses, test_losses = [], []  \n",
    "# train_correct, test_correct = [], []  \n",
    "\n",
    "# for i in range(epochs):  \n",
    "#     trn_corr, tst_corr = 0, 0  \n",
    "\n",
    "#     # Training loop  \n",
    "#     for b, (X_train, y_train) in enumerate(train_loader):  \n",
    "#         if b == max_trn_batch:  \n",
    "#             break  \n",
    "\n",
    "#         y_pred = CNNmodel(X_train)  # Forward pass  \n",
    "#         loss = criterion(y_pred, y_train)  \n",
    "\n",
    "#         # Compute accuracy  \n",
    "#         predicted = torch.max(y_pred, 1)[1]  \n",
    "#         trn_corr += (predicted == y_train).sum()  \n",
    "\n",
    "#         # Backpropagation  \n",
    "#         optimizer.zero_grad()  \n",
    "#         loss.backward()  \n",
    "#         optimizer.step()  \n",
    "\n",
    "#         if b % 200 == 0:  \n",
    "#             print(f'Epoch {i}, Batch {b}, Loss: {loss.item():.4f}, Accuracy: {trn_corr.item()*100/(10*b):.2f}%')\n",
    "\n",
    "#     train_losses.append(loss)  \n",
    "#     train_correct.append(trn_corr)  \n",
    "\n",
    "#     # Testing loop  \n",
    "#     with torch.no_grad():  \n",
    "#         for b, (X_test, y_test) in enumerate(test_loader):  \n",
    "#             if b == max_tst_batch:  \n",
    "#                 break  \n",
    "\n",
    "#             y_val = CNNmodel(X_test)  \n",
    "#             predicted = torch.max(y_val, 1)[1]  \n",
    "#             tst_corr += (predicted == y_test).sum()  \n",
    "\n",
    "#     test_losses.append(criterion(y_val, y_test))  \n",
    "#     test_correct.append(tst_corr)  \n",
    "# import torch  \n",
    "\n",
    "# def evaluate_model(model, test_loader, criterion):\n",
    "#     model.eval()  # Set model to evaluation mode  \n",
    "#     test_loss = 0  \n",
    "#     correct = 0  \n",
    "#     total = 0  \n",
    "\n",
    "#     with torch.no_grad():  # No gradient calculation needed  \n",
    "#         for X_test, y_test in test_loader:  \n",
    "#             y_pred = model(X_test)  # Forward pass  \n",
    "#             loss = criterion(y_pred, y_test)  \n",
    "#             test_loss += loss.item()  \n",
    "\n",
    "#             # Get predicted class labels  \n",
    "#             predicted = torch.max(y_pred, 1)[1]  \n",
    "#             correct += (predicted == y_test).sum().item()  \n",
    "#             total += y_test.size(0)  \n",
    "\n",
    "#     avg_loss = test_loss / len(test_loader)  \n",
    "#     accuracy = 100 * correct / total  \n",
    "\n",
    "#     print(f'Test Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%')\n",
    "\n",
    "#     return avg_loss, accuracy  \n",
    "\n",
    "# # Call the function  \n",
    "# test_loss, test_accuracy = evaluate_model(CNNmodel, test_loader, criterion)\n",
    "\n",
    "\n",
    "# print(f'\\nTraining completed in {time.time() - start_time:.0f} seconds')  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ffd3f5-7531-439e-9792-4315ed2629ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "AlexNetmodel = models.alexnet(pretrained=True)\n",
    "AlexNetmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c50e72-2c39-45b2-94d3-173100fa6a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in AlexNetmodel.parameters():\n",
    "    param.requries_grad=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95363b6-911b-4b83-b359-ef87c087e477",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "AlexNetmodel.classifier = nn.Sequential(nn.Linear(9216,1024),\n",
    "                                       nn.ReLU(),\n",
    "                                       nn.Linear(1024,2),\n",
    "                                       nn.LogSoftmax(dim=1),\n",
    "                                       )\n",
    "AlexNetmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61a3987-6cb7-4704-8e39-516c589d6254",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in AlexNetmodel.parameters():\n",
    "    print(param.numel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c3f423-c25e-41e7-b296-d0ba819da49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(AlexNetmodel.classifier.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3fe1a3e-b25b-4063-abd0-ddf99c18e93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "epochs = 1\n",
    "\n",
    "max_trn_batch = 800\n",
    "max_tst_batch = 300\n",
    "\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "train_correct = []\n",
    "test_correct = []\n",
    "\n",
    "for i in range(epochs):\n",
    "    trn_corr = 0\n",
    "    tst_corr = 0\n",
    "    \n",
    "    # Run the training batches\n",
    "    for b, (X_train, y_train) in enumerate(train_loader):\n",
    "        if b == max_trn_batch:\n",
    "            break\n",
    "        b+=1\n",
    "        \n",
    "        # Apply the model\n",
    "        y_pred = AlexNetmodel(X_train)\n",
    "        loss = criterion(y_pred, y_train)\n",
    " \n",
    "        # Tally the number of correct predictions\n",
    "        predicted = torch.max(y_pred.data, 1)[1]\n",
    "        batch_corr = (predicted == y_train).sum()\n",
    "        trn_corr += batch_corr\n",
    "        \n",
    "        # Update parameters\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print interim results\n",
    "        if b%200 == 0:\n",
    "            print(f'epoch: {i:2}  batch: {b:4} [{10*b:6}/8000]  loss: {loss.item():10.8f}  \\\n",
    "accuracy: {trn_corr.item()*100/(10*b):7.3f}%')\n",
    "\n",
    "    train_losses.append(loss)\n",
    "    train_correct.append(trn_corr)\n",
    "\n",
    "    # Run the testing batches\n",
    "    with torch.no_grad():\n",
    "        for b, (X_test, y_test) in enumerate(test_loader):\n",
    "            if b == max_tst_batch:\n",
    "                break\n",
    "\n",
    "            # Apply the model\n",
    "            y_val = AlexNetmodel(X_test)\n",
    "\n",
    "            # Tally the number of correct predictions\n",
    "            predicted = torch.max(y_val.data, 1)[1] \n",
    "            tst_corr += (predicted == y_test).sum()\n",
    "\n",
    "    loss = criterion(y_val, y_test)\n",
    "    test_losses.append(loss)\n",
    "    test_correct.append(tst_corr)\n",
    "\n",
    "print(f'\\nDuration: {time.time() - start_time:.0f} seconds') # print the time elapsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da425ff-87da-4769-a5be-9aef294587d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 2000\n",
    "im = inv_normalize(test_data[x][0])\n",
    "plt.imshow(np.transpose(im.numpy(), (1, 2, 0)));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965d5bf5-3093-461e-899a-7245a404675c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27c27c6-9eee-4350-ba4e-1bbb659c99e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "CNNmodel.eval()\n",
    "with torch.no_grad():\n",
    "    new_pred = CNNmodel(test_data[x][0].view(1,3,224,224)).argmax()\n",
    "    print(class_names[new_pred.item()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800f6650-2fd8-434b-a76d-09927953b70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(new_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb249f8-60a2-4813-bbaa-dd0063ea06a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "AlexNetmodel.eval()\n",
    "with torch.no_grad():\n",
    "    new_pred = CNNmodel(test_data[x][0].view(1,3,224,224)).argmax()\n",
    "    print(class_names[new_pred.item()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1752f7-1153-4982-80b6-780dd32656ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "CNNmodel.eval()  # Set model to evaluation mode\n",
    "\n",
    "with torch.no_grad():  # Disable gradient calculations\n",
    "    img = test_data[x][0]  # Get the image from the dataset\n",
    "\n",
    "    # Ensure it has batch dimension: (1, 3, 224, 224)\n",
    "    img = img.unsqueeze(0)  \n",
    "\n",
    "    # Get predictions from the model\n",
    "    output = CNNmodel(img)\n",
    "\n",
    "    # Get the predicted class index\n",
    "    new_pred = output.argmax(dim=1)  \n",
    "\n",
    "    # Convert index to class name\n",
    "    predicted_class = class_names[new_pred.item()]\n",
    "\n",
    "print(\"Predicted Class:\", predicted_class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8099617d-a312-483e-93eb-6d19d0ea0618",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36a0a61-db69-44e3-8869-080e292c806e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df02da5c-54bc-4bba-beef-f836a584d522",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.ToTensor()\n",
    "\n",
    "train_data = datasets.FashionMNIST(root=r\"C:\\Users\\konde\\Downloads\\fashion_gray_images\", train=True, download=True, transform=transform)\n",
    "test_data = datasets.FashionMNIST(root=r\"C:\\Users\\konde\\Downloads\\fashion_gray_images\", train=False, download=True, transform=transform)\n",
    "\n",
    "class_names = ['T-shirt','Trouser','Sweater','Dress','Coat','Sandal','Shirt','Sneaker','Bag','Boot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65da04f-bb4d-4df1-b6c9-cdbbd8261453",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_data,batch_size=10,shuffle=True)\n",
    "test_loader = DataLoader(test_data,batch_size=10,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30fb7fef-7b0b-48ee-bbc8-01d64441dbbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for images,labels in train_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4019bf16-da65-4b61-88f0-31b593e40cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5362d1-3f68-4651-8c43-017ca11473b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "im=make_grid(images)\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.imshow(np.transpose(im.numpy(),(1,2,0)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93df5c9d-4946-48b2-88c0-e9734e80fc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = nn.Conv2d(1,1,5,1)\n",
    "for x,labels in train_loader:\n",
    "    break\n",
    "print('original size:',x.shape)\n",
    "x=conv(x)\n",
    "print('down size:',x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3fd46a-c224-44f6-b5e8-77ad98fe2b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=F.max_pool2d(x,2,2)\n",
    "print('down size:',x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382a709e-a989-4b5e-a832-df006469d474",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ConvolutionalNetwork(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         self.conv1 = nn.Conv2d(1,6,3,1)  # 1 color channel ,6 filters, 3 kernal size ,stride=1\n",
    "#         self.conv2 = nn.Conv2d(6,16,3,1)\n",
    "#         self.fc1 = nn.Linear(5*5*16,100)\n",
    "#         self.fc2 = nn.Linear(100,10)\n",
    "        \n",
    "#     def forward(self,x):\n",
    "#         X = F.relu(self.conv1(X))\n",
    "#         X = F.max_pool2d(X,2,2) #(x, kernel_size=2, stride=2)\n",
    "#         X = F.Relu(self.conv2(X))\n",
    "#         X = F.max_pool2d(X,2,2)\n",
    "#         X = X.view(-1,5*5*16)    #X = X.view(X.size(0), 5*5*16) \n",
    "#         X = F.relu(self.conv1(X))\n",
    "#         X = self.fc2(X)\n",
    "#         return F.log_softmax(X,dim=1)\n",
    "\n",
    "# torch.manual_seed(101)\n",
    "# model = ConvolutionalNetwork()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3976c1-58bc-43ce-ba71-4e2187faa94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "(((28-2)/2)-2)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9efc4d0-87f6-4fdf-b8a7-0ea313e22ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39d7388-f3f7-4db2-8595-c69ff097d72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in model.parameters():\n",
    "    print(p.numel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182f7cf4-580f-4738-ae13-019d0c74e943",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion =nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbdf4de1-67f3-494c-98ca-84c4aedcce6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ConvolutionalNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 3, 1)  # 1 input channel, 6 filters, 3x3 kernel, stride=1\n",
    "        self.conv2 = nn.Conv2d(6, 16, 3, 1) # 6 input channels → 16 filters\n",
    "        self.fc1 = nn.Linear(5*5*16, 100)   # Flattened layer with 100 neurons\n",
    "        self.fc2 = nn.Linear(100, 10)       # Output layer with 10 neurons (for classification)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))         # First convolutional layer + activation\n",
    "        x = F.max_pool2d(x, 2, 2)         # Max pooling\n",
    "        x = F.relu(self.conv2(x))         # Second convolutional layer + activation\n",
    "        x = F.max_pool2d(x, 2, 2)         # Max pooling\n",
    "\n",
    "        x = x.view(-1, 5*5*16)            # Flatten the tensor for the fully connected layer\n",
    "        x = F.relu(self.fc1(x))           # Fully connected layer + activation\n",
    "        x = self.fc2(x)                   # Output layer\n",
    "        return F.log_softmax(x, dim=1)    # Log softmax for classification\n",
    "\n",
    "torch.manual_seed(101)\n",
    "model = ConvolutionalNetwork()\n",
    "\n",
    "# DON'T WRITE HERE\n",
    "epochs = 5\n",
    "\n",
    "for i in range(epochs):\n",
    "    for X_train, y_train in train_loader:\n",
    "\n",
    "        # Apply the model\n",
    "        y_pred = model(X_train)\n",
    "        loss = criterion(y_pred, y_train)\n",
    "\n",
    "        # Update parameters\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # OPTIONAL print statement\n",
    "    print(f'{i+1} of {epochs} epochs completed')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05f4e8c-e2e5-4812-bcfb-d5fd91562707",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    for X_test,y_test in test_loader:\n",
    "        y_val = model(X_test)\n",
    "        predicted = torch.max(y_val,1)[1]\n",
    "        correct += (predicted == y_test).sum()\n",
    "        \n",
    "print(f'Test accuracy: {correct.item()}/{len(test_data)} = {correct.item()*100/(len(test_data)):7.3f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa4c8a4-b11e-47fd-9195-c76a5da14404",
   "metadata": {},
   "source": [
    "# Basic RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99216be-e078-4a39-831d-a1e4d42bb790",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7165dfaa-15c6-49fa-9355-a5028df5be0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.linspace(0,799,steps=800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5544c48-6cb4-4b6c-afe2-ebc0616f72e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823d062a-4098-4626-a41d-f302302b1111",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.sin(x*2*3.1416/40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb15c00-66d3-428a-a359-556f1de762c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "plt.xlim(-10,801)\n",
    "plt.grid(True)\n",
    "plt.plot(y.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84df363-b710-4ffa-87e3-48a52a5aefc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b199d338-a24d-45fa-b553-8dd45bae2ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 40\n",
    "\n",
    "train_set = y[:-test_size]\n",
    "test_set = y[-test_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a30a5c2-3388-438c-97b4-4256957fedf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,4))\n",
    "plt.xlim(-10,801)\n",
    "plt.grid(True)\n",
    "plt.plot(train_set.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ff0273-bf27-4f93-b38b-89babf0203cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7913e4d-930c-42b1-9019-7e3a9f56b076",
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_data(seq,ws):  # ws is the window size\n",
    "    out = []\n",
    "    L = len(seq)\n",
    "    for i in range(L-ws):\n",
    "        window = seq[i:i+ws]\n",
    "        label = seq[i+ws:i+ws+1]\n",
    "        out.append((window,label))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90ff1fe-7a52-4c34-b533-d3517bb8b298",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seq = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "# ws = 3\n",
    "# i = 0  # First iteration\n",
    "\n",
    "# window = seq[i:i+ws]  # seq[0:3]\n",
    "# print(window)  # Output: [1, 2, 3]\n",
    "# window = seq[1:4]  # Output: [2, 3, 4]   2st iteration\n",
    "\n",
    "# #Extracting label (Target Output)\n",
    "# seq = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "# ws = 3\n",
    "# i = 0  # First iteration\n",
    "\n",
    "# label = seq[i+ws:i+ws+1]  # seq[3:4]\n",
    "# print(label)  # Output: [4]\n",
    "# label = seq[4:5]  # Output: [5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26af4ad2-92d5-4abb-a175-3e7536611bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From above:\n",
    "# test_size = 40\n",
    "# train_set = y[:-test_size]\n",
    "# test_set = y[-test_size:]\n",
    "\n",
    "window_size = 40\n",
    "\n",
    "# Create the training dataset of sequence/label tuples:\n",
    "train_data = input_data(train_set,window_size)\n",
    "\n",
    "len(train_data) # this should equal 760-40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210cb4b0-f00f-40dd-a916-ef65948ad4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first (seq/label) tuple in train_data\n",
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86eb7621-f539-4c89-909f-7d707dd41fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_size=50, out_size=1):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # Add an LSTM layer:\n",
    "        self.lstm = nn.LSTM(input_size,hidden_size)\n",
    "        \n",
    "        # Add a fully-connected layer:\n",
    "        self.linear = nn.Linear(hidden_size,out_size)\n",
    "        \n",
    "        # Initialize h0 and c0:\n",
    "        self.hidden = (torch.zeros(1,1,hidden_size),\n",
    "                       torch.zeros(1,1,hidden_size))\n",
    "    \n",
    "    def forward(self,seq):\n",
    "        lstm_out, self.hidden = self.lstm(seq.view(len(seq), 1, -1), self.hidden)\n",
    "        pred = self.linear(lstm_out.view(len(seq),-1))\n",
    "        return pred[-1]   # we only care about the last prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25bdc682-46ae-4dab-8d36-7302329f61b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "model = LSTM()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8d20ee-b4f2-4ddd-b30c-5b0c7ef64657",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    params = [p.numel() for p in model.parameters() if p.requires_grad]\n",
    "    for item in params:\n",
    "        print(f'{item:>6}')\n",
    "    print(f'______\\n{sum(params):>6}')\n",
    "    \n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175ca2eb-6747-4a99-b84d-bb5b137e6016",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "future = 40\n",
    "\n",
    "for i in range(epochs):\n",
    "    \n",
    "    # tuple-unpack the train_data set\n",
    "    for seq, y_train in train_data:\n",
    "        \n",
    "        # reset the parameters and hidden states\n",
    "        optimizer.zero_grad()\n",
    "        model.hidden = (torch.zeros(1,1,model.hidden_size),\n",
    "                        torch.zeros(1,1,model.hidden_size))\n",
    "        \n",
    "        y_pred = model(seq)\n",
    "        \n",
    "        loss = criterion(y_pred, y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    # print training result\n",
    "    print(f'Epoch: {i+1:2} Loss: {loss.item():10.8f}')\n",
    "    \n",
    "    # MAKE PREDICTIONS\n",
    "    # start with a list of the last 10 training records\n",
    "    preds = train_set[-window_size:].tolist()\n",
    "\n",
    "    for f in range(future):  \n",
    "        seq = torch.FloatTensor(preds[-window_size:])\n",
    "        with torch.no_grad():\n",
    "            model.hidden = (torch.zeros(1,1,model.hidden_size),\n",
    "                            torch.zeros(1,1,model.hidden_size))\n",
    "            preds.append(model(seq).item())\n",
    "            \n",
    "    loss = criterion(torch.tensor(preds[-window_size:]),y[760:])\n",
    "    print(f'Loss on test predictions: {loss}')\n",
    "\n",
    "    # Plot from point 700 to the end\n",
    "    plt.figure(figsize=(12,4))\n",
    "    plt.xlim(700,801)\n",
    "    plt.grid(True)\n",
    "    plt.plot(y.numpy())\n",
    "    plt.plot(range(760,800),preds[window_size:])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8623ee9-0b3f-4e83-9f1d-2227e39f3b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "window_size = 40\n",
    "future = 40\n",
    "\n",
    "# Create the full set of sequence/label tuples:\n",
    "all_data = input_data(y,window_size)\n",
    "len(all_data)  # this should equal 800-40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9fd746-04c5-4f78-954b-197d1b289271",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "for i in range(epochs):\n",
    "    \n",
    "    # tuple-unpack the entire set of data\n",
    "    for seq, y_train in all_data:  \n",
    "       \n",
    "        # reset the parameters and hidden states\n",
    "        optimizer.zero_grad()\n",
    "        model.hidden = (torch.zeros(1,1,model.hidden_size),\n",
    "                        torch.zeros(1,1,model.hidden_size))\n",
    "        \n",
    "        y_pred = model(seq)\n",
    "        \n",
    "        loss = criterion(y_pred, y_train)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    # print training result\n",
    "    print(f'Epoch: {i+1:2} Loss: {loss.item():10.8f}')\n",
    "    \n",
    "print(f'\\nDuration: {time.time() - start_time:.0f} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b007c89c-56be-4114-ab3a-e7accdbeae72",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = y[-window_size:].tolist()\n",
    "\n",
    "for i in range(future):  \n",
    "    seq = torch.FloatTensor(preds[-window_size:])\n",
    "    with torch.no_grad():\n",
    "        # Reset the hidden parameters\n",
    "        model.hidden = (torch.zeros(1,1,model.hidden_size),\n",
    "                        torch.zeros(1,1,model.hidden_size))  \n",
    "        preds.append(model(seq).item())\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.xlim(-10,841)\n",
    "plt.grid(True)\n",
    "plt.plot(y.numpy())\n",
    "plt.plot(range(800,800+future),preds[window_size:])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4efa63a6-bf36-4f6f-a41c-bde17943ae5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a164cce3-1185-4666-aa93-644eb2b58594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Step 1: Generate Sine Wave Data\n",
    "# x = torch.linspace(0, 799, steps=800)\n",
    "# y = torch.sin(x * 2 * 3.1416 / 40)  # Sine wave with period of 40\n",
    "\n",
    "# # Step 2: Split Data into Training and Testing Sets\n",
    "# test_size = 40\n",
    "# train_set = y[:-test_size]  # First 760 points for training\n",
    "# test_set = y[-test_size:]   # Last 40 points for testing\n",
    "\n",
    "# # Step 3: Create Input Sequences (Windowed Data)\n",
    "# def create_sequences(data, window_size):\n",
    "#     sequences = []\n",
    "#     for i in range(len(data) - window_size):\n",
    "#         window = data[i:i+window_size]   # Input sequence\n",
    "#         label = data[i+window_size]      # Target value\n",
    "#         sequences.append((window, label))\n",
    "#     return sequences\n",
    "\n",
    "# window_size = 40\n",
    "# train_data = create_sequences(train_set, window_size)\n",
    "\n",
    "# torch.manual_seed(42)  # For reproducibility\n",
    "\n",
    "# # Step 4: Define LSTM Model\n",
    "# class LSTM(nn.Module):\n",
    "#     def __init__(self, input_size=1, hidden_size=50, output_size=1):\n",
    "#         super().__init__()\n",
    "#         self.hidden_size = hidden_size\n",
    "#         self.lstm = nn.LSTM(input_size, hidden_size)  # LSTM layer\n",
    "#         self.linear = nn.Linear(hidden_size, output_size)  # Fully connected layer\n",
    "#         self.hidden = (torch.zeros(1, 1, hidden_size), torch.zeros(1, 1, hidden_size))\n",
    "    \n",
    "#     def forward(self, seq):\n",
    "#         lstm_out, self.hidden = self.lstm(seq.view(len(seq), 1, -1), self.hidden)\n",
    "#         pred = self.linear(lstm_out.view(len(seq), -1))\n",
    "#         return pred[-1]  # Return last predicted value\n",
    "\n",
    "# # Step 5: Initialize Model, Loss, and Optimizer\n",
    "# model = LSTM()\n",
    "# criterion = nn.MSELoss()\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.01)  # Adam is better for beginners\n",
    "\n",
    "# # Step 6: Training Loop\n",
    "# epochs = 10\n",
    "# for epoch in range(epochs):\n",
    "#     for seq, label in train_data:\n",
    "#         optimizer.zero_grad()  # Reset gradients\n",
    "#         model.hidden = (torch.zeros(1, 1, model.hidden_size), torch.zeros(1, 1, model.hidden_size))  # Reset hidden state\n",
    "#         y_pred = model(seq)  # Forward pass\n",
    "#         loss = criterion(y_pred, label)  # Compute loss\n",
    "#         loss.backward()  # Backpropagation\n",
    "#         optimizer.step()  # Update weights\n",
    "#     print(f'Epoch {epoch+1}, Loss: {loss.item():.6f}')\n",
    "\n",
    "# # Step 7: Make Future Predictions\n",
    "# future = 40\n",
    "# preds = train_set[-window_size:].tolist()\n",
    "# for _ in range(future):\n",
    "#     seq = torch.FloatTensor(preds[-window_size:])\n",
    "#     with torch.no_grad():\n",
    "#         model.hidden = (torch.zeros(1, 1, model.hidden_size), torch.zeros(1, 1, model.hidden_size))\n",
    "#         preds.append(model(seq).item())\n",
    "\n",
    "# # Step 8: Plot Results\n",
    "# plt.figure(figsize=(12, 6))\n",
    "# plt.plot(y.numpy(), label='Actual Data')\n",
    "# plt.plot(range(760, 800), preds[window_size:], label='Predictions', linestyle='dashed')\n",
    "# plt.legend()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002f9bb5-0365-4c43-b30c-e77d7bcdb58d",
   "metadata": {},
   "source": [
    "# Time Series "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f506e46f-a121-4174-a0ad-c3b6c9bb352d",
   "metadata": {},
   "source": [
    "# prediction of sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25357e8-ad18-42d5-8f7e-a2eb983d818b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# This relates to plotting datetime values with matplotlib:\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53b546b-727b-4d3b-bcec-e03c062b2ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(r\"C:\\Users\\konde\\OneDrive\\PYTORCH_NOTEBOOKS\\PYTORCH_NOTEBOOKS\\Data\\TimeSeriesData\\Alcohol_Sales.csv\",index_col=0,parse_dates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1931a69-d925-4824-aaa7-f0375cca9c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98961e30-37bb-4f67-afd9-6b8a80dc052a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd865cb-73c7-4113-8b30-3171ef40d90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823fd7a5-b923-484a-85a4-a919fe659742",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot(figsize=(12,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfc2e8b-0693-4274-ab1a-9d80b59386fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c8893d-9e4d-4605-ba57-a6c7930024bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#y=df['S4248SM144NCEN'].astype(float)\n",
    "y=df['S4248SM144NCEN'].values.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd39f6f7-9791-4a09-a386-28c02aeea8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6eae950-727a-4ea4-b966-ef2b351095b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a test size\n",
    "test_size = 12\n",
    "\n",
    "# Create train and test sets\n",
    "train_set = y[:-test_size]\n",
    "test_set = y[-test_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69681d60-042c-46b2-85e5-a781ce8c4ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d4c117-2819-4b25-85e2-2b704e998b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Instantiate a scaler with a feature range from -1 to 1\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747ec868-6bc6-4c79-97ee-db6587fc025e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the training set\n",
    "train_norm = scaler.fit_transform(train_set.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba0245c-0d4a-4a28-9ec4-1e5ad277fd87",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_norm.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13bc0d9-b914-4c67-a04c-e7a916368c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_norm.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec46e65-0b8a-4152-a793-41867d952097",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert train_norm from an array to a tensor\n",
    "train_norm = torch.FloatTensor(train_norm).view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b696dd-f547-461d-9c83-bf9ed0cd7563",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d17c0e-e055-4b68-8a1d-6e16813e3d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size=12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad35c75-c426-424f-9248-b4537a96a367",
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_data(seq,ws):\n",
    "    out=[]\n",
    "    L=len(seq)\n",
    "    for i in range(L-ws):\n",
    "        window = seq[i:i+ws]\n",
    "        label = seq[i+ws:i+ws+1]\n",
    "        out.append((window,label))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8296e1b4-698c-4db2-a809-c412b8766dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_data=(input_data(train_norm,window_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca85568-ac27-4216-bec2-a3c9e853bff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(trained_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994a78df-4ebd-4c88-8c4b-8614e6ea41cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class LSTMnetwork(nn.Module):\n",
    "#     def __init__(self,input_size=1,hidden_size=100,output_size=1):\n",
    "#         super().__init__()\n",
    "#         self.hidden_size=hidden_size\n",
    "#         self.lstm=nn.LSTM(input_size,hidden_size)\n",
    "#         self.linear=nn.Linear(hidden_size,output_size)\n",
    "#         self.hidden=(torch.zeros(1,1,hidden_size),\n",
    "#                      torch.zeros(1,1,hidden_size))\n",
    "#         def forward(self,seq):\n",
    "#             lstm_out,self.hidden=self.lstm(seq.view(len(seq),1,-1),self.hidden))\n",
    "#             pred = self.linear(lstm_out.view(len(seq),-1))\n",
    "#         return pred[-1]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6a57e4-0037-40dc-9536-196086533ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMnetwork(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_size=100, output_size=1):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size)\n",
    "        self.linear = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, seq):\n",
    "        # Initialize hidden state inside forward\n",
    "        hidden = (\n",
    "            torch.zeros(1, 1, self.hidden_size),\n",
    "            torch.zeros(1, 1, self.hidden_size)\n",
    "        )\n",
    "        \n",
    "        lstm_out, _ = self.lstm(seq.view(len(seq), 1, -1), hidden)\n",
    "        pred = self.linear(lstm_out.view(len(seq), -1))\n",
    "        return pred[-1]  # Return the last prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e520794-9157-4969-8033-6e6e473bc124",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(101)\n",
    "model = LSTMnetwork()\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac475302-824e-4ddd-bdae-ffc5195e36b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in model.parameters():\n",
    "    print(p.numel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04ba2c3-b722-4a50-9353-734078dcaeae",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=100\n",
    "\n",
    "import time\n",
    "start_time =time.time()\n",
    "\n",
    "for i in range(epochs):\n",
    "    for seq,y_train in trained_data:\n",
    "        optimizer.zero_grad()\n",
    "        model.hidden = (\n",
    "            torch.zeros(1, 1, model.hidden_size),\n",
    "            torch.zeros(1, 1, model.hidden_size)\n",
    "        )\n",
    "        y_pred = model(seq)\n",
    "\n",
    "        loss = criterion(y_pred, y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'epoch{i+1} loss{loss.item()}')\n",
    "        \n",
    "total_time =time.time()-start_time\n",
    "print(total_time/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b533bb58-5442-4ec1-9427-5d33b90b7a53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6c88839d-cda4-4b8e-ac83-a88b077ce4ac",
   "metadata": {},
   "source": [
    "Run predictions and compare to known test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3859f87-2db0-4916-b35b-e683e42d4045",
   "metadata": {},
   "outputs": [],
   "source": [
    "future = 12\n",
    "\n",
    "# Add the last window of training values to the list of predictions\n",
    "preds = train_norm[-window_size:].tolist()\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "for i in range(future):\n",
    "    seq = torch.FloatTensor(preds[-window_size:])\n",
    "    with torch.no_grad():\n",
    "        model.hidden = (torch.zeros(1,1,model.hidden_size),\n",
    "                        torch.zeros(1,1,model.hidden_size))\n",
    "        preds.append(model(seq).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1bb2571-5b3d-41d9-bbbc-86a6af1251cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_predictions = scaler.inverse_transform(np.array(preds[window_size:]).reshape(-1, 1))\n",
    "true_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e738a2-0072-45fb-88c7-b919ef82b4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['S4248SM144NCEN'][-12:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a18be0-819f-40f4-ad02-ca68c55fd85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember that the stop date has to be later than the last predicted value.\n",
    "x = np.arange('2018-02-01', '2019-02-01', dtype='datetime64[M]').astype('datetime64[D]')\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1f9bfa-da08-4665-9dee-bc1100b7de18",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,4))\n",
    "plt.title('Beer, Wine, and Alcohol Sales')\n",
    "plt.ylabel('Sales (millions of dollars)')\n",
    "plt.grid(True)\n",
    "plt.autoscale(axis='x',tight=True)\n",
    "plt.plot(df['S4248SM144NCEN'])\n",
    "plt.plot(x,true_predictions)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a53241-5a4c-46f4-ba9b-f8d166f063f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the end of the graph\n",
    "fig = plt.figure(figsize=(12,4))\n",
    "plt.title('Beer, Wine, and Alcohol Sales')\n",
    "plt.ylabel('Sales (millions of dollars)')\n",
    "plt.grid(True)\n",
    "plt.autoscale(axis='x',tight=True)\n",
    "fig.autofmt_xdate()\n",
    "\n",
    "# Select the end of the graph with slice notation:\n",
    "plt.plot(df['S4248SM144NCEN']['2017-01-01':])\n",
    "plt.plot(x,true_predictions)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b3805f-ef4b-49a9-b6e2-402177c69793",
   "metadata": {},
   "source": [
    "Forecast into an unknown future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db32d2e-d540-43b7-ba06-c60d40c9d878",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "\n",
    "# set model to back to training mode\n",
    "model.train()\n",
    "\n",
    "# feature scale the entire dataset\n",
    "y_norm = scaler.fit_transform(y.reshape(-1, 1))\n",
    "y_norm = torch.FloatTensor(y_norm).view(-1)\n",
    "all_data = input_data(y_norm,window_size)\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    # train on the full set of sequences\n",
    "    for seq, y_train in all_data:  \n",
    "        \n",
    "        # reset the parameters and hidden states\n",
    "        optimizer.zero_grad()\n",
    "        model.hidden = (torch.zeros(1,1,model.hidden_size),\n",
    "                        torch.zeros(1,1,model.hidden_size))\n",
    "        \n",
    "        y_pred = model(seq)\n",
    "        \n",
    "        loss = criterion(y_pred, y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    # print training result\n",
    "    print(f'Epoch: {epoch+1:2} Loss: {loss.item():10.8f}')\n",
    "    \n",
    "print(f'\\nDuration: {time.time() - start_time:.0f} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f4fbe9-7d36-44be-a66e-e3d6f71bb375",
   "metadata": {},
   "source": [
    "Predict future values, plot the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb17cc4c-fcf4-4b30-bbe8-4d34c57472b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 12\n",
    "future = 12\n",
    "L = len(y)\n",
    "\n",
    "preds = y_norm[-window_size:].tolist()\n",
    "\n",
    "model.eval()\n",
    "for i in range(future):  \n",
    "    seq = torch.FloatTensor(preds[-window_size:])\n",
    "    with torch.no_grad():\n",
    "        # Reset the hidden parameters here!\n",
    "        model.hidden = (torch.zeros(1,1,model.hidden_size),\n",
    "                        torch.zeros(1,1,model.hidden_size))  \n",
    "        preds.append(model(seq).item())\n",
    "\n",
    "# Inverse-normalize the prediction set\n",
    "true_predictions = scaler.inverse_transform(np.array(preds).reshape(-1, 1))\n",
    "\n",
    "# PLOT THE RESULT\n",
    "# Set a data range for the predicted data.\n",
    "# Remember that the stop date has to be later than the last predicted value.\n",
    "x = np.arange('2019-02-01', '2020-02-01', dtype='datetime64[M]').astype('datetime64[D]')\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.title('Beer, Wine, and Alcohol Sales')\n",
    "plt.ylabel('Sales (millions of dollars)')\n",
    "plt.grid(True)\n",
    "plt.autoscale(axis='x',tight=True)\n",
    "plt.plot(df['S4248SM144NCEN'])\n",
    "plt.plot(x,true_predictions[window_size:])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b9f5dc-6eb7-446b-b17d-1196164943e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,4))\n",
    "plt.title('Beer, Wine, and Alcohol Sales')\n",
    "plt.ylabel('Sales (millions of dollars)')\n",
    "plt.grid(True)\n",
    "plt.autoscale(axis='x',tight=True)\n",
    "fig.autofmt_xdate()\n",
    "plt.plot(df['S4248SM144NCEN']['2017-01-01':])\n",
    "plt.plot(x,true_predictions[window_size:])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1291ae0e-dbf4-4ec9-b094-8d2e1fd2f203",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898615dd-699a-4673-8b7a-d26c0ff18d6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c1e85a-6ac0-4a9f-b248-ee60151697e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423b5057-cbc8-423b-b00e-f90b52106b4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7256f95-b3e3-4fa1-9130-cc894bec03eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814f80d1-c276-4941-9412-ed3079059b74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24b9b26-e49e-4af6-b6b3-12fb89af3fe1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48900c54-d969-48c6-a0e8-c72e452e806b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3171a1df-bcf4-4e92-a536-a2057a82de51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4936d32-000e-475f-8d5a-f79c5a4776cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
